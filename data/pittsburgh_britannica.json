[
  {
    "url": "https://www.britannica.com/place/Pittsburgh",
    "content": "Pittsburgh, city, seat (1788) of Allegheny county, southwestern Pennsylvania, U.S. The city is located at the confluence of the Allegheny and Monongahela rivers, which unite at the point of the “Golden Triangle” (the business district) to form the Ohio River. A city of hills, parks, and valleys, it is the centre of an urban industrial complex that includes the surrounding cities of Aliquippa (northwest), New Kensington (northeast), McKeesport (southeast), and Washington (southwest) and the borough of Wilkinsburg (east). Inc. borough, 1794; city, 1816. Area city, 58 square miles (150 square km). Pop. (2010) 305,704; Pittsburgh Metro Area, 2,356,285; (2020) 302,971; Pittsburgh Metro Area, 2,370,930.\nAlgonquian- and Iroquoian-speaking peoples were early inhabitants of the region. The conflict between the British and French over territorial claims in the area was settled in 1758 when General John Forbes and his British and colonial army expelled the French from Fort Duquesne (built 1754). Forbes named the site for the British statesman William Pitt the Elder. The British built Fort Pitt (1761) to ensure their dominance at the source of the Ohio. Settlers began arriving after Native American forces led by Ottawa chief Pontiac were defeated in 1763; an agreement subsequently was made between Native American groups and the Penn family, and a boundary dispute was ended between Pennsylvania and Virginia. Pittsburgh was laid out (1764) by John Campbell in the area around the fort (now the Golden Triangle). Following the American Revolution, the town became an outfitting point for settlers traveling westward down the Ohio River.\nPittsburgh’s strategic location and wealth of natural resources spurred its commercial and industrial growth in the 19th century. A blast furnace, erected by George Anschutz about 1792, was the forerunner of the iron and steel industry that for more than a century was the city’s economic mainstay; by 1850 Pittsburgh was known as the “Iron City.” The Pennsylvania Canal and the Portage Railroad, both completed in 1834, opened vital markets for trade and shipping. The city suffered a great loss in 1845 when some 24 blocks of businesses, homes, churches, and other buildings were destroyed by fire.\nAfter the American Civil War, great numbers of European immigrants swelled Pittsburgh’s population, and industrial magnates such as Andrew Carnegie, Henry Clay Frick, and Thomas Mellon built their steel empires there. The city became the focus of historic friction between labour and management, and the American Federation of Labor was born there in 1881.\nBy 1900 the city’s population had reached 321,616. Growth continued nearly unabated through World War II, the war years bringing a particularly great boon for the economy. The population crested at more than 675,000 in 1950, after which it steadily declined; by the end of the century, it had returned almost to the 1900 level. Most citizens were still of European ancestry, but the growing African American proportion of the population exceeded one-fourth. During the period of economic and population growth, Pittsburgh had come to epitomize the grimy, polluted industrial city. After the war, however, the city undertook an extensive redevelopment program that emphasized smoke-pollution control, flood prevention, and sewage disposal. In 1957 it became the first American city to generate electricity by nuclear power.\nBy the late 1970s and early ’80s, the steel industry had virtually disappeared—a result of foreign competition and decreased demand. Many of the surrounding mill towns were laid to waste by unemployment, becoming a symbol of the notorious Rust Belt, the U.S. region where steelmaking and manufacturing once thrived before succumbing to widespread unemployment and poverty. Pittsburgh, however, successfully diversified its economy through more emphasis on light industries—though metalworking, chemicals, and plastics remained important—and on such high-technology industries as computer software, industrial automation (robotics), and biomedical and environmental technologies. Numerous industrial research laboratories were established in the area, and the service sector became increasingly important. Pittsburgh long has been one of the nation’s largest inland ports, and it remains a leading transportation centre.\nMuch of the Golden Triangle has been rebuilt and includes Point State Park (containing Fort Pitt Blockhouse and Fort Pitt Museum), the Gateway Center (site of several skyscrapers and a garden), and the David L. Lawrence Convention Center. The University of Pittsburgh was chartered in 1787. Other educational institutions include Carnegie Mellon (1900), Duquesne (1878), Point Park (1960), Chatham (1869), and Carlow (1929) universities and two campuses of the Community College of Allegheny County (1966).\nCentral to the city’s cultural life is the Carnegie Museums of Pittsburgh (formerly Carnegie Institute), an umbrella organization consisting of a number of institutions. Its museums include those for the fine arts and natural history (both founded in 1895), the Carnegie Science Center (1991), which now also houses the Henry Buhl, Jr., Planetarium and Observatory (1939), and the Andy Warhol Museum (1994), which exhibits the works of the Pittsburgh-born artist and filmmaker. Other institutions affiliated with the organization are the Carnegie Library of Pittsburgh, which contains more than 3.3 million volumes, and the Carnegie Music Hall. The Pittsburgh Symphony Orchestra performs at Heinz Hall, a restored movie theatre.\nPhipps Conservatory and Botanical Gardens (1893), which covers 15 acres (6 hectares), is noted for its extensive greenhouses. The city’s zoo, in the northeastern Highland Park neighbourhood, includes an aquarium. Two new sports venues opened in 2001 on the north bank of the Allegheny opposite the Golden Triangle: PNC Park is home of the Pirates, the city’s professional baseball team, and Acrisure Stadium houses the Steelers, its professional football team. The Penguins, Pittsburgh’s professional ice hockey team, plays at PPG Paints Arena. Popular summertime attractions include riverboat excursions on Pittsburgh’s waterways and Kennywood, an amusement park southeast of the city in West Mifflin.\nCarnegie Mellon University, private, coeducational institution of higher learning in Pittsburgh, Pennsylvania, U.S. The university includes the Carnegie Institute of Technology, the College of Humanities and Social Sciences, the College of Fine Arts, the Mellon College of Science, the School of Computer Science, the H. John Heinz III School of Public Policy and Management, and the Graduate School of Industrial Administration. Undergraduate and graduate degree programs are offered in a range of fields. Total enrollment is about 7,700.\nIn 1900 the industrialist Andrew Carnegie gave a gift of $1 million to the city of Pittsburgh for the creation of a technical school. Originally called Carnegie Technical Schools, it was renamed Carnegie Institute of Technology in 1912. The institute merged with the Mellon Institute (established in 1913 in Pittsburgh by financier Andrew W. Mellon) in 1967. The university has built a reputation as a vital arts centre, operating three art galleries, two concert halls, and two theatres. The faculty has included Nobel Prize-winning economists Herbert Alexander Simon and Merton Miller.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/place/Ohio-state",
    "content": "Ohio, constituent state of the United States of America, on the northeastern edge of the Midwest region. Lake Erie lies on the north, Pennsylvania on the east, West Virginia and Kentucky on the southeast and south, Indiana on the west, and Michigan on the northwest. Ohio ranks 34th in terms of total area among the 50 states, and it is one of the smallest states west of the Appalachian Mountains. The state ranks near the top, however, in population. Ohio’s capital, after being located in Chillicothe and Zanesville during the early years of statehood, was finally established in newly founded and centrally located Columbus in 1816. The state takes its name from the Ohio River, which in turn traces its name to an Iroquoian word meaning “great water.”\nThe first state to be carved from the Northwest Territory, Ohio became the 17th member of the union on March 1, 1803. In many respects, Ohio has come to reflect the urbanized, industrialized, and ethnically mixed United States that developed from an earlier agrarian period. The pattern of its life is so representative of the country as a whole that it is often used to test attitudes, ideas, and commercial products. Significantly, Ohio has supplied by birth or residence eight U.S. presidents—William H. Harrison, Ulysses S. Grant, Rutherford B. Hayes, James A. Garfield, Benjamin Harrison, William McKinley, William H. Taft, and Warren G. Harding.\nThe state’s accessibility has been perhaps the key factor in its growth. Its location between the Eastern Seaboard and the heart of the Midwest and its lack of natural barriers to movement made it a corridor for east-west travel. In addition, the state lies in the heart of the country’s old industrial belt, close to major resources of raw materials and labour and to the markets of the East, Midwest, and South.\nArea 44,826 square miles (116,098 square km). Population (2020) 11,799,448; (2023 est.) 11,785,935.\nThe physiographic features of Ohio have strongly influenced its patterns of human settlement and land use. In most of the state, the topography, river systems, groundwater, and soils are the products of glacial activity.\nOhio straddles two major subregions of the Interior Lowlands physiographic region of the United States: the Appalachian Plateau on the east and the Central Lowland on the west. These two subregions divide the state almost in half. The Appalachian Plateau, reaching westward from Pennsylvania and West Virginia, stretches along Ohio’s eastern border, roughly from Lake Erie in the north to the Ohio River in the south. The northeast is only partially glaciated, while the southeast is unglaciated terrain. Throughout the plateau the land is dissected by rivers winding among steep hills, and many areas reach elevations of some 1,300 feet (395 metres).\nThe Central Lowland reaches westward from the Appalachian Plateau. The Lake Plains section of the lowland extends along Lake Erie and across the northwestern segment of the state to the Michigan border, before stretching irregularly to the south. It then levels to become slightly rolling terrain that was once under water; the swampiness of the northwest, around Toledo, posed obstacles to settlement before drainage made the land more arable. The Central (or Till) Plains, which extend westward toward the Mississippi River, include parts of western and southwestern Ohio and provide a deep soil. That region contains the state’s highest and lowest points: Campbell Hill, the highest point, at 1,549 feet (472 metres), is located near Bellefontaine; the lowest point, at 433 feet (132 metres), lies at the confluence of the Miami and Ohio rivers, near Cincinnati.\nThe principal water sources are rain-fed streams, lakes, and reservoirs. Floods, once prevalent, have generally been brought under control by state and federal dams and other conservation measures. Groundwater is used widely for public supplies, though the industrial and population centres have limited access to these resources. Huge stores of these waters are buried in preglacial valleys in central and south-central Ohio.\nLake Erie, with an average depth of only 62 feet (19 metres), is the shallowest of the Great Lakes. It is also the most tempestuous, with frontal storms often roaring across it from Canada, and the most liable to shoreline erosion, harbour silting, and filling of its bed. Its shallowness, coupled with the concentration of population, farms, and industrial plants in its watersheds, led to severe pollution by the mid-20th century. Subsequent attempts to abate pollution in Lake Erie have shown signs of success, however. Fish returned to previously uninhabitable waters, a revival of sport fishing and recreational activity stimulated economic growth along the shoreline, and urban water supplies were protected.\nA low watershed separates the roughly one-fifth of Ohio drained by the Maumee, Cuyahoga, and other rivers emptying into Lake Erie from the rest of the state, which is drained by the Miami, Scioto, Muskingum, and others flowing into the combined system of the Ohio and Mississippi rivers. The Ohio, only a tiny part of which is under state jurisdiction, is canalized and channeled for its entire length, as is the Muskingum River from Zanesville to Marietta. More than 100 lakes and reservoirs supply recreational and industrial water.\nMost of Ohio’s soils are well suited to agriculture. The naturally fertile Central Plains soils contain glacial limestone, and they also are rich in phosphates—one of the principal ingredients in fertilizer. Lake Plains soils also are highly productive. The sandstone-based soils of central and northeastern Ohio are best adapted to pasturelands, while the thin-soiled and heavily eroded hilly areas of the southeast support little productive farming except in river bottomlands.\nTemperatures in Ohio are similar to those across the north-central and eastern United States, with summer highs seldom reaching 100 °F (38 °C) and winter lows rarely dropping below −20 °F (−29 °C). On a typical July day the temperature will rise from the mid-60s F (upper 10s C) to the mid-80s F (about 30 °C), while in January it will reach a high in the mid-30s F (about 2 °C) from a low of about 20 °F (about −7 °C). The state is open to cold, dry fronts from Canada and warm, moist fronts from the Gulf of Mexico. The frequent meeting of such fronts causes much of the state’s precipitation, which typically totals about 40 inches (1,000 mm) annually, including an average annual snowfall of 28 inches (700 mm). In the northeastern snowbelt, however, snowfall averages over 100 inches (2,500 mm) per year. Ohio occasionally experiences mild earth tremors and destructive tornadoes.\nThe great hardwood forests that covered almost all of Ohio prior to European settlement were reduced to about one-tenth of the state’s area by 1900. However, during the 20th century the state government inaugurated various woodlands-reclamation and forest-management programs that helped increase Ohio’s forest cover to about one-third by the early 21st century. Much land in the southeastern and south-central regions has been reforested. The glaciated areas have stands of timber that include oak, ash, maple, walnut, basswood, hickory, and beech. The Ohio buckeye (Aesculus glabra), the official state tree, is common along rivers and creeks. Wildflowers such as trillium, jack-in-the-pulpit, mayapple, and phlox abound, as do many domesticated species.\nOf some 350 bird species found in Ohio, more than half are native. Among more than 150 fish species are bass, trout, walleye, muskellunge, and perch, while the dozens of species of mammals include deer, opossums, foxes, skunks, raccoons, groundhogs, and rabbits. Beaver and wild turkey populations have been reestablished. The number of coyotes has been increasing since the late 20th century.\nMore than four-fifths of the population of Ohio is of white European ancestry. The first official settlement by white Europeans in the Northwest Territory was established at Marietta, on the Ohio River, in what is now Ohio, in 1788 by a company of New Englanders who had fought in the American Revolution. In the same year a group from New Jersey settled near Cincinnati, and in the next few years other villages sprang up. In the south, particularly in the Virginia Military District between the Scioto and Little Miami rivers, many of the settlers came from Virginia and Kentucky. In 1796 the Western Reserve, a territory in far northeastern Ohio, was first settled, mainly by New Englanders from Connecticut.\nSouth of the Western Reserve, but still in northeastern Ohio, Canton and Steubenville were established by Pennsylvania Germans. German settlers also were attracted to the rolling surface and fertile soil of Wayne county, which later became one of the top agricultural counties in the country. German-speaking Moravian missionaries came to the Tuscarawas River valley in the early 1770s to convert the native populations to Christianity. In 1817 an experimental communist settlement was founded in Zoar that lasted until 1898. The Swiss settled around Dover and Sugar Creek in Tuscarawas county, as well as in Monroe county. In Holmes county, Amish immigrants from Germany and Switzerland established settlements that still remain today. There are now more Mennonites in Ohio than in Switzerland, and the state’s Old Order Amish community is the largest in the world. Quakers and many Scotch-Irish Protestants from the Middle Atlantic states and the South settled in eastern and southwestern Ohio early in the 19th century.\nPrior to 1830, Pennsylvania Germans and Swiss came to the east-central region from areas to the northwest. After 1830, settlers came directly from Germany and Ireland. Many Irish came to work on the Ohio canals and stayed on, and when the railroads were built, the Irish and German workers remained as permanent settlers. Germans who drained the swamp country of the northwest stayed on to develop the resultant farmlands. After 1830 Roman Catholic immigrants from southern Ireland settled in such cities as Cleveland, Columbus, and Cincinnati, where by 1850 they were second in number to the Germans among foreign-born residents. Both German and Irish immigrants were widely dispersed, however, and they also often settled in smaller communities like Lancaster, where they joined the Pennsylvania Germans who had founded the city.\nThe Welsh arrived in the early 19th century to develop the mineral resources in several regions of Ohio. They were especially numerous in Jackson county in the southeast. Indeed, Jackson county was a Welsh cultural nexus for a long period, with the Welsh language persisting to the third generation in many communities.\nIn 1850 the principal population of the state was of Scotch-Irish descent, although the German and English communities also were significant. By 1870, however, nearly 14 percent of Ohio’s total population and 40 percent of Cleveland’s were foreign-born. The New England character of early northern Ohio had changed, as each new immigrant group established its own newspapers, clubs, social life, and churches.\nIncreasing numbers of immigrants from eastern and southern Europe came to Ohio after 1880. By 1920 large numbers of Italians, Poles, Hungarians, Russians, and other groups had come to Cleveland, Toledo, Youngstown, and other industrial cities. Meanwhile, Southern white settlers from the Appalachian Mountains came in large numbers to Akron, Dayton, and Cincinnati. Cleveland, however, became Ohio’s most ethnically diverse city. Its foreign-born population was supplemented between 1880 and 1890 by new arrivals from Austria-Hungary, Poland, the Netherlands, Russia, Portugal, Greece, China, Japan, Turkey, and Mexico. The city’s character was enriched by dozens of such groups, representing a broad array of linguistic and cultural backgrounds. The descendants of these ethnic communities firmly established themselves in the social, economic, and political life of the state.\nThroughout the period of white settlement, Ohio’s population of black African descent also grew, albeit sporadically. Slavery was banned in Ohio in 1802, and strong efforts to prevent black immigration limited growth of the black community until the American Civil War (1861–65). In 1850 the state had about 25,000 black residents; after the war, however, the black population surged, more than doubling by 1870, with most of the newcomers settling in the state’s southern regions. In the early 21st century Ohio’s African American community constituted more than one-tenth of the state’s total population; it was distributed across the state but generally concentrated in urban areas.\nSince the late 20th century the state’s Hispanic and Asian populations have shown rapid growth, becoming significant (though still small) minorities by the early 21st century. The Hispanic community is largely of Mexican and Puerto Rican heritage. Residents of Chinese and South Asian ancestry predominate within the Asian population. Native Americans make up just a tiny fraction of Ohio’s people.\nDespite Ohio’s many major cities and metropolitan areas—the largest of which include Columbus, Cleveland, Cincinnati, Toledo, and Akron—roughly three-fourths of the state is cropland and forest. The urban areas of Ohio first exceeded the rural in population in 1910, and by the turn of the 21st century the urban population made up about 75 percent of the total. Residential areas outlining the central cities contain more than two-thirds of the urban population, and Ohio’s large cities, with the exception of Columbus, have followed the national pattern of losing population to surrounding suburbs. The growth of Columbus proper is largely attributable to annexation of township lands.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/event/Industrial-Revolution",
    "content": "Industrial Revolution, in modern history, the process of change from an agrarian and handicraft economy to one dominated by industry and machine manufacturing. These technological changes introduced novel ways of working and living and fundamentally transformed society. This process began in Britain in the 18th century and from there spread to other parts of the world. Although used earlier by French writers, the term Industrial Revolution was first popularized by the English economic historian Arnold Toynbee (1852–83) to describe Britain’s economic development from 1760 to 1840. Since Toynbee’s time the term has been more broadly applied as a process of economic transformation than as a period of time in a particular setting. This explains why some areas, such as China and India, did not begin their first industrial revolutions until the 20th century, while others, such as the United States and western Europe, began undergoing “second” industrial revolutions by the late 19th century.\nA brief treatment of the Industrial Revolution follows. For full treatment of the Industrial Revolution as it occurred in Europe, see Europe, history of: The Industrial Revolution.\n(Read James Watt’s 1819 Britannica essay on the steam engine.)\nThe main features involved in the Industrial Revolution were technological, socioeconomic, and cultural. The technological changes included the following: (1) the use of new basic materials, chiefly iron and steel, (2) the use of new energy sources, including both fuels and motive power, such as coal, the steam engine, electricity, petroleum, and the internal-combustion engine, (3) the invention of new machines, such as the spinning jenny and the power loom that permitted increased production with a smaller expenditure of human energy, (4) a new organization of work known as the factory system, which entailed increased division of labour and specialization of function, (5) important developments in transportation and communication, including the steam locomotive, steamship, automobile, airplane, telegraph, and radio, and (6) the increasing application of science to industry. These technological changes made possible a tremendously increased use of natural resources and the mass production of manufactured goods.\nThere were also many new developments in nonindustrial spheres, including the following: (1) agricultural improvements that made possible the provision of food for a larger nonagricultural population, (2) economic changes that resulted in a wider distribution of wealth, the decline of land as a source of wealth in the face of rising industrial production, and increased international trade, (3) political changes reflecting the shift in economic power, as well as new state policies corresponding to the needs of an industrialized society, (4) sweeping social changes, including the growth of cities, the development of working-class movements, and the emergence of new patterns of authority, and (5) cultural transformations of a broad order. Workers acquired new and distinctive skills, and their relation to their tasks shifted; instead of being craftsmen working with hand tools, they became machine operators, subject to factory discipline. Finally, there was a psychological change: confidence in the ability to use resources and to master nature was heightened.\nIn the period 1760 to 1830 the Industrial Revolution was largely confined to Britain. Aware of their head start, the British forbade the export of machinery, skilled workers, and manufacturing techniques. The British monopoly could not last forever, especially since some Britons saw profitable industrial opportunities abroad, while continental European businessmen sought to lure British know-how to their countries. Two Englishmen, William and John Cockerill, brought the Industrial Revolution to Belgium by developing machine shops at Liège (c. 1807), and Belgium became the first country in continental Europe to be transformed economically. Like its British progenitor, the Belgian Industrial Revolution centred in iron, coal, and textiles.\nFrance was more slowly and less thoroughly industrialized than either Britain or Belgium. While Britain was establishing its industrial leadership, France was immersed in its Revolution, and the uncertain political situation discouraged large investments in industrial innovations. By 1848 France had become an industrial power, but, despite great growth under the Second Empire, it remained behind Britain.\nOther European countries lagged far behind. Their bourgeoisie lacked the wealth, power, and opportunities of their British, French, and Belgian counterparts. Political conditions in the other nations also hindered industrial expansion. Germany, for example, despite vast resources of coal and iron, did not begin its industrial expansion until after national unity was achieved in 1870. Once begun, Germany’s industrial production grew so rapidly that by the turn of the century that nation was outproducing Britain in steel and had become the world leader in the chemical industries. The rise of U.S. industrial power in the 19th and 20th centuries also far outstripped European efforts. And Japan too joined the Industrial Revolution with striking success.\nThe eastern European countries were behind early in the 20th century. It was not until the five-year plans that the Soviet Union became a major industrial power, telescoping into a few decades the industrialization that had taken a century and a half in Britain. The mid-20th century witnessed the spread of the Industrial Revolution into hitherto nonindustrialized areas such as China and India.\nThe technological and economic aspects of the Industrial Revolution brought about significant sociocultural changes. In its initial stages it seemed to deepen labourers’ poverty and misery. Their employment and subsistence became dependent on costly means of production that few people could afford to own. Job security was lacking: workers were frequently displaced by technological improvements and a large labour pool. Lack of worker protections and regulations meant long work hours for miserable wages, living in unsanitary tenements, and exploitation and abuse in the workplace. But even as problems arose, so too did new ideas that aimed to address them. These ideas pushed innovations and regulations that provided people with more material conveniences while also enabling them to produce more, travel faster, and communicate more rapidly.\nDespite considerable overlapping with the “old,” there was mounting evidence for a “new” Industrial Revolution in the late 19th and 20th centuries. In terms of basic materials, modern industry began to exploit many natural and synthetic resources not hitherto utilized: lighter metals, rare earths, new alloys, and synthetic products such as plastics, as well as new energy sources. Combined with these were developments in machines, tools, and computers that gave rise to the automatic factory. Although some segments of industry were almost completely mechanized in the early to mid-19th century, automatic operation, as distinct from the assembly line, first achieved major significance in the second half of the 20th century.\nOwnership of the means of production also underwent changes. The oligarchical ownership of the means of production that characterized the Industrial Revolution in the early to mid-19th century gave way to a wider distribution of ownership through purchase of common stocks by individuals and by institutions such as insurance companies. In the first half of the 20th century, many countries of Europe socialized basic sectors of their economies. There was also during that period a change in political theories: instead of the laissez-faire ideas that dominated the economic and social thought of the classical Industrial Revolution, governments generally moved into the social and economic realm to meet the needs of their more complex industrial societies. That trend was reversed in the United States and the United Kingdom beginning in the 1980s.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/dictionary/vital",
    "content": "",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/sports/ice-hockey",
    "content": "ice hockey, game between two teams, each usually having six players, who wear skates and compete on an ice rink. The object is to propel a vulcanized rubber disk, the puck, past a goal line and into a net guarded by a goaltender, or goalie. With its speed and its frequent physical contact, ice hockey has become one of the most popular of international sports. The game is an Olympic sport, and worldwide there are more than a million registered players performing regularly in leagues. It is perhaps Canada’s most popular game.\nExplore the ProCon debate\nUntil the mid-1980s it was generally accepted that ice hockey derived from English field hockey and Indian lacrosse and was spread throughout Canada by British soldiers in the mid-1800s. Research then turned up mention of a hockeylike game, played in the early 1800s in Nova Scotia by the Mi’kmaq (Micmac) Indians, which appeared to have been heavily influenced by the Irish game of hurling; it included the use of a “hurley” (stick) and a square wooden block instead of a ball. It was probably fundamentally this game that spread throughout Canada via Scottish and Irish immigrants and the British army. The players adopted elements of field hockey, such as the “bully” (later the face-off) and “shinning” (hitting one’s opponent on the shins with the stick or playing with the stick on one “shin” or side); this evolved into an informal ice game later known as shinny or shinty. The name hockey—as the organized game came to be known—has been attributed to the French word hoquet (shepherd’s stick). The term rink, referring to the designated area of play, was originally used in the game of curling in 18th-century Scotland. Early hockey games allowed as many as 30 players a side on the ice, and the goals were two stones, each frozen into one end of the ice. The first use of a puck instead of a ball was recorded at Kingston Harbour, Ontario, Canada, in 1860.\nThe first recorded public indoor ice hockey game, with rules largely borrowed from field hockey, took place in Montreal’s Victoria Skating Rink in 1875 between two teams of McGill University students. Unfortunately, the reputation for violence that the game would later develop was presaged in this early encounter, where, as The Daily British Whig of Kingston, Ontario, reported, “Shins and heads were battered, benches smashed and the lady spectators fled in confusion.” The first organized team, the McGill University Hockey Club, formed in 1877, codified their game’s rules and limited the number of players on a side to nine.\nBy the late 1800s ice hockey competed with lacrosse as Canada’s most popular sport. The first national hockey organization, the Amateur Hockey Association (AHA) of Canada (which limited players to seven a side), was formed in Montreal in 1885, and the first league was formed in Kingston during the same year, with four teams: the Kingston Hockey Club, Queen’s University, the Kingston Athletics, and the Royal Military College. Queen’s University scored a 3–1 victory over the Athletics in the first championship game.\nBy the opening of the 20th century, sticks were being manufactured, shin pads were worn, the goaltender began to wear a chest protector (borrowed from baseball), and arenas (still with natural ice and no heat for spectators) were being constructed throughout eastern Canada. In 1893 national attention was focused on the game when the Canadian governor-general, Frederick Arthur, Lord Stanley of Preston, donated a cup to be given annually to the top Canadian team. The three-foot-high silver cup became known as the Stanley Cup and was first awarded in 1892–93. (The first winner was the Montreal Amateur Athletic Association team, which also captured the Stanley Cup the following season by winning the initial challenge series to determine the Cup holder, which was the Cup-awarding format that Lord Stanley originally intended.) Since 1926 the cup has gone to the winner of the National Hockey League playoffs.\nIn 1899 the Canadian Amateur Hockey League was formed. All hockey in Canada at the time was “amateur,” it being “ungentlemanly” to admit to being paid for athletic services. Thus, the first acknowledged professional hockey team in the world was formed in the United States, in 1903, in Houghton, Michigan. The team, the Portage Lakers, was owned by a dentist named J.L. Gibson, who imported Canadian players. In 1904 Gibson formed the first acknowledged professional league, the International Pro Hockey League. Canada accepted professional hockey in 1908 when the Ontario Professional Hockey League was formed. By that time Canada had become the center of world hockey.\nThe National Hockey Association (NHA), the forerunner of the National Hockey League (NHL), was organized in 1910 and became the strongest hockey association in North America. Rising interest in the game created problems, however, for there were few artificial-ice rinks. In 1911 the Pacific Coast Hockey Association (PCHA) was formed by Joseph Patrick and his sons, who built two enclosed artificial-ice arenas, beginning a boom in the construction of artificial-ice rinks.\nThe PCHA became involved in a money and player war with the NHA. Although the NHA ultimately emerged as the stronger league, it was the PCHA that introduced many of the changes that improved the game. The only radical rule change adopted by the NHA was to reduce the number of players on a side to six, and that move was made to save money. The western league retained seven-man hockey, but it allowed the goalie to leap or dive to stop the puck. Under the previous rules, a goalie had had to remain stationary when making a save. The western league also changed the offside rule. Under the old rules, a player had been deemed offside if he was ahead of the puck carrier when he received a pass. The PCHA divided the ice into three zones by painting two blue lines across the surface and allowed forward passing in the center zone between the blue lines. This opened up the game and made it more exciting. Another innovation in the western league was the idea of the assist. Previously, only the goal scorer had been credited with a point. In the PCHA the player or players who set up his goal were credited with an assist. The first numbered uniforms also appeared in their league.\nLike some of its predecessors, the NHA had its dissenters. In a move to eject one of the league members, the NHA decided to disband and form a new league. The result was the creation in 1917 of the National Hockey League (NHL), which became the world’s foremost professional hockey league. In 1924 the first U.S. team, the Boston Bruins, joined the NHL. In 1925 the New York Americans and Pittsburgh Pirates were admitted, followed in 1926 by the New York Rangers, the Chicago Blackhawks, and the Detroit Cougars (later called the Red Wings). To stock the new teams, the NHL bought out the Patricks’ league in 1926 for $250,000. Among the players who shifted to Boston was Eddie Shore, known as a “rushing” defenseman, whose style helped change the game. He was one of the sport’s most ferocious and, many experts say, most skilled players, a forerunner of such future NHL players as Gordie Howe, who played mostly for the Detroit Red Wings. The Pittsburgh Pirates and the New York Americans eventually dropped out of the league, and, until the expansion of 1967, the NHL was composed of only six teams: the Rangers, the Bruins, the Blackhawks, the Red Wings, the Toronto Maple Leafs, and the Montreal Canadiens.\nIn 1967 the NHL undertook one of the greatest expansions in professional sports history when it doubled in size to 12 teams. A new 12-team league, the World Hockey Association (WHA), was formed in 1972, and the ensuing rivalry caused an escalation in players’ salaries. In 1979 the NHL, which had grown to 17 teams, merged with the WHA to become a 21-team league; by 2017, 31 teams played in the NHL. In 2004, owners locked out players, insisting that they accept a salary cap that would slow the rapid growth of payroll costs. The players rejected the owners’ demands, and the entire 2004–05 season was canceled. (The league resumed play in 2005–06 after the owners ultimately prevailed, and the NHL became the last of the major North American team-sport leagues to institute a salary cap.) The regular season consists of 82 games and determines the 16 teams that will qualify for the playoffs. The playoff winner is awarded the Stanley Cup.\nNHL individual awards are the Vezina Trophy, for the goalie voted best at his position by NHL managers; the William M. Jennings Trophy, for the goalie or goalies with the team permitting the fewest goals; the Calder Memorial Trophy, for the rookie of the year; the Hart Memorial Trophy, for the most valuable player; the James Norris Memorial Trophy, for the outstanding defenseman; the Art Ross Trophy, for the top point scorer; the Lady Byng Memorial Trophy, for the player best combining clean play with a high degree of skill; the Conn Smythe Trophy, for the playoffs’ outstanding performer; the Frank J. Selke Trophy, for the best defensive forward; the Jack Adams Award, for the coach of the year; the Bill Masterton Memorial Trophy, for the player who best exemplifies sportsmanship, perseverance, and dedication to hockey; and the Lester Patrick Trophy, for outstanding service to U.S. hockey.\nFor much of the 20th century, amateur athletes dominated international competition. League competition among amateurs in England began in 1903. The International Ice Hockey Federation (IIHF) was formed in Europe in 1908. Its five original members were Great Britain, Bohemia, Switzerland, France, and Belgium. The first European championship was held at Avants, Switzerland, in 1910, with Great Britain the winner. From that time the federation broadened its membership, taking applicants from the world over. Canada captured the first Olympic Games title in 1920 and, concurrently, the first IIHF world championship. Canada, which also won at the first Olympic Winter Games in 1924, dominated international competition until the emergence of the Soviet team in the early 1960s. The Soviets continued to be the most powerful team in international hockey until the 1990s and the dissolution of the Soviet Union.\nIn 1995 an agreement between the NHL, the NHL Players’ Association, and the IIHF ended amateur domination of international play as professional athletes were allowed to compete at the Olympics and World Cup championships. Although the decision had little effect on the world tournament, the Winter Games competition underwent numerous changes. Given the high visibility of professional players and their skills, selection to the Canadian, U.S., Russian, Finnish, Swedish, and Czech Olympic teams was no longer based on tryouts but rather on the decisions of hockey personnel from each country’s national hockey governing body. The six \"dream teams\" were automatically placed in the final round of eight; the two remaining slots were filled by the winners of a qualifying round. The NHL suspended play for a period of 16 days in 1998 so professional players could make their Olympic debut in Nagano, Japan, and it continued to temporarily stop the season for Olympic play thereafter.\nThough sometimes considered primarily a male sport, hockey has been played by women for more than 120 years. The first all-female game was in Barrie, Ontario, Canada, in 1892, and the first world championship was held in 1990. Recognizing the growing popularity of the sport, the International Olympic Committee added women’s ice hockey to its 1998 schedule at Nagano, where the sport made its first Winter Games appearance.\nA number of women’s hockey leagues have been formed around the world. In North America, the premier organization is the Professional Women’s Hockey League (established 2023).",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/question/Should-university-student-loan-debt-be-eliminated-via-forgiveness-or-bankruptcy",
    "content": "Whether university student loan debt should be eliminated via forgiveness or bankruptcy is widely debated. Some argue forgiveness would boost the economy, help rectify racial inequity, and foster a healthier citizenry, while denying student loan debtors the benefits of bankruptcy--benefits that other debtors have access to--is unfair. Others argue that people must be held responsible for their personal economic choices, that forgiveness would disproportionately help more financially secure university graduates and would only be a temporary bandage for the much larger problem of inflated university costs, while bankruptcy would allow borrowers to abuse the loan system and encourage universities to increase tuition. For more on the student loan debt debate, visit ProCon.org.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/dictionary/dispute",
    "content": "",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/place/New-Kensington",
    "content": "New Kensington, city, Westmoreland county, western Pennsylvania, U.S., on the Allegheny River, and near the Pennsylvania Turnpike, just northeast of Pittsburgh. Established in 1891 by a group of Pittsburgh merchants interested in establishing a plant for reducing aluminum, it was laid out on the site of Fort Crawford (built during the American Revolutionary period) and named for Kensington in London. Incorporated as a borough in 1892, it absorbed neighbouring Parnassus in 1931 and became a city in 1934.\nNew Kensington’s manufactures include steel, glass, textiles, and metal and petroleum products. Although aluminum is no longer produced in the city, the Aluminum Company of America has an aluminium research-and-development centre there. The New Kensington campus of Pennsylvania State University (Penn State New Kensington) was opened in 1958. Pop. (2000) 14,701; (2010) 13,116.\nAllegheny, county, southwestern Pennsylvania, U.S., consisting of a hilly region on the Allegheny Plateau bounded to the southeast by the Monongahela and Youghiogheny rivers and to the northeast by the Allegheny River. The Ohio, Allegheny, and Monongahela rivers converge in the centre of the county to form an area known as the Golden Triangle; this was a strategic point of contention between the French and the English, who fortified the area with Fort Duquesne (1754) and Fort Pitt (1761), respectively. With the defeat of Pontiac’s warriors (1763), the area opened up to settlers who founded Pittsburgh (1764), which became the county seat when Allegheny county was formed in 1788. The county’s name is derived from the Delaware Indian word oolikhanna, meaning “good river.” The Triangle is now Pittsburgh’s central business district and the location of the popular Point State Park.\nPittsburgh, which was linked to outside markets by the Pennsylvania Canal and the Portage Railroad (both completed in 1834), became the nation’s centre for the iron, steel, and glass industries in the 19th century. The city is also the home of the University of Pittsburgh (1787), Duquesne University (1878), and Carnegie Mellon University (1900), as well as the museums, library, and music hall of The Carnegie, formerly the Carnegie Institute.\nOther communities include Bethel Park, Monroeville, McKeesport, Baldwin, and Wilkinsburg. The main economic activities are services (health care, business, education, and engineering), retail trade, manufacturing (steel and industrial machinery), and finance. Area 730 square miles (1,891 square km). Pop. (2000) 1,281,666; (2010) 1,223,348.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/science/population-growth",
    "content": "population growth, in population ecology, a change in the number of members of a certain plant or animal species in a particular location during a particular time period. Factors affecting population growth include fertility, mortality, and, in animals, migration—i.e., immigration to or emigration from a particular location. The average change in a population over time is referred to as the population growth rate. A positive growth rate indicates a population increase, and a negative growth rate indicates a population decrease. The upper limit of a population in a given environment, referred to as the environment’s carrying capacity, is determined by the amount and availability of resources that are life-sustaining for that population.\nPopulation growth rates in a given location and time period can be calculated by subtracting population loss, or the combined rates of mortality and emigration, from population gain, or the combined rates of fertility and immigration. Fertility is the number of offspring produced on average by an individual species member under certain environmental conditions over a period of time. (Fertility is not to be confused with fecundity—the theoretical maximum number of offspring that can be produced by a species member in a given time period. The sex ratio and age structure of a population affect fertility rates, because they constrain the number of individuals capable of reproducing.\nThe natural environment plays a crucial role in population growth through its carrying capacity. The life-sustaining resources in an environment are limited and can be reduced for individual species members by greater population density and competition from other species, among other factors. Moreover, fertility, mortality, and migration are all affected by food availability, mate availability, and environmental stress factors such as pollution and natural disasters.\nPopulation growth dynamics may be graphically depicted in an S-shaped curve, known as a logistic curve, as depicted in Figure 1. The logistic curve (right) represents an initial lag in growth, a burst of exponential growth, and finally a decline in population growth. When population density is high, mortality tends to increase because of competition for resources, predation, or increased disease transmission, resulting in the plateau in growth at the end of the curve. Population growth rates may also fluctuate in correlation with seasonal variations in the environment. For example, in deep lakes, a spring thaw causes colder, deeper waters to rise to the lake’s surface, releasing nutrients that then cause bursts in the growth of plankton, including algae, bacteria, and protozoans (see water bloom).\nThe population growth of different species may also be determined by predator-prey relationships—for example, following an increase in the population of zooplankton (minute aquatic animals), the population of their algae prey may decrease or grow at a slower rate. This results in an oscillating population pattern, such as the cyclical fluctuation of the snowshoe hare and lynx populations, as depicted in Figure 2.\nThe world’s human population experienced exponential growth from the 18th century, but growth rates have been declining since the second half of the 20th century. Although population growth rates vary significantly between countries and some regions continue to experience increasing growth rates, the overall rate of growth is decreasing.\nThe world’s human population reached 8 billion in November 2022 and is predicted to peak at 10.4 billion by 2080 and to remain at that level until the end of the century. Although human mortality rates have been decreasing on average, the main reason for the decreasing population growth rate is lower fertility. Fertility varies based on reproductive behaviour patterns, which in turn depend upon factors such as cultural traditions, socioeconomic conditions, access to contraception, and ecological variables, such as population density.\nDecreasing mortality among humans in recent decades has changed the age structure of the population. It has been predicted by the Population Division of the United Nations Department of Economic and Social Affairs that by 2050 the world’s population of people age 65 or older will be twice as large as the population of children under age 5. That means that proportionately fewer people will be of reproductive age in the coming decades, which will contribute to a decline in population growth.\ncarrying capacity, the average population density or population size of a species below which its numbers tend to increase and above which its numbers tend to decrease because of shortages of resources. The carrying capacity is different for each species in a habitat because of that species’ particular food, shelter, and social requirements.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/place/Monongahela-River",
    "content": "Monongahela River, river formed by the confluence of the Tygart and West Fork rivers in Marion county, West Virginia, U.S. It flows 128 miles (206 km) in a northerly direction past Morgantown into Pennsylvania, past Brownsville and Charleroi, joining the Allegheny River at Pittsburgh to become a major headwater of the Ohio River. In its upper reaches the river furnishes hydroelectric power to several cities. The Monongahela, made navigable by means of locks, is a major barge route, carrying coal, coke, iron, and steel.\nAppalachian Mountains, great highland system of North America, the eastern counterpart of the Rocky Mountains. Extending for almost 2,000 miles (3,200 km) from the Canadian province of Newfoundland and Labrador to central Alabama in the United States, the Appalachian Mountains form a natural barrier between the eastern Coastal Plain and the vast Interior Lowlands of North America. As a result, they have played a vital role in the settlement and development of the entire continent. They combine a heritage of natural beauty and a distinctive regional culture with contemporary problems of economic deprivation and environmental deterioration.\nThe system may be divided into three large physiographic regions: northern, central, and southern Appalachia. These include such mountains as, in the northern area, the Shickshocks (French: Chic-Chocs) and the Notre Dame ranges in Quebec; the Long Range on the island of Newfoundland; the great monadnock (isolated hill of bedrock) of Mount Katahdin in Maine; the White Mountains of New Hampshire; and Vermont’s Green Mountains, which become the Berkshire Hills in Massachusetts, Connecticut, and eastern New York. New York’s Catskill Mountains are in central Appalachia, as are the beginnings of the Blue Ridge range in southern Pennsylvania and the Allegheny Mountains, which rise in southwestern New York and cover parts of western Pennsylvania, western Maryland, and eastern Ohio before merging into the third, or southern, region. This area includes the Alleghenies of West Virginia and Virginia; the Blue Ridge range, extending across Virginia and western North Carolina, the northwestern tip of South Carolina, and the northeastern corner of Georgia; the Unaka Mountains in southwestern Virginia, eastern Tennessee, and western North Carolina (of which the Great Smoky Mountains are a part); and the Cumberland Mountains of eastern Kentucky, southwestern West Virginia, southwestern Virginia, eastern Tennessee, and northern Alabama.\nThe highest elevations in the Appalachians are in the northern division, with Maine’s Mount Katahdin (5,268 feet [1,606 metres]), New Hampshire’s Mount Washington (6,288 feet), and other pinnacles in the White Mountains rising above 5,000 feet (1,525 metres), and in the southern region, where peaks of the North Carolina Black Mountains and the Tennessee–North Carolina Great Smoky Mountains rise above 6,000 feet (1,825 metres) and the entire system reaches its highest summit, on Mount Mitchell (6,684 feet [2,037 metres]).\nA distinctive feature of the system is the Great Appalachian Valley. It includes the St. Lawrence River valley in Canada and the Kittatinny, Cumberland, Shenandoah, and Tennessee valleys in the United States; the latter is the site of the world-famous Tennessee Valley Authority (TVA), a government agency for natural resource conservation, power production, and regional development.\nIn the area known geologically as “New” Appalachia, especially where there are softer limestone rocks that yield to the constant solution by water and weak acids, numerous caves are a distinctive feature of the physiography. The chief caverns lie within or border the Great Valley region of Pennsylvania, Maryland, West Virginia, Virginia, and Tennessee. Caverns of the Shenandoah Valley in Virginia provide well-known and dramatic examples of underground passages, rooms, watercourses, formations, and other cave features that honeycomb much of the land below central and southern Appalachia.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/Animals-Nature",
    "content": "The Difference Between Alligators and Crocodiles\n9 of the World’s Deadliest Spiders\nTitanosaurs: 8 of the World's Biggest Dinosaurs\nDo Plants Feel Pain?\nThese shaggy-haired bears, native to Europe, Asia, and North America, include species such as the Grizzly and the Kodiak bear, which weighs up to 720kg (1,600 pounds) and is often considered the world's largest carnivore. \nAll About the Brown Bear Family\nJellyfish\nBaobab Trees\nLions\nGymnosperms\nTurtles\nSharks",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/place/Carnegie-Library-of-Pittsburgh",
    "content": "…with the organization are the Carnegie Library of Pittsburgh, which contains more than 3.3 million volumes, and the Carnegie Music Hall. The Pittsburgh Symphony Orchestra performs at Heinz Hall, a restored movie theatre.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/place/Pittsburgh/additional-info",
    "content": "",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/money/Heinz-Company",
    "content": "Heinz is a division and brand of the Kraft Heinz Company, a major manufacturer of processed foods and beverages that was formed by the 2015 merger of H.J. Heinz Holding Corporation and Kraft Foods Group. Heinz is known for its “57 Varieties” slogan, which was devised in 1896, though the company was actually then producing more than 60 products, including its famous Heinz ketchup (first sold as “catsup” in 1876); he reportedly chose the number 57 because 5 and 7 were his and his wife’s lucky numbers, respectively. By the early 21st century, the company was marketing more than 5,700 products. The Heinz headquarters are in Pittsburgh, PA.\nThe Heinz Company was founded in Sharpsburg, Pennsylvania, in 1869 by Henry John Heinz (1844–1919), who was later to become nationally known as the “Pickle King.” Heinz had become interested in selling food when he was a child; by age 16 he had several employees working to cultivate the hotbeds and gardens he had built and to deliver his produce to Pittsburgh grocers.\nHis first company, a partnership with two other men, was formed to prepare and market horseradish, sauerkraut, vinegar, and pickles. Although the company did not survive the severe economic downturn that started with the Panic of 1873, Heinz reorganized it in 1876 and built it into a major national company by the end of the century. By 1905 it had become the H.J. Heinz Company, the largest producer of pickles, vinegar, and ketchup (catsup) in the United States. By 1919 the company had more than 6,000 employees and 25 factories.\nHeinz was an astute marketer of his products; he set up a massive electric sign in New York City (1900) to advertise his firm’s relishes, condiments, and pickles (see advertising). Heinz was a progressive employer for his time and was one of the few food processors to support a federal Pure Food Act. The corporation was headed by members of the Heinz family until 1969.\nIn 1965, the Heinz Company acquired Ore-Ida Foods, producer of Tater Tots and other frozen potato products. Soon afterward the company began a period of global expansion that continued through the early 21st century. In addition to acquiring food-processing companies and established subsidiaries in China, Africa, central and eastern Europe, and the Pacific Rim, Heinz acquired:\nThe Kraft Heinz megamerger got off to an impressive start in terms of share price growth and profitability. But within a couple of years, the bloom had fallen off the ketchup-colored rose. Read more about the Kraft Heinz merger and aftermath.\nIn 1999 Heinz sold Weight Watchers (later called WW), although it continued to market the brand until 2018. In 2002 the company sold several underperforming North American food and pet food businesses, including StarKist seafood, to the Del Monte Food Company. Heinz completed a takeover of the Australian food and drink maker Golden Circle in 2008.\nAlthough the H.J. Heinz Company company got its start selling pickles, vinegar, and ketchup, by the time Berkshire Hathaway and private equity firm 3G Capital collaborated to take it private in 2013, the company produced and/or marketed several thousand brands across the world. Two years later, in 2015, Heinz’s holding company merged with Kraft Foods Group to form Kraft Heinz, with Heinz becoming a division and brand within the newly formed conglomerate.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/Biographies",
    "content": "Famous West Indians in Sports and Entertainment\nClaudia Sheinbaum \nHow Bob Odenkirk Became Bob Odenkirk\nWho Is Hanukkah Harry?\nThe Polish-born French physicist was famous for her work on radioactivity, becoming the first woman to win a Nobel Prize in 1903 (for Physics), and later becoming the only woman to win a Nobel in two different fields when, in 1911, she won the Nobel Prize for Chemistry.\nThe Extraordinary Life of Marie Curie\nCharles Darwin\nRoyal Weddings\nAristotle",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/sports/baseball",
    "content": "baseball, game played with a bat, a ball, and gloves between two teams of nine players each on a field with four white bases laid out in a diamond (i.e., a square oriented so that its diagonal line is vertical). Teams alternate positions as batters (offense) and fielders (defense), exchanging places when three members of the batting team are “put out.” As batters, players try to hit the ball out of the reach of the fielding team and make a complete circuit around the bases for a “run.” The team that scores the most runs in nine innings (times at bat) wins the game.\nThe United States is credited with developing several popular sports, including some (such as baseball, gridiron football, and basketball) that have large fan bases and, to varying degrees, have been adopted internationally. But baseball, despite the spread of the game throughout the globe and the growing influence of Asian and Latin American leagues and players, is the sport that Americans still recognize as their “national pastime.” The game has long been woven into the fabric of American life and identity. “It’s our game,” exclaimed the poet Walt Whitman more than a century ago, “that’s the chief fact in connection with it: America’s game.” He went on to explain that baseball\nhas the snap, go, fling of the American atmosphere—it belongs as much to our institutions, fits into them as significantly, as our constitutions, laws: is just as important in the sum total of our historic life. It is the place where memory gathers.\nPerhaps Whitman exaggerated baseball’s importance to and its congruency with life in the United States, but few would argue the contrary, that baseball has been merely a simple or an occasional diversion.\nIt was nationalistic sentiment that helped to make baseball “America’s game.” In the quest to obtain greater cultural autonomy, Americans yearned for a sport they could claim as exclusively their own. Just as the English had cricket and the Germans their turnvereins (gymnastic clubs), a sporting newspaper declared as early as 1857 that Americans should have a “game that could be termed a ‘Native American Sport.’ ” A powerful confirmation of baseball as the sport to fill that need came in 1907 when a special commission appointed by A.G. Spalding, a sporting goods magnate who had formerly been a star pitcher and an executive with a baseball team, reported that baseball owed absolutely nothing to England and the children’s game of rounders. Instead, the commission claimed that, to the best of its knowledge (a knowledge based on flimsy research and self-serving logic), baseball had been invented by Abner Doubleday at Cooperstown, New York, in 1839. This origin myth was perpetuated for decades.\nIn a country comprising a multiplicity of ethnic and religious groups, one without a monarchy, an aristocracy, or a long and mythic past, the experience of playing, watching, and talking about baseball games became one of the nation’s great common denominators. It provided, in the perceptive words of British novelist Virginia Woolf, “a centre, a meeting place for the divers activities of a people whom a vast continent isolates [and] whom no tradition controls.” No matter where one lived, the “hit-and-run,” the “double play,” and the “sacrifice bunt” were carried out the same way. The unifying power of baseball in the United States was evident in the Depression-ravaged 1930s, when a group of Cooperstown’s businessmen along with officials from the major leagues established the National Baseball Hall of Fame and Museum. The Hall of Fame became a quasi-religious shrine for many Americans, and, since its founding, millions of fans have made “pilgrimages” to Cooperstown, where they have observed the “relics”—old bats, balls, and uniforms—of bygone heroes.\nBaseball also reshaped the nation’s calendar. With the rise of industrialization, the standardized clock time of the office or factory robbed people of the earlier experience of time in its rich associations with the daylight hours, the natural rhythms of the seasons, and the traditional church calendar. Yet, for Americans, the opening of the baseball training season signaled the arrival of spring, regular-season play meant summer, and the World Series marked the arrival of fall. In the winter, baseball fans participated in “hot stove leagues,” reminiscing about past games and greats and speculating about what the next season had to offer.\nThe World Series, inaugurated in 1903 and pitting the champions of the American and National Leagues in a postseason play-off, quickly took its place alongside the Fourth of July and Christmas as one of the most popular annual rites. The series was, said Everybody’s Magazine in 1911, “the very quintessence and consummation of the Most Perfect Thing in America.” Each fall it absorbed the entire nation.\nBaseball terms and phrases, such as “He threw me a curve,” “Her presentation covered all the bases,” and “He’s really out in left field,” soon became part of the national vocabulary, so entrenched is baseball in the ordinary conversation of Americans. During the administration of President George H.W. Bush, a baseball player during his years at Yale University, the foreign press struggled to translate the president’s routine use of baseball metaphors. As early as the 1850s, baseball images began to appear in periodicals, and, in the 20th century, popular illustrator Norman Rockwell often used baseball as the subject for his The Saturday Evening Post covers. “Casey at the Bat” and “Take Me Out to the Ballgame” remain among the best-known poems and songs, respectively, among Americans. Novelists and filmmakers frequently have turned to baseball motifs. After the mid-20th century, at the very time baseball at the grassroots level had begun a perceptible descent, baseball fiction proliferated. American colleges and universities even began to offer courses on baseball literature, and baseball films likewise proliferated. In 1994 the Public Broadcasting System released Ken Burns’s nostalgic Baseball, arguably the most monumental historical television documentary ever made.\nWhile baseball possessed enormous integrative powers, the game’s history also has been interwoven with and reflective of major social and cultural cleavages. Until the first decades of the 20th century, middle-class Evangelical Protestants viewed the sport with profound suspicion. They associated baseball, or at least the professional version of the game, with ne’er-do-wells, immigrants, the working class, drinking, gambling, and general rowdiness. Conversely, these very qualities provided a foothold for the upward ascent of ethnic groups from the nation’s ghettos. Usually encountering less discrimination in baseball (as well as in other venues of commercial entertainment) than they did in the more “respectable” occupations, in the 19th century Irish and German Americans were so conspicuous in professional baseball that some observers wondered if they had a special capacity for playing the game.\nFor a brief time in the 1880s, before racial segregation became the norm in the United States, Black players competed with white players in professional baseball. After that period, however, Black players had to carve out a separate world of baseball. Dozens of Black teams faced local semiprofessional teams while barnstorming throughout the United States, Canada, Mexico, and the Caribbean. Despite playing a high quality of baseball, the players frequently engaged in various forms of clowning that perpetuated prevailing stereotypes of Black people to appeal to spectators. From the 1920s until the ’50s, separate Black professional leagues—the Negro leagues—existed as well, but in 1947 Jackie Robinson crossed the long-standing color bar in major league baseball. Because baseball was the national game, its racial integration was of enormous symbolic importance in the United States; indeed, it preceded the U.S. Supreme Court’s decision ending racial segregation in the schools (in 1954 in Brown v. Board of Education of Topeka) and helped to usher in the civil rights movement of the 1950s and ’60s. Moreover, in the 1980s and ’90s a huge influx of Hispanics into professional baseball reflected the country’s changing ethnic composition.\nBaseball likewise contributed to the shaping of American conceptions of gender roles. Although women were playing baseball as early as the 1860s, their involvement in the sport was confined for the most part to the role of spectator. To counter the game’s reputation for rowdiness, baseball promoters took pains to encourage women to attend. “The presence of an assemblage of ladies purifies the moral atmosphere of a baseball gathering,” reported the Baseball Chronicle, “repressing as it does, all the out-burst of intemperate language which the excitement of a contest so frequently induces.” When women played on barnstorming teams in the 19th and the first half of the 20th century, the press routinely referred to them as “Amazons,” “freaks,” or “frauds.” In 1943, during World War II, when it was feared that professional baseball might be forced to close down, the All-American Girls Professional Baseball League made its debut. After having provided more than 600 women an opportunity to play baseball and to entertain several million fans, the league folded in 1954.\nBut, even if unable to heal conflicts arising from fundamental social divisions, baseball exhibited an extraordinary capacity for fostering ties. In the 1850s, young artisans and clerks, frequently displaced in the city and finding their way of life changing rapidly in the midst of the Industrial Revolution, conceived of themselves as members of what was known as the “base ball fraternity.” Like the volunteer fire departments and militia units of the day, they donned special uniforms, developed their own rituals, and, in playing baseball, shared powerful common experiences. Playing and watching baseball contests also strengthened occupational, ethnic, and racial identities. Butchers, typesetters, draymen, bricklayers, and even clergymen organized baseball clubs. So did Irish Americans, German Americans, and African Americans.\nProfessional baseball nourished and deepened urban identities. “If we are ahead of the big city [New York] in nothing else,” crowed the Brooklyn Eagle as early as 1862, “we can beat her in baseball.” Fans invested their emotions in their professional representative nines. “A deep gloom settled over the city,” reported a Chicago newspaper in 1875 after the local White Stockings had been defeated by the St. Louis (Missouri) Brown Stockings. “Friends refused to recognize friends, lovers became estranged, and business was suspended.” Even in the late 20th century, in an age more given to cynicism, the successes and failures of professional teams continued to evoke strong feelings among local residents. For example, during the 1990s, after having experienced urban decay and demoralization in the previous two decades, Cleveland experienced a great civic revival fueled in part by the success of the Indians baseball team.\nThe significance of specific baseball teams and individual players extended beyond the localities that they represented. The New York Yankees, who in the first half of the 20th century were the quintessential representatives of the big city, of the East, of urban America with its sophistication, and of ethnic and religious heterogeneity, became synonymous with supernal success, while the St. Louis Cardinals emerged as the quintessential champions of the Midwest, of small towns and the farms, of rural America with its simplicity, rusticity, and old-stock Protestant homogeneity. In the 1920s Babe Ruth became the diamond’s colossal demigod. To those toiling on assembly lines or sitting at their desks in corporate bureaucracies, Ruth embodied America’s continuing faith in upward social mobility. His mighty home runs furnished vivid proof that men remained masters of their own destinies and that they could still rise from mean, vulgar beginnings to fame and fortune. For African Americans, Black stars such as Satchel Paige and Josh Gibson furnished equally compelling models of individual inspiration and success.\nBaseball parks became important local civic monuments and repositories of collective memories. The first parks had been jerry-built, flimsy wooden structures, but between 1909 and 1923 some 15 major league clubs constructed new, more permanent parks of steel and concrete. These edifices were akin to the great public buildings, skyscrapers, and railway terminals of the day; local residents proudly pointed to them as evidence of their city’s size and its achievements.\nSeeing them as retreats from the noise, dirt, and squalor of the industrial city, the owners gave the first parks pastoral names—Ebbets Field, Sportsman’s Park, and the Polo Grounds—but, with the construction of symmetrical, multisports facilities in the 1960s and ’70s, urban and futuristic names such as Astrodome and Kingdome predominated. In a new park-building era in the 1990s, designers sought to recapture the ambience of earlier times by designing “retro parks,” a term that was something of an oxymoron in that, while the new parks offered the fan the intimacy of the old-time parks, they simultaneously provided modern conveniences such as escalators, climate-controlled lounges, high-tech audiovisual systems, Disneyesque play areas for children, and space for numerous retail outlets. The increasing corporate influence on the game was reflected in park names such as Network Associates Stadium and Bank One Ballpark.\nAfter about the mid-20th century, baseball’s claim to being America’s game rested on more precarious foundations than in the past. The sport faced potent competition, not only from other professional sports (especially gridiron football) but even more from a massive conversion of Americans from public to private, at-home diversions. Attendance as a percentage of population fell at all levels of baseball, the minor leagues became a shell of their former selves, and hundreds of semipro and amateur teams folded. In the 1990s, player strikes, free agency, disparities in competition, and the rising cost of attending games added to the woes of major league baseball. Yet, baseball continued to exhibit a remarkable resiliency; attendance at professional games improved, and attendance at minor league games was close to World War II records by the end of the century. As the 21st century opened, baseball still faced serious problems, but the sport was gaining in popularity around the world, and a strong case could still be made for baseball holding a special place in the hearts and minds of the American people.\nThe term base-ball can be dated to 1744, in John Newbery’s children’s book A Little Pretty Pocket-Book. The book has a brief poem and an illustration depicting a game called base-ball. Interestingly, the bases in the illustration are marked by posts instead of the bags and flat home plate now so familiar in the game. The book was extremely popular in England and was reprinted in North America in 1762 (New York) and 1787 (Massachusetts).\nMany other early references to bat-and-ball games involving bases are known: a 1749 British newspaper that refers to Frederick Louis, prince of Wales, playing “Bass-Ball” in Surrey, England; “playing at base” at the American army camp at Valley Forge in 1778; the forbidding of students to “play with balls and sticks” on the common of Princeton College in 1787; a note in the memoirs of Thurlow Weed, an upstate New York newspaper editor and politician, of a baseball club organized about 1825; a newspaper report that the Rochester (New York) Baseball Club had about 50 members at practice in the 1820s; and a reminiscence of the elder Oliver Wendell Holmes concerning his Harvard days in the late 1820s, stating that he played a good deal of ball at college.\nThe Boy’s Own Book (1828), a frequently reprinted book on English sports played by boys of the time, included in its second edition a chapter on the game of rounders. As described there, rounders had many resemblances to the modern game of baseball: it was played on a diamond-shaped infield with a base at each corner, the fourth being that at which the batter originally stood and to which he had to advance to score a run. When a batter hit a pitched ball through or over the infield, he could run. A ball hit elsewhere was foul, and he could not run. Three missed strikes at the ball meant the batter was out. A batted ball caught on the fly put the batter out. One notable difference from baseball was that, in rounders, when a ball hit on the ground was fielded, the fielder put the runner out by hitting him with the thrown ball; the same was true with a runner caught off base. Illustrations show flat stones used as bases and a second catcher behind the first, perhaps to catch foul balls. The descent of baseball from rounders seems indisputably clear-cut. The first American account of rounders was in The Book of Sports (1834) by Robin Carver, who credits The Boy’s Own Book as his source but calls the game base, or goal, ball.\nIn 1845, according to baseball legend, Alexander J. Cartwright, an amateur player in New York City, organized the New York Knickerbocker Base Ball Club, which formulated a set of rules for baseball, many of which still remain. The rules were much like those for rounders, but with a significant change in that the runner was put out not by being hit with the thrown ball but by being tagged with it. This change no doubt led to the substitution of a harder ball, which made possible a larger-scale game.\nThe adoption of these rules by the Knickerbockers and other amateur club teams in the New York City area led to an increased popularity of the game. The old game with the soft ball continued to be popular in and around Boston; a Philadelphia club that had played the old game since 1833 did not adopt the Knickerbocker or New York version of the game until 1860. Until the American Civil War (1861–65), the two versions of the game were called the Massachusetts game (using the soft ball) and the New York game (using the hard ball). During the Civil War, soldiers from New York and New Jersey taught their game to others, and after the war the New York game became predominant.\nIn 1854 a revision of the rules prescribed the weight and size of the ball, along with the dimensions of the infield, specifications that have not been significantly altered since that time. The National Association of Base Ball Players was organized in 1857, comprising clubs from New York City and vicinity. In 1859 Washington, D.C., organized a club, and in the next year clubs were formed in Lowell, Massachusetts; Allegheny, Pennsylvania; and Hartford, Connecticut. The game continued to spread after the Civil War—to Maine, Kentucky, and Oregon. Baseball was on its way to becoming the national pastime. It was widely played outside the cities, but the big-city clubs were the dominant force. In 1865 a convention was called to confirm the rules and the amateur status of baseball and brought together 91 amateur teams from such cities as St. Louis; Chattanooga, Tennessee; Louisville, Kentucky; Washington, D.C.; Boston; and Philadelphia.\nTwo important developments in the history of baseball occurred in the post-Civil War period: the spread of the sport to Latin America and Asia (discussed later) and the professionalization of the sport in the United States. The early baseball clubs such as the New York Knickerbockers were clubs in the true sense of the word: members paid dues, the emphasis was on fraternity and socializing, and baseball games were played largely among members. But the growth of baseball’s popularity soon attracted commercial interest. In 1862 William Cammeyer of Brooklyn constructed an enclosed baseball field with stands and charged admission to games. Following the Civil War, this practice quickly spread, and clubs soon learned that games with rival clubs and tournaments drew larger crowds and brought prestige to the winners. The interclub games attracted the interest and influence of gamblers. With a new emphasis on external competition, clubs felt pressure to field quality teams. Players began to specialize in playing a single position, and field time was given over to a club’s top players so they could practice. Professionalism began to appear about 1865–66 as some teams hired skilled players on a per game basis. Players either were paid for playing or were compensated with jobs that required little or no actual work. Amateurs resented these practices and the gambling and bribery that often accompanied them, but the larger public was enthralled by the intense competition and the rivalries that developed. The first publicly announced all-professional team, the Cincinnati (Ohio) Red Stockings, was organized in 1869; it toured that year, playing from New York City to San Francisco and winning some 56 games and tying 1. The team’s success, especially against the hallowed clubs of New York, resulted in national notoriety and proved the superior skill of professional players. The desire of many other cities and teams to win such acclaim guaranteed the professionalization of the game, though many players remained nominally in the amateur National Association of Base Ball Players until the amateurs withdrew in 1871. Thereafter professional teams largely controlled the development of the sport.\nThe National Association of Professional Base Ball Players was formed in 1871. The founding teams were the Philadelphia Athletics; the Chicago White Stockings (who would also play as the Chicago Colts and the Chicago Orphans before becoming the Cubs—the American League Chicago White Sox were not formed until 1900); the Brooklyn (New York) Eckfords; the Cleveland (Ohio) Forest Citys; the Forest Citys of Rockford, Illinois; the Haymakers of Troy, New York; the Kekiongas of Fort Wayne, Indiana; the Olympics of Washington, D.C.; and the Mutuals of New York City. The league disbanded in 1876 with the founding of the rival National League of Professional Baseball Clubs. The change from a players’ association to one of clubs was particularly significant. The teams making up the new league represented Philadelphia, Hartford (Connecticut), Boston, Chicago, Cincinnati, Louisville (Kentucky), St. Louis, and New York City. When William Hulbert, president of the league (1877–82), expelled four players for dishonesty, the reputation of baseball as an institution was significantly enhanced.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/technology/steel",
    "content": "steel, alloy of iron and carbon in which the carbon content ranges up to 2 percent (with a higher carbon content, the material is defined as cast iron). By far the most widely used material for building the world’s infrastructure and industries, it is used to fabricate everything from sewing needles to oil tankers. In addition, the tools required to build and manufacture such articles are also made of steel. As an indication of the relative importance of this material, in 2013 the world’s raw steel production was about 1.6 billion tons, while production of the next most important engineering metal, aluminum, was about 47 million tons. (For a list of steel production by country, see below World steel production.) The main reasons for the popularity of steel are the relatively low cost of making, forming, and processing it, the abundance of its two raw materials (iron ore and scrap), and its unparalleled range of mechanical properties.\nThe major component of steel is iron, a metal that in its pure state is not much harder than copper. Omitting very extreme cases, iron in its solid state is, like all other metals, polycrystalline—that is, it consists of many crystals that join one another on their boundaries. A crystal is a well-ordered arrangement of atoms that can best be pictured as spheres touching one another. They are ordered in planes, called lattices, which penetrate one another in specific ways. For iron, the lattice arrangement can best be visualized by a unit cube with eight iron atoms at its corners. Important for the uniqueness of steel is the allotropy of iron—that is, its existence in two crystalline forms. In the body-centred cubic (bcc) arrangement, there is an additional iron atom in the centre of each cube. In the face-centred cubic (fcc) arrangement, there is one additional iron atom at the centre of each of the six faces of the unit cube. It is significant that the sides of the face-centred cube, or the distances between neighbouring lattices in the fcc arrangement, are about 25 percent larger than in the bcc arrangement; this means that there is more space in the fcc than in the bcc structure to keep foreign (i.e., alloying) atoms in solid solution.\nIron has its bcc allotropy below 912° C (1,674° F) and from 1,394° C (2,541° F) up to its melting point of 1,538° C (2,800° F). Referred to as ferrite, iron in its bcc formation is also called alpha iron in the lower temperature range and delta iron in the higher temperature zone. Between 912° and 1,394° C iron is in its fcc order, which is called austenite or gamma iron. The allotropic behaviour of iron is retained with few exceptions in steel, even when the alloy contains considerable amounts of other elements.\nThere is also the term beta iron, which refers not to mechanical properties but rather to the strong magnetic characteristics of iron. Below 770° C (1,420° F), iron is ferromagnetic; the temperature above which it loses this property is often called the Curie point.\nIn its pure form, iron is soft and generally not useful as an engineering material; the principal method of strengthening it and converting it into steel is by adding small amounts of carbon. In solid steel, carbon is generally found in two forms. Either it is in solid solution in austenite and ferrite or it is found as a carbide. The carbide form can be iron carbide (Fe3C, known as cementite), or it can be a carbide of an alloying element such as titanium. (On the other hand, in gray iron, carbon appears as flakes or clusters of graphite, owing to the presence of silicon, which suppresses carbide formation.)\nThe effects of carbon are best illustrated by an iron-carbon equilibrium diagram. The A-B-C line represents the liquidus points (i.e., the temperatures at which molten iron begins to solidify), and the H-J-E-C line represents the solidus points (at which solidification is completed). The A-B-C line indicates that solidification temperatures decrease as the carbon content of an iron melt is increased. (This explains why gray iron, which contains more than 2 percent carbon, is processed at much lower temperatures than steel.) Molten steel containing, for example, a carbon content of 0.77 percent (shown by the vertical dashed line in the figure) begins to solidify at about 1,475° C (2,660° F) and is completely solid at about 1,400° C (2,550° F). From this point down, the iron crystals are all in an austenitic—i.e., fcc—arrangement and contain all of the carbon in solid solution. Cooling further, a dramatic change takes place at about 727° C (1,341° F) when the austenite crystals transform into a fine lamellar structure consisting of alternating platelets of ferrite and iron carbide. This microstructure is called pearlite, and the change is called the eutectoidic transformation. Pearlite has a diamond pyramid hardness (DPH) of approximately 200 kilograms-force per square millimetre (285,000 pounds per square inch), compared with a DPH of 70 kilograms-force per square millimetre for pure iron. Cooling steel with a lower carbon content (e.g., 0.25 percent) results in a microstructure containing about 50 percent pearlite and 50 percent ferrite; this is softer than pearlite, with a DPH of about 130. Steel with more than 0.77 percent carbon—for instance, 1.05 percent—contains in its microstructure pearlite and cementite; it is harder than pearlite and may have a DPH of 250.\nAdjusting the carbon content is the simplest way to change the mechanical properties of steel. Additional changes are made possible by heat-treating—for instance, by accelerating the rate of cooling through the austenite-to-ferrite transformation point, shown by the P-S-K line in the figure. (This transformation is also called the Ar1 transformation, r standing for refroidissement, or “cooling.”) Increasing the cooling rate of pearlitic steel (0.77 percent carbon) to about 200° C per minute generates a DPH of about 300, and cooling at 400° C per minute raises the DPH to about 400. The reason for this increasing hardness is the formation of a finer pearlite and ferrite microstructure than can be obtained during slow cooling in ambient air. In principle, when steel cools quickly, there is less time for carbon atoms to move through the lattices and form larger carbides. Cooling even faster—for instance, by quenching the steel at about 1,000° C per minute—results in a complete depression of carbide formation and forces the undercooled ferrite to hold a large amount of carbon atoms in solution for which it actually has no room. This generates a new microstructure, martensite. The DPH of martensite is about 1,000; it is the hardest and most brittle form of steel. Tempering martensitic steel—i.e., raising its temperature to a point such as 400° C and holding it for a time—decreases the hardness and brittleness and produces a strong and tough steel. Quench-and-temper heat treatments are applied at many different cooling rates, holding times, and temperatures; they constitute a very important means of controlling steel’s properties. (See also below Treating of steel: Heat-treating.)\nA third way to change the properties of steel is by adding alloying elements other than carbon that produce characteristics not achievable in plain carbon steel. Each of the approximately 20 elements used for alloying steel has a distinct influence on microstructure and on the temperature, holding time, and cooling rates at which microstructures change. They alter the transformation points between ferrite and austenite, modify solution and diffusion rates, and compete with other elements in forming intermetallic compounds such as carbides and nitrides. There is a huge amount of empirical information on how alloying affects heat-treatment conditions, microstructures, and properties. In addition, there is a good theoretical understanding of principles, which, with the help of computers, enables engineers to predict the microstructures and properties of steel when alloying, hot-rolling, heat-treating, and cold-forming in any way.\nA good example of the effects of alloying is the making of a high-strength steel with good weldability. This cannot be done by using only carbon as a strengthener, because carbon creates brittle zones around the weld, but it can be done by keeping carbon low and adding small amounts of other strengthening elements, such as nickel or manganese. In principle, the strengthening of metals is accomplished by increasing the resistance of lattice structures to the motion of dislocations. Dislocations are failures in the lattices of crystals that make it possible for metals to be formed. When elements such as nickel are kept in solid solution in ferrite, their atoms become embedded in the iron lattices and block the movements of dislocations. This phenomenon is called solution hardening. An even greater increase in strength is achieved by precipitation hardening, in which certain elements (e.g., titanium, niobium, and vanadium) do not stay in solid solution in ferrite during the cooling of steel but instead form finely dispersed, extremely small carbide or nitride crystals, which also effectively restrict the flow of dislocations. In addition, most of these strong carbide or nitride formers generate a small grain size, because their precipitates have a nucleation effect and slow down crystal growth during recrystallization of the cooling metal. Producing a small grain size is another method of strengthening steel, since grain boundaries also restrain the flow of dislocations.\nAlloying elements have a strong influence on heat-treating, because they tend to slow the diffusion of atoms through the iron lattices and thereby delay the allotropic transformations. This means, for example, that the extremely hard martensite, which is normally produced by fast quenching, can be produced at lower cooling rates. This results in less internal stress and, most important, a deeper hardened zone in the workpiece. Improved hardenability is achieved by adding such elements as manganese, molybdenum, chromium, nickel, and boron. These alloying agents also permit tempering at higher temperatures, which generates better ductility at the same hardness and strength.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/Science-Tech",
    "content": "The Difference Between a Solstice and Equinox\n10 Important Dates in Pluto History\nDefy Gravity: 7 of the Biggest Things That Flew\n6 of the World's Most Dangerous Birds\nA team of scientists recently recreated the face of Peru's most famous mummy, \"Juanita,\" or the \"Ice Maiden.\" The girl is thought to have been sacrificed when she was between the ages of 13 and 15 some 500 years ago. Mummified by centuries of ice and snow, the body was well-preserved and is just one example of how mummification can occur naturally.\nMethods of Mummification\n7 Wonders of the Natural World\nMeteorites\nCoral\nLife\nTornadoes\nThe Solar System",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/browse/Cities-Towns-P-S",
    "content": "",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/place/Acrisure-Stadium",
    "content": "…city’s professional baseball team, and Acrisure Stadium houses the Steelers, its professional football team. The Penguins, Pittsburgh’s professional ice hockey team, plays at PPG Paints Arena. Popular summertime attractions include riverboat excursions on Pittsburgh’s waterways and Kennywood, an amusement park southeast of the city in West Mifflin.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/place/Pittsburgh/additional-info#history",
    "content": "",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/sports/American-football",
    "content": "American football, version of the sport of football that evolved from English rugby and soccer (association football); it differs from soccer chiefly in allowing players to touch, throw, and carry the ball with their hands, and it differs from rugby in allowing each side to control the ball in alternating possessions. The sport, played with 11 on each side, originated in North America, primarily in the United States, where it eventually became the country’s leading spectator sport. It also developed simultaneously in Canada, where it evolved into a 12-man game, though Canadian football never achieved the great popularity and status of ice hockey there. American football has not been taken up in the rest of the world to the same degree as other American sports such as basketball and baseball. Since the 1980s, however, primarily through the marketing efforts of the National Football League, teams and leagues have been established in Europe, and the game has achieved a degree of international popularity through television. The sport is sometimes called gridiron football because of the vertical yard lines marking the rectangular field.\nGridiron football was the creation of elite American universities, a fact that has shaped its distinctive role in American culture and life. After several decades of informal, student-organized games that were tolerated by faculty as an alternative to more destructive rowdiness, the first intercollegiate football game was played on November 6, 1869, in New Brunswick, New Jersey, between in-state rivals Princeton and Rutgers according to rules adapted from those of the London Football Association. This soccer-style game became the dominant form as Columbia, Cornell, Yale, and a few other colleges in the Northeast took up the sport in the early 1870s, and in 1873 representatives from Princeton, Yale, and Rutgers met in New York City to found the Intercollegiate Football Association and to adopt a common code. Conspicuously missing was Harvard, the country’s premier university, whose team insisted on playing the so-called “Boston Game,” a cross between soccer and rugby. In May 1874, in the second of two matches with McGill University of Montreal (the first was played by the rules of the Boston Game), Harvard’s players were introduced to the rugby game and immediately preferred it to their own. The following year, for Harvard’s first football contest with Yale, representatives of the two schools agreed on “concessionary rules” that were chiefly Harvard’s. When spectators (including Princeton students) as well as Yale players saw the advantages of the rugby style, the stage was set for a meeting in 1876 of representatives from Harvard, Yale, Princeton, and Columbia to form a new Intercollegiate Football Association based on rugby rules.\n(Read Walter Camp’s 1903 Britannica essay on football.)\nHarvard made the first breach in rugby rules. Rejecting the traditional manner of putting the ball in play—players from both teams massed about the ball in a “scrummage,” or “scrum,” trying to kick it forward through the mass of players—Harvard opted for “heeling it out,” or kicking the ball backward to a teammate. The further transformation of English rugby into American football came chiefly through the efforts of Walter Camp, who even during his lifetime was known as the “Father of American Football.” As an undergraduate and then a medical student at Yale, Camp played football from 1876 through 1881, but—more important—beginning in 1878, he dominated the rules committee for nearly three crucial decades. Two of Camp’s revisions in particular effectively created the gridiron game. The first, in 1880, further refined Harvard’s initial innovation, abolishing the scrummage altogether in favor of a scrimmage, which awarded possession of the ball to one of the two teams. It was then put in play by heeling it out. (Snapping the ball with the hand became legal in 1890, though snapping with the foot continued as an option until 1913.)\nThe second crucial rule change was necessitated by the first. Camp’s more orderly manner of initiating play did not require the team in possession of the ball to give it up. After Princeton simply held the ball for an entire half in its 1880 and 1881 contests with Yale, both games ending in scoreless ties that bored spectators as much as they frustrated Yale’s players, Camp proposed a rule that a team must advance the ball 5 yards or lose 10 in three downs (plays), or it would be obliged to surrender the ball to the other side. Camp was also responsible for having 11 players on a side, for devising a new scoring system in 1883 with two points for a touchdown, four points for the goal after a touchdown, and five points for a field goal (a field goal became worth three points in 1909, a touchdown six points in 1912), for creating the quarterback position, for marking the field with stripes, and for proposing several other innovations, but it was those two simple rules adopted in 1880 and 1882 that most fundamentally created American football.\nAfter the crucial rule changes, the play of the game was relatively open, featuring long runs and numerous lateral passes, as in rugby. In 1888 Camp proposed that tackling below the waist be legalized, in order to offset the advantage of speedy backs streaking around the ends. The new rule resulted in the rise of mass plays, an offensive strategy that massed players on a single point of the defense, most famously in Harvard’s “flying wedge” in 1892. This style of play proved so brutal that the game was nearly abolished in the 1890s and early 1900s.\nThe spirit of early football can be glimpsed in the introduction of a rule in 1894 that banned projecting nails or iron plates in shoes and any metal substance on the player’s person. Rules establishing boundaries between permissible and impermissible violence have been continually revised over the years, sometimes in response to periods of heightened concern over deaths and injuries (in the early 1930s as well as the 1890s, for example). To ensure greater safety, the number of officials grew from two in 1873 to seven by 1983. Over time, improvements in equipment also provided more safeguards against serious injuries. In the 1890s players’ only protection against blows to the head came from their own long hair and leather nose guards. The first headgear, in 1896, consisted simply of three leather straps. It evolved into close-fitting leather caps with ear flaps. The suspension helmet, which used straps to create space between the helmet shell and the head of the wearer, was introduced in 1917. However, helmets were not required in college football until 1939 (1943 for the National Football League). Improved equipment sometimes increased rather than curtailed the game’s dangers. The plastic helmet, introduced in 1939, became a potentially lethal weapon, eventually requiring rules against “spearing”—using the head to initiate contact.\nIn 1879 the University of Michigan and Racine College of Wisconsin inaugurated football in the Midwest. Michigan under Fielding Yost in 1901–05 and the University of Chicago under Amos Alonzo Stagg in 1905–09 emerged as major powers. The game also spread throughout the rest of the country by the 1890s, though the Big Three—Harvard, Yale, and Princeton—continued to dominate the collegiate football world into the 1920s. Ever mindful of their superiority to the latecomers, the three (joined by the University of Pennsylvania to create briefly a Big Four) formed the Intercollegiate Rules Committee in 1894, separate from the Intercollegiate Football Association. In 1895 in the Midwest, colleges dissatisfied with this divided leadership asserted their independence by forming what became the Western (now the Big Ten) Conference. The game also spread to the South and West, though conferences were not formed until later in those regions.\nThe brutality of mass play also spread through the nation; over the course of the 1905 season, 18 young men died from football injuries. Concerned that football might be abolished altogether, President Theodore Roosevelt in October 1905 summoned representatives (including Camp) from Harvard, Yale, and Princeton to the White House, where he urged them to reform the game. On December 28 of that year representatives from 62 colleges and universities (not including the Big Three, who would continue for decades to balk at submitting to the will of “inferior” institutions) met in New York to form the Intercollegiate Athletic Association of the United States, which became the National Collegiate Athletic Association (NCAA) in 1910. To reduce mass play, the group at its initial meeting increased the yardage required for a first down from 5 yards to 10 and legalized the forward pass, the final element in the creation of the game of American football. The founding of the NCAA effectively ended the period when the Big Three (and Walter Camp personally) dictated rules of play to the rest of the football world. It also ended student involvement in controlling the game, though the question of who should rule college football—coaches, alumni and boosters, or college administrators—would continue to bedevil the NCAA throughout its history.\nBrutality did not end with the revised rules of 1906. New crises prompted additional rule changes in 1910 (requiring seven men on the line of scrimmage) and 1912 (increasing the number of downs to gain 10 yards from three to four) to eliminate mass play. Nor did the forward pass immediately transform the game. The restrictive 1906 rules made passes riskier than fumbles, and it was only after several years of cautious experimentation that Notre Dame’s upset of Army in 1913 highlighted the remarkable possibilities in the passing game. It would be another three decades, however—during which restrictive rules were gradually dropped and the circumference of the ball reduced to facilitate passing—before those possibilities could be fully realized.\nThis early period in American football was formative in another way as well. Beginning in 1876, the original Intercollegiate Football Association staged a championship game at the end of each season, on Thanksgiving Day, matching the two best teams from the previous year. Initially the game was played in Hoboken, New Jersey, but in 1880 it was shifted to New York City to make it easier for students from all the universities in the association to attend the game. The attendance for that first contest in New York was 5,000. By 1884 it had climbed to 15,000; it rose to 25,000 by 1890 and 40,000 by 1893, the last Thanksgiving Day game to be played in the city. By this time, accounts of the game in New York’s major newspapers were taking up as much as three pages in an eight-page paper, and wire services carried reports to every corner of the country. By the 1890s an extracurricular activity at a handful of elite northeastern universities was becoming a spectator sport with a nationwide audience. College football became known for its bands and cheerleaders, pep rallies and bonfires, and homecoming dances and alumni reunions as much as for its athletic thrills. Pursuing the institution’s educational mission while serving the public’s desire for entertainment posed a dilemma with which college administrators struggled for more than a century. For some of the public, college football’s association with institutions of higher education and immersion in college spirit imbued the game with a kind of amateur purity. For others, the colleges’ profession of commitment to academic goals while commercializing their sports teams only smacked of hypocrisy.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/browse/Geography-Travel",
    "content": "",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/topic/city",
    "content": "city, relatively permanent and highly organized centre of population, of greater size or importance than a town or village. The name city is given to certain urban communities by virtue of some legal or conventional distinction that can vary between regions or nations. In most cases, however, the concept of city refers to a particular type of community, the urban community, and its culture, known as “urbanism.”\nCity government is almost everywhere the creation of higher political authority—usually state or national. In most Western countries, devolution of powers to the cities occurs through legislative acts that delegate limited self-government to local corporations. Some European countries adopted general municipal codes that permitted centralized administrative control over subordinate areas through a hierarchy of departmental prefects and local mayors. Socialist countries generally employed a hierarchical system of local councils corresponding to, and under the authority of, governing bodies at higher levels of government.\nAs a type of community, the city may be regarded as a relatively permanent concentration of population, together with its diverse habitations, social arrangements, and supporting activities, occupying a more or less discrete site and having a cultural importance that differentiates it from other types of human settlement and association. In its elementary functions and rudimentary characteristics, however, a city is not clearly distinguishable from a town or even a large village. Mere size of population, surface area, or density of settlement are not in themselves sufficient criteria of distinction, while many of their social correlates (division of labour, nonagricultural activity, central-place functions, and creativity) characterize in varying degree all urban communities from the small country town to the giant metropolis.\nIn the Neolithic Period (New Stone Age; roughly 9000 to 3000 BC), humans achieved relatively fixed settlement, but for perhaps 5,000 years such living was confined to the semipermanent peasant village—semipermanent because, when the soil had been exhausted by the relatively primitive methods of cultivation, the entire village was usually compelled to pick up and move to another location. Even when a village prospered in one place, it would commonly split in two after the population had grown relatively large so that all cultivators would have ready access to the soil.\nThe evolution of the Neolithic village into a city took at least 1,500 years—in the Old World from 5000 to 3500 BC. The technological developments making it possible for humankind to live in urban places were at first mainly advances in agriculture. Neolithic-era domestication of plants and animals eventually led to improved methods of cultivation and stock breeding, which eventually produced a surplus and made it possible to sustain a higher population density while also freeing up some members of the community for craftsmanship and the production of nonessential goods and services.\nAs human settlements increased in size through advances in irrigation and cultivation, the need for improving the circulation of goods and people became ever more acute. Pre-Neolithic humans, who led a nomadic existence in their never-ending search for food, moved largely by foot and carried their essential goods with the help of other humans. Neolithic people, upon achieving the domestication of animals, used them for transportation as well as for food and hides—thus making it possible to travel greater distances. Then came the use of draft animals in combination with a sledge equipped with runners for carrying heavier loads. The singular technological achievement in the early history of transportation, however, was the invention of the wheel, used first in the Tigris-Euphrates valley about 3500 BC and constructed of solid materials (the development of hubs, spokes, and rims would follow). Wheels, to be used efficiently, required roads, and thus came road building, an art most highly developed in ancient times by the Romans. Parallel improvements were made in water transport: irrigation ditches and freshwater supply routes first constructed in the 7th century BC were followed by the development of navigable canals, while rafts, dugouts, and reed floats were eventually succeeded by wooden boats.\nThe first recognizable cities had emerged by approximately 3500 BC. As the earliest urban populations, they were distinguished by literacy, technological progress (notably in metals), and increasingly sophisticated forms of social and political organization (formalized in religious-legal codes and symbolized in temples and walls). Such places first developed in the Nile valley and on the Sumerian coast at Ur, appearing in the Indus valley at Mohenjo-daro during the 3rd millennium BC; by 2000 BC cities had also appeared in the Wei River valley in China. The overland trade routes brought about the proliferation of cities from Turkestan to the Caspian Sea and then to the Persian Gulf and eastern Mediterranean. Their economic base in agriculture (supplemented by trade) and their political-religious institutions gave cities an unprecedented degree of occupational specialization and social stratification. City life was not insular, however, as many cities lent some coherence and direction to life and society in their hinterlands.\nIt was in the Greek city-state, or polis, that the city idea reached its peak. Originally a devout association of patriarchal clans, the polis came to be a small self-governing community of citizens, in contrast to the Asian empires and nomadic groups elsewhere in the world. For citizens, at least, the city and its laws constituted a moral order symbolized in an acropolis, magnificent buildings, and public assemblies. It was, in Aristotle’s phrase, “a common life for a noble end.”\nWhen the exclusive requirements for citizenship (citizens originally being landowning men with no history of servitude) were relaxed and as new commercial wealth surpassed that of the older landed citizenry, social strife at home and rivalry abroad gradually weakened the common life of the city-republics. The creativity and variety of the polis gave way before the unifying forces of king worship and empire epitomized by Alexander the Great and his successors. To be sure, many new cities—often named Alexandria because Alexander had founded them—were planted between the Nile and the Indus, facilitating contacts between the major civilizations of Europe and Asia and giving rise to cultural exchanges and commercial trade that left a lasting impact on both East and West. While remaining culturally vibrant, the city itself ceased to be an autonomous body politic and became a dependent member of a larger political-ideological whole.\nThe Romans, who fell heir to the Hellenistic world, transplanted the city into the technologically backward areas beyond the Alps inhabited by pastoral-agricultural Celtic and Germanic peoples. But, if Rome brought order to civilization and carried both to barbarians along the frontier, it made of the city a means to empire (a centre for military pacification and bureaucratic control) rather than an end in itself. The enjoyment of the imperial Roman peace entailed the acceptance of the status of municipium—a respectable but subordinate rank within the Roman state. The municipia were supported fiscally by taxes on trade, contributions from members of the community, and income from lands owned by each municipium. Over time, however, the idea of public duty gave way to private ambition, especially as Roman citizenship became more universal (see civitas). Municipal functions atrophied, and the city survived into the Byzantine era principally as a mechanism of fiscal administration, although it often remained a locus of educational development and religious and cultural expression.\nIn Latin Europe neither political nor religious reforms could sustain the Roman regime. The breakdown of public administration and the breach of the frontier led to a revival of parochial outlook and allegiance, but the focus was not upon the city. Community life centred instead on the fortress (e.g., walled city), whereas the civitas was attached to the precincts of the episcopal throne, as in Merovingian Gaul.\nEarly medieval society was a creation of camp and countryside that fulfilled the local imperatives of sustenance and defense. With Germanic variations on late Roman forms, communities were restructured into functional estates, each of which owned formal obligations, immunities, and jurisdictions. What remained of the city was comprehended in this manorial order, and the distinction between town and country was largely obscured when secular and ecclesiastical lords ruled over the surrounding counties—often as the vassals of barbarian kings (see manorialism). Social ethos and organization enforced submission to the common good of earthly survival and heavenly reward. The attenuation of city life in most of northern and western Europe was accompanied by provincial separatism, economic isolation, and religious otherworldliness. Not before the cessation of attacks by Magyars, Vikings, and Saracens did urban communities again experience sustained growth.\nRecovery after the 10th century was not confined to the city or to any one part of Europe. The initiatives of monastic orders, seigneurs, or lords of the manor, and merchants alike fostered a new era of increased tillage, craftsmanship and manufacturing, a money economy, scholarship, growth of rural population, and founding of “new towns,” as distinguished from those “Roman” cities that had survived from the period of Germanic and other encroachments. In almost all the “new” medieval towns, the role of the merchant was central in catalyzing the long-distance trade of commodities and staple goods.\nBefore the year 1000, contacts with rich Byzantine and Islamic areas in the Levant had revitalized the mercantile power in Venice, which grew wealthy from its command of the profitable route to the Holy Land during the Crusades. Meanwhile, merchant communities had attached themselves to the more-accessible castle towns and dioceses in northern Italy and on the main routes to the Rhineland and Champagne. They later appeared along the rivers of Flanders and northern France and on the west-east road from Cologne to Magdeburg (see Hanseatic League). In all of these towns, trade was the key to their growth and development.\nIt was no coincidence that the 12th and 13th centuries, which saw the founding of more new towns than any time between the fall of Rome and the Industrial Revolution, also witnessed a singular upsurge toward civic autonomy. Throughout western Europe, towns acquired various kinds of municipal institutions loosely grouped under the designation commune. Broadly speaking, the history of the medieval towns is that of the rising merchant classes seeking to free their communities from lordly jurisdiction and to secure their government to themselves. Wherever monarchical power was strong, the merchants had to be content with a municipal status, but elsewhere they created city-states. Taking advantage of renewed conflict between popes and emperors, they allied with local nobility to establish communal self-government in the largest cities of Lombardy, Tuscany, and Liguria. In Germany the city councils sometimes usurped the rights of higher clergy and nobility; Freiburg im Breisgau obtained its exemplary charter of liberties in 1120. The movement spread to Lübeck and later to associated Hanse towns on the Baltic and North seas, touching even the Christian “colonial” towns east of the Elbe and Saale rivers. In the 13th century the great towns of Bruges, Ghent, and Ypres, creditors of the counts of Flanders, virtually governed the entire province. In France, revolutionary uprisings, directed against nobility and clergy, sometimes established free communes, but most communities were content with a franchise from their sovereign—despite their limitations compared with the relative liberty of English boroughs after the Norman Conquest. Finally the corporate freedom of the towns brought emancipation to individuals. When bishops in the older German cities treated newcomers as serfs, the emperor Henry V affirmed the principle Stadtluft macht frei (German: “City air brings freedom”) in charters for Speyer and Worms; such new towns, founded on the lands of lay and clerical lords, offered freedom and land to settlers who took up residence for more than “a year and a day.” In France the villes neuves (“new towns”) and bastides (medieval French towns laid out on a rectangular grid) likewise conferred rights on servile persons.\nIn the 14th century the growth of urban centres subsided as Europe suffered a series of shocks that included famine from 1315 to 1317, the emergence of the Black Death, which spread across Europe starting in 1347, and a period of political anarchy and economic decline that continued through the 15th century. Turkish encroachments on the routes to Asia worsened conditions in town and country alike. Europe turned inward upon itself, and, except for a few large centres, activity in the marketplace was depressed. At a time when local specialization and interregional exchange required more-liberal trade policies, craft protectionism and corporate particularism in the cities tended to hobble the course of economic growth. The artisan and labouring classes, moreover, grew strong enough to challenge the oligarchical rule of the wealthy burghers and gentry through disruptions such as the Revolt of the Ciompi (1378), while social warfare peaked in peasant uprisings typified by the Jacquerie (1358), but these tended to be short-lived revolts that failed to bring enduring social change. The era of decline was relieved, some argue, by the slow process of individual emancipation and the cultural efflorescence of the Renaissance, which effectively grew out of the unique urban environment of Italy and was strengthened by a high regard for the Classical heritage. These values laid the intellectual basis for the great age of geographic and scientific discovery exemplified in the new technologies of gunpowder, mining, printing, and navigation. Not before the triumph of princely government, in fact, did political allegiance, economic interests, and spiritual authority again become centred in a viable unit of organization, the absolutist nation-state.\nThe virtue of absolutism in the early modern period lay in its ability to utilize the new technologies on a large scale. Through the centralization of power, economy, and belief, it brought order and progress to Europe and provided a framework in which individual energies could once more be channeled to a common end. While the nation stripped the cities of their remaining pretensions to political and economic independence (heretofore symbolized in their walls and tariff barriers), it created larger systems of interdependence in which territorial division of labour could operate. National wealth also benefited from the new mercantilist policies, but all too often the wealth generated by cities was captured by the state in taxes and then dissipated—either in war or by supporting the splendour of court life and the Baroque glory of palaces and churches. Only in colonial areas, notably the Americas, did the age of expansion see the development of many new cities, and it is significant that the capitals and ports of the colonizing nations experienced their most rapid growth during these years. Under absolutist regimes, however, a few large political and commercial centres grew at the expense of smaller outlying communities and the rural hinterlands.\nBy the 18th century the mercantile classes had grown increasingly disenchanted with monarchical rule. Merchants resented their lack of political influence and assured prestige, and they objected to outmoded regulations that created barriers to commerce—especially those that hindered their efforts to link commercial operations with improved production systems such as factories. Eventually, the merchants would unite with other dissident groups to curb the excesses of absolutism, erase the vestiges of feudalism, and secure a larger voice in the shaping of public policy. In northwestern Europe, where these liberal movements went furthest, the city populations and their influential bourgeois elites played a critical role that was disproportionate to their numbers. Elsewhere, as in Germany, the bourgeoisie was more reconciled to existing regimes or, as in northern Italy, had assumed a passive if not wholly parasitical role.\nWith the exceptions of Great Britain and the Netherlands, however, the proportion of national populations resident in urban areas nowhere exceeded 10 percent. As late as 1800 only 3 percent of world population lived in towns of more than 5,000 inhabitants. No more than 45 cities had populations over 100,000, and fewer than half of these were situated in Europe. Asia had almost two-thirds of the world’s large-city population, and cities such as Beijing (Peking), Guangzhou (Canton), and Tokyo (Edo) were larger than ancient Rome or medieval Constantinople at their peaks.\nBefore 1800, innovations in agricultural and manufacturing techniques had permitted a singular concentration of productive activity close to the sources of mechanical power—water and coal. A corresponding movement of population was accelerated by the perfection of the steam engine and the superiority of the factory over preindustrial business organization. From the standpoint of economy, therefore, the localization of differentiated but functionally integrated work processes near sources of fuel was the mainspring of industrial urbanism. Under conditions of belt-and-pulley power transmission, urban concentration was a means of (1) minimizing the costs of overcoming frictions in transport and communications and (2) maximizing internal economies of scale and external economies of agglomeration. Although the intellectual and social prerequisites for industrialization were not uniquely present in any one nation, an unusual confluence of commercial, geographic, and technological factors in Britain led to far-reaching changes in such strategic activities as textiles, transport, and iron. Britain became “the workshop of the world” and London its “head office.” Differentiation went so far that the cotton, wool, and iron districts became more specialized and productive, each proceeding within its own cycle of technical and organizational change. By the mid-19th century, similar if less-comprehensive industrial organization was evident in parts of France, the Low Countries, and the northeastern United States.\nThe concentration of the manufacturing labour force in mill towns and coke towns gradually undermined traditional social structures and relations. Problems of public order, health, housing, utilities, education, and morals were aggravated by the influx of newcomers from the countryside. The combination of high rural birth rates and the industrialization of agriculture raised production levels of foods and fibres but also caused more children to migrate to cities, as fewer were needed to work on the increasingly mechanized farms. Though the lowering of mortality in the 19th century was later offset by declines in fertility, the population of the more-industrialized nations boomed into the 20th century, and the greater part of the increment migrated to the larger towns. The outcome was rural depopulation and the urbanization of society. Local political and social institutions, often of medieval origin, were unable to cope with conditions that exaggerated poverty, disrupted family life, and complicated personal adjustment. Piecemeal reforms did little to improve the new milieu because, in the last analysis, the “city problem” arose not so much from the lack of public authority as from an unwillingness to pay the costs of social planning, public health, and civic improvement. Generations of urbanites therefore faced long work hours, poor work conditions, overcrowded housing, and inadequate sanitation. The populations of cities, however, adapted to the new urban norms, evidently striking a balance between the deleterious consequences of urbanization and the economic and cultural opportunities uniquely associated with the city.\nIn the century after 1850, world population doubled, and the proportion of people living in cities of more than 5,000 inhabitants rose from less than 7 percent to almost 30 percent. Between 1900 and 1950 the population living in large cities (100,000 plus) rose by 250 percent, the rate of increase in Asia being three times that in Europe and the United States. Nevertheless, the pattern of industrial urbanization—an overwhelmingly nonagricultural economy organized in a hierarchical system of different-sized cities ranging from one or more metropolitan centres at the top to a broad base of smaller-sized cities underneath—was still largely confined to the economically advanced areas: Europe, North America, Japan, and, to a lesser extent, Australasia.\nMeanwhile, industrial urbanism had entered its metropolitan phase. Especially in the United States, the widespread use of cheap electric power, the advent of rapid transit and communications, new building materials, the automobile, and rising levels of per capita personal income had led to some relaxation of urban concentration. City dwellers began moving out from older downtown areas to suburbs and satellite communities where conditions were thought to be less wearing on nerves and bodies. Rising central-area land values and property taxes, traffic congestion, decaying infrastructure, and street crime reinforced the exodus. At the city’s core the composition of the resident population came to include growing proportions of the aged, minority groups, and the very poor.\nJust as populations shifted from city centres to suburbs and broader conurbations, manufacturing companies began building their production plants on suburban or rural sites, thereby taking advantage of cheaper land costs, lower labour expenses, less-cumbersome municipal regulation, smaller tax burdens, and, in many cases, more-efficient transportation routes. By the 21st century it was evident that advanced economies would depend more on human capital than on physical capital and on the production of services rather than the manufacture of goods; as a consequence, cities were shifting from loci of industrial production to centres of knowledge. Thus, while its advantages for manufacturers have diminished, the city remains a fundamental locus for the mass of specialized service activity that forms so large a part of the modern economy.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/place/Pittsburgh/images-videos",
    "content": "",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/dictionary/Undergraduate",
    "content": "",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/story/britannicas-us-presidents-bingo",
    "content": "Whose smile is that? Whose mustache? See how many of these 24 U.S. presidents you can name.\nFor more presidential play, check out our quiz about U.S. Presidential Nicknames.\nAfter the glitzy red-carpet arrivals, the feel-good montages, and the host’s opening roast, the Oscars ceremony switches to its raison d’être: revealing the previous year’s highest achievers in cinema. One after another, the presenters list the nominees, open a lavish envelope, and reveal the winner in each category. The champions breathlessly accept their awards and, over and over, effusively thank their mothers, God, and the academy. The academy has so much gratitude bestowed on it throughout the ceremony that many of us watching at home may start to wonder: What is this obscure body? The answer is less enthralling than one might think. The academy—that is, the Academy of Motion Picture Arts and Sciences—is the organization that votes for the Oscar winners. Perhaps what’s more interesting is figuring out who the members of the academy are and how they vote.\nThe academy is an exclusive Hollywood institution that has its own governing body (aptly named the Board of Governors), 17 separate branches, and a thorough rule book on membership eligibility and voting processes. Since 2016, when the board announced that it would diversify its membership, the academy has grown to about 8,000 members. It doesn’t publicize the names of all those members, but each spring it releases a list of the individuals it has invited to join its ranks. Invitees have included Mindy Kaling, Rashida Jones, Kendrick Lamar, Melissa Etheridge, and J.K. Rowling. The academy also posts the names of members of its current Board of Governors, which includes elected representatives of the 17 branches. These branches represent the various fields of cinema: acting, directing, writing, sound editing, and others. In 2019 the academy’s Website showed that the Board of Governors’ president was cinematographer John Bailey, its first vice president was makeup artist Lois Burwell, and its other officers included actors Laura Dern, Whoopi Goldberg, and Alfred Molina.\nThe rest of the academy members are not listed, but we can guess who a few are by looking at some of the requirements to join the institution. To qualify, an individual must work in the film industry. This means that neither individuals who work exclusively in television nor members of the press may join. Oscar nominees are often considered for membership automatically, while other candidates must be sponsored by two active members of the branch they wish to join. Each branch also has its own specific requirements. Directors, for example, must have a minimum of two directing credits, at least one of them within the past 10 years. So we can be pretty sure that such Hollywood treasures as Meryl Streep, Jack Nicholson, Steven Spielberg, and Tom Hanks, who have each been nominated several times and have won Oscars, are members of the academy. New members may choose only one of the branches to join. This means that Sofia Coppola and Alfonso Cuarón, for example, who were each nominated for and won Oscars, could sign on to either the directing branch or the writing branch but not both.\nWhile the membership of the academy is largely obscure, the voting process is perhaps only slightly clearer. It involves two phases: first, nominating the Oscar candidates and, second, voting for the winners. In the first phase, members receive a ballot that lists qualifying movies. To be considered for nomination, a movie must be feature-length and must have been publicly screened for paid admission for at least one week at a commercial theater in Los Angeles county between January 1 and December 31 of the award year. Documentaries and foreign films have their own eligibility requirements. Members may nominate only for awards within their branch and for best picture. Emma Stone may thus suggest nominees for best actress, actor, supporting actress, and supporting actor, but she may not nominate candidates in the best sound editing or best sound mixing categories. Each member of the academy picks up to five candidates for each of their designated categories and lists them by preference.\nTo determine the nominees in each category, the ballots are tallied by certified public accountants from a firm designated by the academy’s president in a somewhat arcane system that might seem like a sacred ritual to an outsider. To ensure that nominees have broad, rather than just popular, support, the academy uses instant runoff voting, sometimes called preferential voting, which involves several rounds and a “magic” number, wherein a candidate must receive a predetermined number of votes to be considered a nominee. A few weeks after the nominees are announced in January, the second phase of voting begins. For the final voting, all active or lifetime academy members are allowed to cast ballots in any category, but they are discouraged from voting in categories where they lack expertise. Accountants once again tally the ballots, using the preferential system to determine the winner for best picture but using the popular vote for all other categories.\nAfter all the voting and tallying, the winners are finally determined, but they are not reported to anyone. Only two accountants see the final results, and they are responsible for keeping those results secret until the awards ceremony. The accountants memorize the names of the winners, stuff two sets of envelopes, and pack and store two briefcases at an undisclosed location until the day of the ceremony. At the ceremony, neither the members of the academy nor the producers of the awards show know who will receive an Oscar. It is a complete mystery until the presenter utters one of the most famous lines in Hollywood: “And the Oscar goes to....”",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/topic/Carnegie-Mellon-University/images-videos",
    "content": "",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/browse/Education",
    "content": "",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/biography/John-Forbes",
    "content": "John Forbes led a new advance upon Fort Duquesne. Forbes resolved not to use Braddock’s road but to cut a new one west from Raystown, Pennsylvania. Washington disapproved of the route but played an important part in the movement. Late in the autumn the French…\n…settled in 1758 when General John Forbes and his British and colonial army expelled the French from Fort Duquesne (built 1754). Forbes named the site for the British statesman William Pitt the Elder. The British built Fort Pitt (1761) to ensure their dominance at the source of the Ohio. Settlers…\nJohn Forbes cut a road across Pennsylvania and seized Fort Duquesne, evacuated by the French; Amherst took the fortress of Louisbourg for the second and last time; and other troops took possession of outposts on the Ohio River. In the summer of 1759 came the…",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/biography/Robert-H-Dennard",
    "content": "Robert H. Dennard (born September 5, 1932, Terrell, Texas, U.S.—died April 23, 2024, Sleepy Hollow, New York) was an American engineer credited with the invention of the one-transistor cell for dynamic random-access memory (DRAM) and with pioneering the set of consistent scaling principles that underlie the improved performance of increasingly miniaturized integrated circuits, two pivotal innovations that helped spur more than three decades of growth in the computer industry.\nDennard received a B.S. (1954) and an M.S. (1956) in electrical engineering from Southern Methodist University, Dallas, and a Ph.D. (1958) from Carnegie Institute of Technology (now Carnegie Mellon University), Pittsburgh. He joined the International Business Machines Corporation (IBM) in 1958 as a staff engineer and first worked on memory and logic circuits and on the development of data communication techniques. In the early 1960s he began focusing on microelectronics. His design for one-transistor-cell DRAM improved upon other types of computer memory that were then in development (including a memory system consisting of wire mesh and magnetic rings), and in 1968 Dennard was granted a patent for the design. It was one of more than four dozen patents that he was eventually issued. Dennard was given the title of IBM fellow in 1979, and he held several positions over the length of his career of more than 50 years with the company.\nDRAM consists of an array of semiconductor memory cells that are integrated on a silicon chip. The type of memory cell invented by Dennard in the 1960s used a single metal-oxide-semiconductor (MOS) transistor to store and read binary data as an electrical charge on a MOS capacitor, and the high-density memory made possible by that design resulted in relatively low production costs and power requirements for DRAM. Following its introduction as a commercial product in the 1970s, one-transistor-cell DRAM was extensively used in computers and other electronic devices. With miniaturization, it was possible to develop DRAM chips that contain billions of memory cells.\nDennard was elected to the U.S. National Academy of Engineering in 1984 and was inducted into the U.S. National Inventors Hall of Fame in 1997. Among the other awards and honours that Dennard garnered were the U.S. National Medal of Technology and Innovation, which he received (1988) from U.S. Pres. Ronald Reagan, and the 2005 Lemelson-MIT (Massachusetts Institute of Technology) Lifetime Achievement Award. In 2009 he received both the Medal of Honor from the Institute of Electrical and Electronics Engineers and the National Academy of Engineering’s Charles Stark Draper Prize. He later was awarded the Kyoto Prize (2013).\nintegrated circuit (IC), an assembly of electronic components, fabricated as a single unit, in which miniaturized active devices (e.g., transistors and diodes) and passive devices (e.g., capacitors and resistors) and their interconnections are built up on a thin substrate of semiconductor material (typically silicon). The resulting circuit is thus a small monolithic “chip,” which may be as small as a few square centimetres or only a few square millimetres. The individual circuit components are generally microscopic in size.\nIntegrated circuits have their origin in the invention of the transistor in 1947 by William B. Shockley and his team at the American Telephone and Telegraph Company’s Bell Laboratories. Shockley’s team (including John Bardeen and Walter H. Brattain) found that, under the right circumstances, electrons would form a barrier at the surface of certain crystals, and they learned to control the flow of electricity through the crystal by manipulating this barrier. Controlling electron flow through a crystal allowed the team to create a device that could perform certain electrical operations, such as signal amplification, that were previously done by vacuum tubes. They named this device a transistor, from a combination of the words transfer and resistor. The study of methods of creating electronic devices using solid materials became known as solid-state electronics. Solid-state devices proved to be much sturdier, easier to work with, more reliable, much smaller, and less expensive than vacuum tubes. Using the same principles and materials, engineers soon learned to create other electrical components, such as resistors and capacitors. Now that electrical devices could be made so small, the largest part of a circuit was the awkward wiring between the devices.\nIn 1958 Jack Kilby of Texas Instruments, Inc., and Robert Noyce of Fairchild Semiconductor Corporation independently thought of a way to reduce circuit size further. They laid very thin paths of metal (usually aluminum or copper) directly on the same piece of material as their devices. These small paths acted as wires. With this technique an entire circuit could be “integrated” on a single piece of solid material and an integrated circuit (IC) thus created. ICs can contain hundreds of thousands of individual transistors on a single piece of material the size of a pea. Working with that many vacuum tubes would have been unrealistically awkward and expensive. The invention of the integrated circuit made technologies of the Information Age feasible. ICs are now used extensively in all walks of life, from cars to toasters to amusement park rides.\nAnalog, or linear, circuits typically use only a few components and are thus some of the simplest types of ICs. Generally, analog circuits are connected to devices that collect signals from the environment or send signals back to the environment. For example, a microphone converts fluctuating vocal sounds into an electrical signal of varying voltage. An analog circuit then modifies the signal in some useful way—such as amplifying it or filtering it of undesirable noise. Such a signal might then be fed back to a loudspeaker, which would reproduce the tones originally picked up by the microphone. Another typical use for an analog circuit is to control some device in response to continual changes in the environment. For example, a temperature sensor sends a varying signal to a thermostat, which can be programmed to turn an air conditioner, heater, or oven on and off once the signal has reached a certain value.\nA digital circuit, on the other hand, is designed to accept only voltages of specific given values. A circuit that uses only two states is known as a binary circuit. Circuit design with binary quantities, “on” and “off” representing 1 and 0 (i.e., true and false), uses the logic of Boolean algebra. (Arithmetic is also performed in the binary number system employing Boolean algebra.) These basic elements are combined in the design of ICs for digital computers and associated devices to perform the desired functions.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/print/article/96314",
    "content": "Carnegie Mellon University, private, coeducational institution of higher learning in Pittsburgh, Pennsylvania, U.S. The university includes the Carnegie Institute of Technology, the College of Humanities and Social Sciences, the College of Fine Arts, the Mellon College of Science, the School of Computer Science, the H. John Heinz III School of Public Policy and Management, and the Graduate School of Industrial Administration. Undergraduate and graduate degree programs are offered in a range of fields. Total enrollment is about 7,700.\nIn 1900 the industrialist Andrew Carnegie gave a gift of $1 million to the city of Pittsburgh for the creation of a technical school. Originally called Carnegie Technical Schools, it was renamed Carnegie Institute of Technology in 1912. The institute merged with the Mellon Institute (established in 1913 in Pittsburgh by financier Andrew W. Mellon) in 1967. The university has built a reputation as a vital arts centre, operating three art galleries, two concert halls, and two theatres. The faculty has included Nobel Prize-winning economists Herbert Alexander Simon and Merton Miller.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/story/causes-of-the-great-depression",
    "content": "The Great Depression of the late 1920s and ’30s remains the longest and most severe economic downturn in modern history. Lasting almost 10 years (from late 1929 until about 1939) and affecting nearly every country in the world, it was marked by steep declines in industrial production and in prices (deflation), mass unemployment, banking panics, and sharp increases in rates of poverty and homelessness. In the United States, where the effects of the depression were generally worst, between 1929 and 1933 industrial production fell nearly 47 percent, gross domestic product (GDP) declined by 30 percent, and unemployment reached more than 20 percent. By comparison, during the Great Recession of 2007–09, the second largest economic downturn in U.S. history, GDP declined by 4.3 percent, and unemployment reached slightly less than 10 percent.\nThere is no consensus among economists and historians regarding the exact causes of the Great Depression. However, many scholars agree that at least the following four factors played a role.\nThe stock market crash of 1929. During the 1920s the U.S. stock market underwent a historic expansion. As stock prices rose to unprecedented levels, investing in the stock market came to be seen as an easy way to make money, and even people of ordinary means used much of their disposable income or even mortgaged their homes to buy stock. By the end of the decade hundreds of millions of shares were being carried on margin, meaning that their purchase price was financed with loans to be repaid with profits generated from ever-increasing share prices. Once prices began their inevitable decline in October 1929, millions of overextended shareholders fell into a panic and rushed to liquidate their holdings, exacerbating the decline and engendering further panic. Between September and November, stock prices fell 33 percent. The result was a profound psychological shock and a loss of confidence in the economy among both consumers and businesses. Accordingly, consumer spending, especially on durable goods, and business investment were drastically curtailed, leading to reduced industrial output and job losses, which further reduced spending and investment.\nBanking panics and monetary contraction. Between 1930 and 1932 the United States experienced four extended banking panics, during which large numbers of bank customers, fearful of their bank’s solvency, simultaneously attempted to withdraw their deposits in cash. Ironically, the frequent effect of a banking panic is to bring about the very crisis that panicked customers aim to protect themselves against: even financially healthy banks can be ruined by a large panic. By 1933 one-fifth of the banks in existence in 1930 had failed, leading the new Franklin D. Roosevelt administration to declare a four-day “bank holiday” (later extended by three days), during which all of the country’s banks remained closed until they could prove their solvency to government inspectors. The natural consequence of widespread bank failures was to decrease consumer spending and business investment, because there were fewer banks to lend money. There was also less money to lend, partly because people were hoarding it in the form of cash. According to some scholars, that problem was exacerbated by the Federal Reserve, which raised interest rates (further depressing lending) and deliberately reduced the money supply in the belief that doing so was necessary to maintain the gold standard (see below), by which the U.S. and many other countries had tied the value of their currencies to a fixed amount of gold. The reduced money supply in turn reduced prices, which further discouraged lending and investment (because people feared that future wages and profits would not be sufficient to cover loan payments).\nThe gold standard. Whatever its effects on the money supply in the United States, the gold standard unquestionably played a role in the spread of the Great Depression from the United States to other countries. As the United States experienced declining output and deflation, it tended to run a trade surplus with other countries because Americans were buying fewer imported goods, while American exports were relatively cheap. Such imbalances gave rise to significant foreign gold outflows to the United States, which in turn threatened to devalue the currencies of the countries whose gold reserves had been depleted. Accordingly, foreign central banks attempted to counteract the trade imbalance by raising their interest rates, which had the effect of reducing output and prices and increasing unemployment in their countries. The resulting international economic decline, especially in Europe, was nearly as bad as that in the United States. \nDecreased international lending and tariffs. In the late 1920s, while the U.S. economy was still expanding, lending by U.S. banks to foreign countries fell, partly because of relatively high U.S. interest rates. The drop-off contributed to contractionary effects in some borrower countries, particularly Germany, Argentina, and Brazil, whose economies entered a downturn even before the beginning of the Great Depression in the United States. Meanwhile, American agricultural interests, suffering because of overproduction and increased competition from European and other agricultural producers, lobbied Congress for passage of new tariffs on agricultural imports. Congress eventually adopted broad legislation, the Smoot-Hawley Tariff Act (1930), that imposed steep tariffs (averaging 20 percent) on a wide range of agricultural and industrial products. The legislation naturally provoked retaliatory measures by several other countries, the cumulative effect of which was declining output in several countries and a reduction in global trade.\nJust as there is no general agreement about the causes of the Great Depression, there is no consensus about the sources of recovery, though, again, a few factors played an obvious role. In general, countries that abandoned the gold standard or devalued their currencies or otherwise increased their money supply recovered first (Britain abandoned the gold standard in 1931, and the United States effectively devalued its currency in 1933). Fiscal expansion, in the form of New Deal jobs and social welfare programs and increased defense spending during the onset of World War II, presumably also played a role by increasing consumers’ income and aggregate demand, but the importance of this factor is a matter of debate among scholars.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/place/The-Andy-Warhol-Museum",
    "content": "…on several continents, including the Andy Warhol Museum in Pittsburgh (2006), the 21st Century Museum of Contemporary Art in Kanazawa, Japan (2007), and the MUDAM (Musée d’Art Moderne Grand-Duc Jean) in Luxembourg (2008). For The Walthamstow Tapestry (2009), a textile work that scrolled 49 feet (15 metres) across a gallery…\n…(1939), and the Andy Warhol Museum (1994), which exhibits the works of the Pittsburgh-born artist and filmmaker. Other institutions affiliated with the organization are the Carnegie Library of Pittsburgh, which contains more than 3.3 million volumes, and the Carnegie Music Hall. The Pittsburgh Symphony Orchestra performs at Heinz Hall, a…\n…work is featured in the Andy Warhol Museum in Pittsburgh. In his will, the artist dictated that his entire estate be used to create a foundation for “the advancement of the visual arts.” The Andy Warhol Foundation for the Visual Arts was established in 1987.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/topic/Carnegie-Mellon-University/additional-info#history",
    "content": "",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/biography/William-Pitt-the-Elder",
    "content": "William Pitt, the Elder (born November 15, 1708, London—died May 11, 1778, Hayes, Kent, England) was a British statesman, twice virtual prime minister (1756–61, 1766–68), who secured the transformation of his country into an imperial power.\nPitt was born in London of a distinguished family. His mother, Lady Harriet Villiers, daughter of Viscount Grandison, belonged to the Anglo-Irish nobility; his father, Robert Pitt, member of Parliament, was the son of Thomas (“Diamond”) Pitt, governor of the East India Company’s “factory” at Madras (now Chennai), India, where he made a vast fortune and secured one of the world’s largest diamonds (sold in 1717 to the regent of France). “Diamond” Pitt had returned from India with a despotic temper made devilish with spleen and gout; he quarrelled violently with his wife and declared war on “that hellish confusion that is my family,” but he treated his grandson William with affection. Father Robert was mean and cantankerous, and the Villiers blood was notoriously unstable. William inherited the gout, as well as a haughty temper and a strain of manic depression.\nSuch was the background and the smoky, explosive inheritance that was suddenly to blaze into genius. But William’s passionate temper and Pitt truculence had to be disciplined, so he was sent to Eton College, where he acquired social polish and learned to be aloof and yet agreeable, to be politely insolent. Delicate health and the early onset of gout deprived him of field sports and hunting, but he learned to ride with a good seat and take his port wine, and he enjoyed the select company of clever and well-connected friends—the two Grenvilles (one to be Earl Temple; the other, George, to be first minister to George III), George Lyttelton, Charles Pratt (to become a follower of Pitt and, as the 1st Earl Camden, a member of his 1766 ministry), and other men who would later become influential in politics, as well as Henry Fielding, author of Tom Jones. But Pitt hated the brutal harshness of Eton and determined to have his own sons educated at home. He continued his education at Trinity College, Oxford, but left after a year without taking a degree. He then spent several months at the University of Utrecht in the Netherlands, probably studying law.\nHis classical education made him think, act, and speak in the grand Roman manner. His favourite poet was Virgil, and he never forgot the patriotic lessons of Roman history; he constantly read Cicero, the golden-tongued orator who could yet lash offenders with his indignation. Later, in Parliament, his organ-like voice could be distinctly heard outside the House. This voice, perfect timing, and splendid gestures were worthy of David Garrick, the greatest actor of the day and a personal friend; Pitt’s lean, tall, commanding figure, combined with a Roman beaky nose and hawklike eyes—large and gray but turning black when he was roused—overwhelmed all onlookers. To his countrymen he was to become almost a divine portent, a voice from the Delphic oracle.\nFor the present, possessed of a mere £100 a year, he nevertheless rejected the church, a younger son’s last resort as a career. While he was vegetating on a small family property in Cornwall, which he called a “cursed hiding-place” in one of his many letters to his adored sister and confidante, clever Nan (Ann) Pitt, help came from a politically powerful millionaire nobleman, Lord Cobham, who lived in splendour in a palatial mansion and vast park at Stowe, in Buckinghamshire, to which William and his friends paid visits. Cobham sent William abroad on “The Grand Tour” of Europe (only France and Switzerland were visited, however) and later bought him a cornetcy—a commission—in his own regiment of horse (1731).\nIn 1735 Pitt was offered one of the “pocket” boroughs his brother controlled, in Wiltshire, and entered Parliament. He belonged to the small group known as “Cobham’s cubs” and the “boy patriots,” the connection of family friends and place hunters whom Cobham was mobilizing to oppose the ministry of Sir Robert Walpole (later the 1st earl of Orford). Walpole had governed England since 1720, monopolizing patronage, and had—they thought—become too ready to compromise in foreign affairs for the sake of peace. The “patriots” joined other discontented Whigs such as John Carteret (later Earl Granville) and William Pulteney (later the 1st earl of Bath) to rally opposition forces behind Frederick Louis, prince of Wales, who was vehemently estranged from his father, King George II.\nThere were no formal political parties in the 18th century, and political power, together with the financial opportunities it brought, was a gift of patronage from a handful of landowning family oligarchies and from the monarch himself; nor was there a formal opposition in Parliament, and opposition to the king’s ministry was regarded as factious and even traitorous. Pitt’s maiden speech in Parliament was so critical of the ministry that it provoked Walpole to deprive him of his military commission, to “muzzle this terrible young cornet of horse.”\nIn 1737 the Prince of Wales made Pitt one of his court officials with a salary of £400 a year. He was still a relatively poor dependent of a powerful Whig clan but already showed an independence of mind and a readiness to appeal to public opinion outside Parliament that were new in English political life: when Walpole dismissed him from his cornetcy, he ostentatiously drove about London in a one-horse chaise to underline his poverty. His talents as an orator had already become clear. He repeatedly referred to the “voice of England,” which had to be sought outside Parliament because Parliament was so packed with placemen and sinecurists. He claimed to speak for the commercial interests and even for the colonies overseas, the latter scarcely represented in the Commons. He was using arguments that carried far beyond the close interests of the Whig families; but he made lasting and valuable friendship with the rich sugar planter-aldermen of the City of London in his opposition to Walpole’s cautious handling of the disputes with Spain over West Indian trade.\nWalpole at last fell from power in 1742 and was replaced by a ministry that included his old colleagues Thomas Pelham-Holles, the 1st duke of Newcastle, and Philip Yorke, the 1st earl of Hardwicke, with Carteret as secretary of state. Pulteney was silenced by the grant of a peerage. The “boy patriots,” of whom Pitt was the acknowledged leader, were still excluded. They opposed Carteret even more vigorously than they had Walpole. In the War of the Austrian Succession (1740–48), Pitt, a former warmonger, now vigorously opposed the sending of men and subsidies to check the French by protecting Hanover (the king’s territory in Germany) and condemned Carteret as a “Hanover troop minister”: for this he was never forgiven by his sovereign.\nPitt insisted that French power should be opposed at sea and in its colonial possessions, not on the Continent. When Carteret was forced to resign in 1744, Newcastle and his brother Henry Pelham took office and wanted to include Pitt in their ministry, but George II refused to accept him, though he did accept Cobham, Lyttelton, and Grenville. It was at this time that Pitt first appeared in Parliament swathed in bandages, on crutches, and with a huge gout boot on his foot, parading his illness. But, in the Jacobite rising of 1745 (the Forty-five Rebellion), Pitt gained new stature as the one effective statesman.\nIn February 1746 the king agreed to appoint Pitt joint vice treasurer of Ireland at £3,000 a year, and two months later he became paymaster general of the forces; he bowed very low as he kissed the king’s hands, but George wept with rage. The post of paymaster was one of the most sought after in government, with ample opportunity for corruption. There was an outcry from Pitt’s friends, who suspected he had been bought, but he proceeded to show both his contempt for moneymaking and his integrity as an honest but comparatively poor man by ostentatiously refusing to take for himself any more than the official salary of more than £4,000 a year: he put the interest that paymasters before had appropriated to themselves, together with the accounts of the paymaster’s funds, into the Bank of England and won the people’s hearts again. He introduced many reforms into the administration, and, though he supported the Pelhams’ alliance with Hanover (which was a change of tack), he tried also to strengthen British naval power.\nA legacy of £10,000 from the old duchess of Marlborough at this time, left to Pitt “for the noble defence he has made for the support of the laws of England, and to prevent the ruin of his country,” enabled him to indulge in more lavish expenditure and generosity. He spent a good deal on landscape gardening and bought a new property near London. After a furious quarrel, he became estranged from his sister Nan, who had been his hostess for years.\nWhen Henry Pelham died in office in 1754, Pitt hoped for advancement, but, after much reshuffling and intrigue, Newcastle and Henry Fox (later 1st Baron Holland) abandoned him for the sake of expediency. He then became ill and retired to a new house at Bath, groaning “I wish for nothing but a decent and innocent retreat, not to afford the world the ridiculous spectacle of being passed by every boat that navigates the river.” An invalid and an aging bachelor, he suddenly fell in love with Lady Hester Grenville, became engaged at once, and was married by November 1754. She was 33 and he 46, and she adored him, possibly from the times in her childhood when he had visited Stowe with her brothers. She was attractive, clever, patient, and eminently practical—particularly about money, arranging mortgages, satisfying creditors, and pouring away her own fortune, in his last years of grandiose extravagance, to protect him.\nIt proved to be an ideally happy marriage with a well-ordered, loving home and family (“the infantry” Pitt called them); later he was to be found making hay with them at Hayes Place, his house in Kent, going for picnics, and chasing butterflies. He magically became healthy and happy, ready for his last big parliamentary fight for high office. But first, because of his attacks on Newcastle’s ministry, he was dismissed, penniless, from the pay office in 1755. His brother-in-law Earl Temple helped with an annuity of £1,000.\nThe outbreak of the Seven Years’ War gave Pitt his supreme opportunity for statesmanship. The war began with heavy losses and considerable confusion of policy. The popular demand for Pitt became irresistible, and he declared, “I am sure I can save this country, and nobody else can.” In November 1756 he formed a ministry that excluded Newcastle, with the Duke of Devonshire as its nominal head. In June 1757 Newcastle returned to office on the understanding that he should control all the patronage and leave Pitt to conduct the war.\nPitt determined that it should be in every sense a national war and a war at sea. He revived the militia, reequipped and reorganized the navy, and sought to unite all parties and public opinion behind a coherent and intelligible war policy. He seized upon America and India as the main objects of British strategy: he sent his main expeditions to America, to ensure the conquest of Canada, and supported the East India Company and its “heaven-born general,” Robert Clive, in their struggle against the French East India Company.\nHe subsidized and reinforced the armies of Frederick the Great of Prussia to engage the French on the Continent, while the British Navy harassed the French on their own coasts, in the West Indies, and in Africa. Choosing good generals and admirals, he inspired them with a new spirit of dash and enterprise. His hand, eye, and voice were everywhere. By 1759, the “year of victories,” Horace Walpole, man of letters and son of Sir Robert Walpole, wrote with reluctant admiration, “Our bells are worn out threadbare with ringing for Victories.” Pitt, the “Great Commoner,” was known and feared throughout the world. This resolute and concerted policy was too much for Bourbon France, and, by the terms of the Treaty of Paris in 1763, Great Britain remained supreme in North America and India, held Minorca as a Mediterranean base, and won territory in Africa and the West Indies.\nPitt had given Britain a new empire besides preserving and consolidating the old. But, before the war ended, he had been forced to resign. In 1760 George III came to the throne resolved, as was his chief adviser, the Earl of Bute, to end the war. When Pitt failed to persuade his colleagues to declare war on Spain to forestall its entry into hostilities, he resigned in October 1761. He alone was not tired of war. He never considered its carnage or the ruin facing a bankrupt country. He had tended to concentrate the whole conduct of government into his own hands and worked with furious energy. His haughty manner, which alienated many, and his high-handed treatment of affairs had earned him respect and admiration but little friendship.\nWhen his resignation was accompanied by a peerage for Hester and an annuity to her of £3,000, there was again an outburst of abuse and scurrility. Just as when he had accepted the pay office, this acceptance of a peerage and a pension for his wife seemed to be the result of a political bargain. As rewards for his immense services they were meagre enough, but it was some measure of his unique reputation for highminded disinterestedness that his accepting them should provoke so much bitter disillusionment. His effigy was burned, and Hester was reviled as Lady Cheat’em. Pitt attacked the terms of the Treaty of Paris as an inadequate recognition of Great Britain’s worldwide success. But, though his popular appeal was soon restored, his career as war minister was over.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/list/inventors-and-inventions-of-the-industrial-revolution",
    "content": "The Industrial Revolution (1750–1900) forever changed the way people in Europe and the United States live and work. These inventors and their creations were at the forefront of a new society.\nThe creation of the following ingenious machines made possible the mass production of high-quality cotton and woolen thread and yarn and helped transform Great Britain into the world’s leading manufacturer of textiles in the second half of the 18th century.\nThe spinning jenny. About 1764 James Hargreaves, a poor uneducated spinner and weaver living in Lancashire, England, conceived a new kind of spinning machine that would draw thread from eight spindles simultaneously instead of just one, as in the traditional spinning wheel. The idea reportedly occurred to him after his daughter Jenny accidentally knocked over the family’s spinning wheel. The spindle continued to turn even as the machine lay on the floor, suggesting to Hargreaves that a single wheel could turn several spindles at once. He obtained a patent for the spinning jenny in 1770.\nThe water frame. So called because it was powered by a waterwheel, the water frame, patented in 1769 by Richard Arkwright, was the first fully automatic and continuously operating spinning machine. It produced stronger and greater quantities of thread than the spinning jenny did. Because of its size and power source, the water frame could not be housed in the homes of spinners, as earlier machines had been. Instead, it required a location in a large building near a fast-running stream. Arkwright and his partners built several such factories in the mountainous areas of Britain. Spinners, including child laborers, thereafter worked in ever-larger factories rather than in their homes.\nThe spinning mule. About 1779 Samuel Crompton invented the spinning mule, which he designed by combining features of the spinning jenny and the water frame. His machine was capable of producing fine as well as coarse yarn and made it possible for a single operator to work more than 1,000 spindles simultaneously. Unfortunately, Crompton, being poor, lacked the money to patent his idea. He was cheated out of his invention by a group of manufacturers who paid him much less than they had promised for the design. The spinning mule was eventually used in hundreds of factories throughout the British textile industry.\nThrough its application in manufacturing and as a power source in ships and railway locomotives, the steam engine increased the productive capacity of factories and led to the great expansion of national and international transportation networks in the 19th century.\nWatt’s steam engine. In Britain in the 17th century, primitive steam engines were used to pump water out of mines. In 1765 Scottish inventor James Watt, building on earlier improvements, increased the efficiency of steam pumping engines by adding a separate condenser, and in 1781 he designed a machine to rotate a shaft rather than generate the up-and-down motion of a pump. With further improvements in the 1780s, Watt’s engine became a primary power source in paper mills, flour mills, cotton mills, iron mills, distilleries, canals, and waterworks, making Watt a wealthy man.\nThe steam locomotive. British engineer Richard Trevithick is generally recognized as the inventor of the steam railway locomotive (1803), an application of the steam engine that Watt himself had once dismissed as impractical. Trevithick also adapted his engine to propel a barge by turning paddle wheels and to operate a dredger. Trevithick’s engine, which generated greater power than Watt’s by operating at higher pressures, soon became common in industrial applications in Britain, displacing Watt’s less-efficient design. The first steam-powered locomotive to carry paying passengers was the Active (later renamed the Locomotion), designed by English engineer George Stephenson, which made its maiden run in 1825. For a new passenger railroad line between Liverpool and Manchester, completed in 1830, Stephenson and his son designed the Rocket, which achieved a speed of 36 miles (58 km) per hour.\nTwo important inventions improved the safety and efficiency of steam trains and railways in the late 19th century. In 1897 American inventor Andrew J. Beard patented the Jenny coupler, a device that automatically connected railway cars. It revolutionized the railroad industry by eliminating the need for brakemen to manually couple the cars, a dangerous job that often resulted in serious injuries. About the same time, Canadian American inventor Elijah McCoy patented a lubricating device for steam engine bearings. His portable “lubricating cup,” or “McCoy lubricator,” automatically dripped oil onto engine bearings while the train was in motion, keeping the engine properly lubricated. This device became extremely popular as it allowed trains to run continuously without having to stop frequently for lubrication.\nSteamboats and steamships. Steamboats and steamships were pioneered in France, Britain, and the United States in the late 18th and early 19th centuries. The first commercially successful paddle steamer, the North River Steamboat, designed by American engineer Robert Fulton, traveled up the Hudson River from New York City to Albany, New York, in 1807 at a speed of about 5 miles (8 km) per hour. Eventually, ever larger steamboats delivered cargo as well as passengers over hundreds of miles of inland waterways of the eastern and central United States, especially the Mississippi River. The first transoceanic voyage to employ steam power was completed in 1819 by the Savannah, an American sailing ship with an auxiliary steam-powered paddle. It sailed from Savannah, Georgia, to Liverpool, England, in a little more than 27 days, though its paddle operated for only 85 hours of the voyage. By the second half of the 19th century, ever larger and faster steamships were regularly carrying passengers, cargo, and mail across the North Atlantic, a service dubbed “the Atlantic Ferry.”\nIn the early 19th century, scientists in Europe and the United States explored the relationship between electricity and magnetism, and their research soon led to practical applications of electromagnetic phenomena.\nElectric generators and electric motors. In the 1820s and ’30s British scientist Michael Faraday demonstrated experimentally that passing an electric current through a coil of wire between two poles of a magnet would cause the coil to turn, while turning a coil of wire between two poles of a magnet would generate an electric current in the coil (electromagnetic induction). The first phenomenon eventually became the basis of the electric motor, which converts electrical energy into mechanical energy, while the second eventually became the basis of the electric generator, or dynamo, which converts mechanical energy into electrical energy. Although both motors and generators underwent substantial improvements in the mid-19th century, their practical employment on a large scale depended on the later invention of other machines—namely, electrically powered trains and electric lighting.\nElectric railways and tramways. The first electric railway, intended for use in urban mass transit, was demonstrated by German engineer Werner von Siemens in Berlin in 1879. By the early 20th century, electric railways were operating within and between several major cities in Europe and the United States. The first electrified section of London’s subway system, called the London Underground, began operation in 1890.\nThe incandescent lamp. In 1878–79 Joseph Swan in England and later Thomas Edison in the United States independently invented a practical electric incandescent lamp, which produces continuous light by heating a filament with an electric current in a vacuum (or near vacuum). Both inventors applied for patents, and their legal wrangling ended only after they agreed to form a joint company in 1883. Edison has since been given most of the credit for the invention, because he also devised the power lines and other equipment necessary for a practical lighting system. During the next 50 years, electric incandescent lamps gradually replaced gas and kerosene lamps as the major form of artificial light in urban areas, though gas-lit street lamps persisted in Britain until the mid-20th century.\nLewis Latimer, an American inventor, patented a carbon filament in 1881 that burned for many more hours than previous designs. The innovation allowed for the production of more efficient light bulbs, thus making electric lighting more affordable and accelerating its adoption.\nTwo inventions of the 19th century, the electric telegraph and the electric telephone, made reliable instantaneous communication over great distances possible for the first time. Their effects on commerce, diplomacy, military operations, journalism, and myriad aspects of everyday life were nearly immediate and proved to be long-lasting.\nThe telegraph. The first practical electric telegraph systems were created almost simultaneously in Britain and the United States in 1837. In the device developed by British inventors William Fothergill Cooke and Charles Wheatstone, needles on a mounting plate at a receiver pointed to specific letters or numbers when electric current passed through attached wires.  Inventors Solomon G. Brown, Joseph Henry, Samuel F.B. Morse, and Alfred Vail created their own electric telegraph, Brown serving as one of the telegraph’s principal technicians, Henry having designed the necessary high intensity magnet, and Morse having conceived of the telegraph’s designs, with significant improvements by Vail. Morse created his own electric telegraph and, more famously, a universal code, since known as Morse Code, that could be used in any system of telegraphy. The code, consisting of a set of symbolic dots, dashes, and spaces, was soon adopted (in modified form to accommodate diacritics) throughout the world. A demonstration telegraph line between Washington, D.C., and Baltimore, Maryland, was completed in 1844. The first message sent on it was, “What hath God wrought!” Telegraph cables were first laid across the English Channel in 1851 and across the Atlantic Ocean in 1858. In the United States the spread of telegraphic communication through the growth of private telegraph companies such as Western Union aided the maintenance of law and order in the Western territories and the control of traffic on the railroads. What’s more, it enabled the transmission of national and international news through wire services such as the Associated Press. In 1896 Italian physicist and inventor Guglielmo Marconi perfected a system of wireless telegraphy (radiotelegraphy) that had important military applications in the 20th century.\nThe telephone. In 1876 Scottish-born American scientist Alexander Graham Bell successfully demonstrated the telephone, which transmitted sound, including that of the human voice, by means of an electric current. While Bell is credited as the primary inventor of the telephone, Lewis Latimer, an American inventor and draftsman, contributed to its development through his work on patent drawings. Latimer was hired by Bell’s patent lawyers to draft high-quality patent drawings for the telephone patent application. Bell’s device consisted of two sets of metallic reeds (membranes) and electromagnetic coils. Sound waves produced near one membrane caused it to vibrate at certain frequencies, which induced corresponding currents in the electromagnetic coil connected to it, and those currents then flowed to the other coil, which in turn caused the other membrane to vibrate at the same frequencies, reproducing the original sound waves. The first “telephone call” (successful electric transmission of intelligible human speech) took place between two rooms of Bell’s Boston laboratory on March 10, 1876, when Bell summoned his assistant, Thomas Watson, with the famous words that Bell transcribed in his notes as “Mr. Watson—Come here—I want to see you.” Initially the telephone was a curiosity or a toy for the rich, but by the mid-20th century it had become a common household instrument, billions of which were in use throughout the world.\nAmong the most consequential inventions of the late Industrial Revolution were the internal-combustion engine and, along with it, the gasoline-powered automobile. The automobile, which replaced the horse and carriage in Europe and the United States, offered greater freedom of travel for ordinary people, facilitated commercial links between urban and rural areas, influenced urban planning and the growth of large cities, and contributed to severe air-pollution problems in urban areas.\nThe internal-combustion engine. The internal-combustion engine generates work through the combustion inside the engine of a compressed mixture of oxidizer (air) and fuel, the hot gaseous products of combustion pushing against moving surfaces of the engine, such as a piston or a rotor. The first commercially successful internal-combustion engine, which used a mixture of coal gas and air, was constructed about 1859 by Belgian inventor Étienne Lenoir. Initially expensive to run and inefficient, it was significantly modified in 1878 by German engineer Nikolaus Otto, who introduced the four-stroke cycle of induction-compression-firing-exhaust. Because of their greater efficiency, durability, and ease of use, gas-powered engines based on Otto’s design soon replaced steam engines in small industrial applications. The first gasoline-powered internal-combustion engine, also based on Otto’s four-stroke design, was invented by German engineer Gottlieb Daimler in 1885. Soon afterward, in the early 1890s, another German engineer, Rudolf Diesel, constructed an internal-combustion engine (the diesel engine) that used heavy oil instead of gasoline and was more efficient than the Otto engine. It was widely used to power locomotives, heavy machinery, and submarines.\nThe automobile. Because of its efficiency and light weight, the gasoline-powered engine was ideal for light vehicular locomotion. The first motorcycle and motorcar powered by an internal-combustion engine were constructed by Daimler and Karl Benz, respectively, in 1885. By the 1890s a nascent industry in continental Europe and the United States was producing increasingly sophisticated automobiles for mostly wealthy customers. Less than 20 years later American industrialist Henry Ford perfected assembly-line methods of manufacturing to produce millions of automobiles (especially the Model T) and light trucks annually. The great economies of scale he achieved made automobile ownership affordable for Americans of average income, a major development in the history of transportation.\nNew farm machinery coupled with chemical and agronomic advances helped transform agriculture into a high-yield industrial enterprise. This boosted food production capacity during the Industrial Revolution which helped to feed the rising population.\nThe steel plow. Invented by John Deere in 1837, the steel plow was a major improvement over earlier iron and wooden plows, as it was lighter and stronger and able to break up dense prairie soil in the American Midwest. The plow’s sharp point and smooth surface reduced friction and enabled farmers to cultivate more acres per day with less draft power, contributing to increased crop yields and allowing farming to expand westward into new territories. Within two decades of its invention, over 10,000 steel plows were being produced annually by Deere’s company in the United States.\nThe mechanical reaper. Developed by Cyrus McCormick in 1831, the mechanical reaper greatly increased harvesting efficiency, compared with handheld scythes. McCormick's horse-drawn machine used a cutting bar to cut ripe grain, a platform to carry the cut stems, and a reel to pull them onto the platform for bundling. By automating the cutting and threshing processes, the reaper enabled farmers to quadruple the amount of grain harvested per day, displacing the handheld scythe which had been in use for over 5,000 years.\nMultiple-effect evaporator. Chemist Norbert Rillieux invented the efficient multiple-effect evaporator, which used steam heat and vacuum chambers to boil sugar cane juice in stages. This removed water from the juice while retaining sugar crystals, and in doing so it revolutionized the sugar industry. Rillieux’s apparatus, patented in 1846, cut fuel consumption and boosted sugar yields compared to old open-kettle methods, enabling Louisiana sugar plantations to lower production costs and improve quality and profits. Rillieux’s pioneering work in industrial heat transfer and steam technology paved the way for many later developments, and his innovative refining process continues to be used in chemical processing, pharmaceutical manufacturing, food and beverage production, and wastewater purification.\nSynthetic production. American agronomist George Washington Carver is best remembered for promoting crop-rotation methods to restore soil nutrients depleted by cotton monoculture and for his advances in synthetic production. Conducting research and trials focused on nitrogen-fixing plants like peanuts, soybeans, and sweet potatoes, Carver used synthetic production to develop hundreds of new uses for standard agricultural crops. With regard to peanuts, he created over 300 products, including milk and oil substitutes, paper, and wood stains. His work provided affordable food sources for poor farmers and helped reduce Southern agriculture’s reliance on cotton.\nMass production techniques coupled with expanded distribution networks allowed a huge range of consumer goods, from clothing to cosmetics, to be manufactured affordably and accessed by the general population.\nThe sewing machine. Elias Howe and Isaac Singer patented sewing machines in the 1840s and ’50s. Howe invented and patented the first practical sewing machine that used lock-stitching. This machine could produce 300 stitches per minute compared to a professional seamstress’s 40–50 stitches per minute but only in a straight line, a severe limitation. As sewing technology improved, garment factories were able to quickly and cheaply mass-produce fashionable clothing for the general population. The practical sewing machine was later available for home use, becoming a staple of self-reliance of the American family. In 1851 Singer designed an improved model that utilized Howe’s patented lock-stitch method and a new up-and-down motion mechanism. Howe was able to reestablish his rights in 1854 after a five-year legal battle against Singer and others, whereupon he received royalties on all U.S.-made sewing machines.\nThe shoe-lasting machine. The American inventor Jan Ernst Matzeliger created the shoe-lasting machine in 1883. Before then, shoes were individually lasted by skilled artisans, which limited their availability and affordability. Matzeliger’s machine could produce 150–700 pairs of shoes per day, compared with 50 per artisan, allowing inexpensive mass-produced shoes to become widely available.\nAniline dyes. The English chemist William Henry Perkin first patented aniline dyes in 1856. These artificial dyes allowed for vibrantly colored fabrics to be mass-produced in factories for the first time. Previously, dyes were derived from natural sources and limited in hue. Perkin accidentally created the synthesis of aniline purple, or mauve, in his experiment to produce quinine, a medical drug. The aniline dye process opened the door for affordable brightly colored clothing to reach mainstream consumers. Its immense popularity was dubbed “mauveine measles” and even reached the British royal family; Queen Victoria appeared in a mauveine silk dress at the International Exhibition of 1862, otherwise known as the Great London Exhibition.\nHair products. In the early 1900s Madam C.J. Walker (born Sarah Breedlove) developed a line of cosmetics and hair products for African American women, specializing in pomades and shampoos. Through savvy marketing and the training of a national network of more than 25,000 sales agents, she built a business empire spanning from the United States to Central America to the Caribbean, thus contributing to cosmetics’ transition from small-scale production to mass availability as consumer goods.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/topic/Pittsburgh-Symphony-Orchestra",
    "content": "Pittsburgh Symphony Orchestra (PSO ), American symphony orchestra based in Pittsburgh. It was founded as the Pittsburgh Orchestra in 1896; its first conductor was Frederick Archer (1896–98). Music director Victor Herbert (1898–1904) was followed by permanent conductor Emil Paur (1904–10), after which the orchestra was disbanded until 1926, when the newly formed autonomous Pittsburgh Symphony Society presented a concert conducted by Richard Hageman. From 1927 through 1930, the PSO was led by Elias Breeskin, concertmaster and later conductor, and by such guest conductors as Eugene Goossens and Walter Damrosch. Permanent conductor Antonio Modarelli (1930–37) was succeeded by Otto Klemperer (1937–38), who reorganized the membership and revitalized the orchestra. Music directors have been Fritz Reiner (1938–48), William Steinberg (1952–76; emeritus 1976–78), André Previn (1976–84), Lorin Maazel (1988–96; music consultant 1984–88), Mariss Jansons (1995–2004), and Manfred Honeck (2008– ).\nFrom 1936, PSO concerts were broadcast nationwide over radio. During Reiner’s tenure the PSO made its first foreign tour and its first commercial recording. Steinberg continued Reiner’s work of building the PSO into one of the finest orchestras in the United States, expanding its repertoire to include music from the Baroque period through such central European modernists as Arnold Schoenberg, Alban Berg, Anton Webern, and Gustav Mahler. Under Previn and Maazel the PSO made international tours and championed English, Russian, and late 20th-century music. From the 1960s the orchestra made successful tours of Europe, Asia, and South America. The PSO offered community outreach programs, children’s concerts, and great-performers series. From 1995 to 2012 Marvin Hamlisch served as the first principal conductor of the Pittsburgh Symphony Pops.\nPittsburgh, city, seat (1788) of Allegheny county, southwestern Pennsylvania, U.S. The city is located at the confluence of the Allegheny and Monongahela rivers, which unite at the point of the “Golden Triangle” (the business district) to form the Ohio River. A city of hills, parks, and valleys, it is the centre of an urban industrial complex that includes the surrounding cities of Aliquippa (northwest), New Kensington (northeast), McKeesport (southeast), and Washington (southwest) and the borough of Wilkinsburg (east). Inc. borough, 1794; city, 1816. Area city, 58 square miles (150 square km). Pop. (2010) 305,704; Pittsburgh Metro Area, 2,356,285; (2020) 302,971; Pittsburgh Metro Area, 2,370,930.\nAlgonquian- and Iroquoian-speaking peoples were early inhabitants of the region. The conflict between the British and French over territorial claims in the area was settled in 1758 when General John Forbes and his British and colonial army expelled the French from Fort Duquesne (built 1754). Forbes named the site for the British statesman William Pitt the Elder. The British built Fort Pitt (1761) to ensure their dominance at the source of the Ohio. Settlers began arriving after Native American forces led by Ottawa chief Pontiac were defeated in 1763; an agreement subsequently was made between Native American groups and the Penn family, and a boundary dispute was ended between Pennsylvania and Virginia. Pittsburgh was laid out (1764) by John Campbell in the area around the fort (now the Golden Triangle). Following the American Revolution, the town became an outfitting point for settlers traveling westward down the Ohio River.\nPittsburgh’s strategic location and wealth of natural resources spurred its commercial and industrial growth in the 19th century. A blast furnace, erected by George Anschutz about 1792, was the forerunner of the iron and steel industry that for more than a century was the city’s economic mainstay; by 1850 Pittsburgh was known as the “Iron City.” The Pennsylvania Canal and the Portage Railroad, both completed in 1834, opened vital markets for trade and shipping. The city suffered a great loss in 1845 when some 24 blocks of businesses, homes, churches, and other buildings were destroyed by fire.\nAfter the American Civil War, great numbers of European immigrants swelled Pittsburgh’s population, and industrial magnates such as Andrew Carnegie, Henry Clay Frick, and Thomas Mellon built their steel empires there. The city became the focus of historic friction between labour and management, and the American Federation of Labor was born there in 1881.\nBy 1900 the city’s population had reached 321,616. Growth continued nearly unabated through World War II, the war years bringing a particularly great boon for the economy. The population crested at more than 675,000 in 1950, after which it steadily declined; by the end of the century, it had returned almost to the 1900 level. Most citizens were still of European ancestry, but the growing African American proportion of the population exceeded one-fourth. During the period of economic and population growth, Pittsburgh had come to epitomize the grimy, polluted industrial city. After the war, however, the city undertook an extensive redevelopment program that emphasized smoke-pollution control, flood prevention, and sewage disposal. In 1957 it became the first American city to generate electricity by nuclear power.\nBy the late 1970s and early ’80s, the steel industry had virtually disappeared—a result of foreign competition and decreased demand. Many of the surrounding mill towns were laid to waste by unemployment, becoming a symbol of the notorious Rust Belt, the U.S. region where steelmaking and manufacturing once thrived before succumbing to widespread unemployment and poverty. Pittsburgh, however, successfully diversified its economy through more emphasis on light industries—though metalworking, chemicals, and plastics remained important—and on such high-technology industries as computer software, industrial automation (robotics), and biomedical and environmental technologies. Numerous industrial research laboratories were established in the area, and the service sector became increasingly important. Pittsburgh long has been one of the nation’s largest inland ports, and it remains a leading transportation centre.\nMuch of the Golden Triangle has been rebuilt and includes Point State Park (containing Fort Pitt Blockhouse and Fort Pitt Museum), the Gateway Center (site of several skyscrapers and a garden), and the David L. Lawrence Convention Center. The University of Pittsburgh was chartered in 1787. Other educational institutions include Carnegie Mellon (1900), Duquesne (1878), Point Park (1960), Chatham (1869), and Carlow (1929) universities and two campuses of the Community College of Allegheny County (1966).\nCentral to the city’s cultural life is the Carnegie Museums of Pittsburgh (formerly Carnegie Institute), an umbrella organization consisting of a number of institutions. Its museums include those for the fine arts and natural history (both founded in 1895), the Carnegie Science Center (1991), which now also houses the Henry Buhl, Jr., Planetarium and Observatory (1939), and the Andy Warhol Museum (1994), which exhibits the works of the Pittsburgh-born artist and filmmaker. Other institutions affiliated with the organization are the Carnegie Library of Pittsburgh, which contains more than 3.3 million volumes, and the Carnegie Music Hall. The Pittsburgh Symphony Orchestra performs at Heinz Hall, a restored movie theatre.\nPhipps Conservatory and Botanical Gardens (1893), which covers 15 acres (6 hectares), is noted for its extensive greenhouses. The city’s zoo, in the northeastern Highland Park neighbourhood, includes an aquarium. Two new sports venues opened in 2001 on the north bank of the Allegheny opposite the Golden Triangle: PNC Park is home of the Pirates, the city’s professional baseball team, and Acrisure Stadium houses the Steelers, its professional football team. The Penguins, Pittsburgh’s professional ice hockey team, plays at PPG Paints Arena. Popular summertime attractions include riverboat excursions on Pittsburgh’s waterways and Kennywood, an amusement park southeast of the city in West Mifflin.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/event/American-Revolution",
    "content": "The American Revolution was an insurrection carried out by 13 of Great Britain’s North American colonies that began in 1775 and ended with a peace treaty in 1783. The colonies won political independence and went on to form the United States of America. The war followed more than a decade of growing estrangement between the British crown and a large and influential segment of its North American colonies that was caused by British attempts to assert greater control over colonial affairs after having long adhered to a policy of salutary neglect.\nUntil early in 1778 the conflict was a civil war within the British Empire, but afterward it became an international war as France (in 1778) and Spain (in 1779) joined the colonies against Britain. Meanwhile, the Netherlands, which provided both official recognition of the United States and financial support for it, was engaged in its own war against Britain (see Anglo-Dutch Wars). From the beginning, sea power was vital in determining the course of the war, lending to British strategy a flexibility that helped compensate for the comparatively small numbers of troops sent to America and ultimately enabling the French to help bring about the final British surrender at Yorktown in 1781.\nThe American colonies fought the war on land with essentially two types of organization: the Continental (national) Army and the state militias. The total number of the former provided by quotas from the states throughout the conflict was 231,771 soldiers, and the militias totaled 164,087. At any given time, however, the American forces seldom numbered over 20,000; in 1781 there were only about 29,000 insurgents under arms throughout the country. The war was therefore one fought by small field armies. Militias, poorly disciplined and with elected officers, were summoned for periods usually not exceeding three months. The terms of Continental Army service were only gradually increased from one to three years, and not even bounties and the offer of land kept the army up to strength. Reasons for the difficulty in maintaining an adequate Continental force included the colonists’ traditional antipathy toward regular armies, the objections of farmers to being away from their fields, the competition of the states with the Continental Congress to keep men in the militia, and the wretched and uncertain pay in a period of inflation.\nBy contrast, the British army was a reliable steady force of professionals. Since it numbered only about 42,000, heavy recruiting programs were introduced. Many of the enlisted men were farm boys, as were most of the Americans, while others came from cities where they had been unable to find work. Still others joined the army to escape fines or imprisonment. The great majority became efficient soldiers as a result of sound training and ferocious discipline. The officers were drawn largely from the gentry and the aristocracy and obtained their commissions and promotions by purchase. Though they received no formal training, they were not so dependent on a book knowledge of military tactics as were many of the Americans. British generals, however, tended toward a lack of imagination and initiative, while those who demonstrated such qualities often were rash.\nBecause troops were few and conscription unknown, the British government, following a traditional policy, purchased about 30,000 troops from various German princes. The Lensgreve (landgrave) of Hesse furnished approximately three-fifths of that total. Few acts by the crown roused so much antagonism in America as that use of foreign mercenaries.\nThe colony of Massachusetts was seen by King George III and his ministers as the hotbed of disloyalty. After the Boston Tea Party (December 16, 1773), Parliament responded with the Intolerable Acts (1774), a series of punitive measures that were intended to cow the restive population into obedience. The 1691 charter of the Massachusetts Bay Colony was abrogated, and the colony’s elected ruling council was replaced with a military government under Gen. Thomas Gage, the commander of all British troops in North America. At Gage’s headquarters in Boston, he had four regiments—perhaps 4,000 troops—under his command, and Parliament deemed that force sufficient to overawe the population in his vicinity. William Legge, 2nd earl of Dartmouth, secretary of state for the colonies, advised Gage that\nthe violence committed by those, who have taken up arms in Massachusetts, have appeared to me as the acts of a rude rabble, without plan, without concert, without conduct.\nFrom London, Dartmouth concluded that\na small force now, if put to the test, would be able to conquer them, with greater probability of success, than might be expected of a larger army, if the people should be suffered to form themselves upon a more regular plan.\nGage, for his part, thought that no fewer than 20,000 troops would be adequate for such an endeavor, but he acted with the forces he had at hand. Beginning in the late summer of 1774, Gage attempted to suppress the warlike preparations throughout New England by seizing stores of weapons and powder. Although the colonials were initially taken by surprise, they soon mobilized. Groups such as the Sons of Liberty uncovered advance details of British actions, and Committees of Correspondence aided in the organization of countermeasures. Learning of a British plan to secure the weapons cache at Fort William and Mary, an undermanned army outpost in Portsmouth, New Hampshire, Boston’s Committee of Correspondence dispatched Paul Revere on December 13, 1774, to issue a warning to local allies.\nThe following day, several hundred soldiers assembled and stormed the fort, capturing the six-man garrison, seizing a significant quantity of powder, and striking the British colors; a subsequent party removed the remaining cannons and small arms. That act of open violence against the crown infuriated British officials, but their attempts to deprive the incipient rebellion of vital war matériel over the following months were increasingly frustrated by colonial leaders who denuded British supply caches and sequestered arms and ammunition in private homes.\nOn April 14, 1775, Gage received a letter from Dartmouth informing him that Massachusetts had been declared to be in a state of open revolt and ordering him to “arrest and imprison the principal Actors and Abettors in the [Massachusetts] Provincial Congress.” Gage had received his orders, but the colonials were well aware of his intentions before he could act.\nOn April 16 Revere rode to Concord, a town 20 miles (32 km) northwest of Boston, to advise local compatriots to secure their military stores in advance of British troop movements. Two nights later Revere rode from Charlestown—where he confirmed that the local Sons of Liberty had seen the two lanterns that were posted in Boston’s Old North Church, signaling a British approach across the Charles River—to Lexington to warn that the British were on the march.\nRevolutionary leaders John Hancock and Samuel Adams fled Lexington to safety, and Revere was joined by fellow riders William Dawes and Samuel Prescott. The trio were apprehended outside Lexington by a British patrol, but Prescott escaped custody and was able to continue on to Concord. Revere’s “midnight ride” provided the colonists with vital information about British intentions, and it was later immortalized in a poem by Henry Wadsworth Longfellow.\nSome 700 British troops spent the evening of April 18, 1775, forming ranks on Boston Common, with orders to seize the colonial armory at Concord. The lengthy public display ensured that Gage had lost any chance at secrecy, and by the time the force had been transported across the Charles River to Cambridge it was 2:00 AM the following morning.\nThe march to Lexington was an exercise in misery. It began in a swamp, and the British were forced to wade through brackish water that was, in places, waist deep. By the time the soaked infantry members arrived in Lexington about 5:00 AM, 77 minutemen were among those who had assembled on the village green. Officers on both sides ordered their troops to hold their positions but not to fire their weapons.\nIt is unclear who fired “the shot heard ’round the world,” but it sparked a skirmish that left eight Americans dead. The colonial force evaporated, and the British moved on to Concord, where they were met with determined resistance from hundreds of militia members. Now outnumbered and running low on ammunition, the British column was forced to retire to Boston. On the return march, American snipers took a deadly toll on the British, and only the timely arrival of 1,100 reinforcements prevented the retreat from becoming a rout. Those killed and wounded at the Battles of Lexington and Concord numbered 273 British and 95 Americans.\nRebel militia then converged on Boston from all over New England, while London attempted to formulate a response. Generals Sir William Howe, Sir Henry Clinton, and John Burgoyne were dispatched at once with reinforcements, and Charles Cornwallis followed later. Those four commanders would be identified with the conduct of the principal British operations.\nThe Continental Congress in Philadelphia, acting for the 13 colonies, voted for general defensive measures, called out troops, and appointed George Washington of Virginia commander in chief. Before Washington could take charge of the 15,000 colonial troops laying siege to the British garrison in Boston, Gage ordered Howe to drive the Americans from the heights in Charlestown.\nThe Americans provoked the assault by entrenching on Breed’s Hill, the lower of two hills overlooking the British position. The placement of American artillery on the heights would have made the British position in Boston untenable, so on June 17, 1775, Howe led a British frontal assault on the American fortifications.\nIn the misleadingly named Battle of Bunker Hill (Breed’s Hill was the primary locus of combat), Howe’s 2,300 troops encountered withering fire while storming the rebel lines. The British eventually cleared the hill but at the cost of more than 40 percent of the assault force, and the battle was a moral victory for the Americans.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/topic/Pittsburgh-Penguins",
    "content": "Pittsburgh Penguins, American professional ice hockey team based in Pittsburgh, Pennsylvania. The Penguins have won the Stanley Cup five times (1991, 1992, 2009, 2016, and 2017).\nFounded during the 1967 National Hockey League (NHL) expansion, the Penguins took their name from the igloolike appearance of Mellon Arena, where the team played from its inception through the 2009–10 season. After finding moderate success in the 1970s and then struggling throughout the early 1980s, the Penguins drafted Mario Lemieux in 1984, who was an instant offensive force on the ice and would become one of the greatest players in the history of the game. As the club drafted and traded for other standout players, the Penguins became a regular presence in the playoffs in the late 1980s. After the addition of right wing Jaromir Jagr, they won both the 1991 and 1992 Stanley Cup championships.\nIn 1993 team captain Lemieux was diagnosed with Hodgkin disease; however, he overcame the disease and not only returned to the team after a year off but also led the league in goals during the 1995–96 season. Following Lemieux’s initial retirement in 1997, Jagr became team captain and won the league scoring title four years in a row (from 1997–98 to 2000–01).\nThe Penguins franchise struggled financially, and the team filed for bankruptcy in 1998. The following year Lemieux converted years of unpaid salary to equity and became a part owner of the team. In 2000 he left retirement and became the first player-owner in the history of the NHL before retiring again midway through the 2005–06 season.\nIn 2005 a new Penguins era began with the drafting of Sidney Crosby, who in 2006–07 was the NHL’s leading scorer and, at age 19, became the youngest team captain in NHL history. In the 2007–08 season Crosby and second-year star Evgeni Malkin led the Penguins to the Stanley Cup finals, which they lost to the Detroit Red Wings. The following season Malkin led the NHL in points while Crosby finished third. That year the Penguins finished fourth in the Atlantic Division but advanced again to the Stanley Cup finals, this time defeating the Red Wings in seven games.\nIn the six seasons after the 2009 Stanley Cup win, Pittsburgh remained one of the best teams in the NHL, running off a franchise record three straight seasons with at least 100 points (a team receives two points for a victory, one for an overtime loss, and none for a regulation loss) between 2009–10 and 2011–12 and winning three division titles. However, the team failed to advance past the conference finals over that span, losing to a lower-seeded team in five of the six postseasons. The team broke through in 2015–16, ending the regular season on a hot streak that carried over into the postseason, during which the Penguins captured another Stanley Cup title (over the San Jose Sharks in six games). The Penguins tallied the second most points in the NHL (111) in 2016–17, and in the postseason the team won seven-game series in both the second round and Eastern Conference finals to return to the Stanley Cup finals, where they faced the Nashville Predators. The Penguins then defeated the Predators in six games, becoming the first NHL team to win consecutive Stanley Cups since 1998. Pittsburgh returned to the playoffs in 2018 but lost in the second round, and in 2019 the team was swept in its first-round series. The next two NHL seasons were shortened due to the COVID-19 pandemic. The Penguins failed to advance past the qualifying round of the 2019–20 playoffs, and the following year the team lost in the first round of the postseason.\nSidney Crosby (born August 7, 1987, Cole Harbour, Nova Scotia, Canada) is a Canadian ice hockey player who in 2007 became the youngest captain of a National Hockey League (NHL) team and who led the Pittsburgh Penguins to three Stanley Cup championships (2009, 2016, and 2017).\nCrosby, the son of a goaltender drafted by the Montreal Canadiens, was able to skate by age three. In his sophomore year of high school in Faribault, Minnesota, he scored 72 goals and had 90 assists in 57 games. This feat gained the attention of ice hockey legend Wayne Gretzky, who speculated that his own records would one day be surpassed by Crosby. In 2003 Rimouski Océanic, a Quebec Major Junior Hockey League team, drafted Crosby, who went on to score 120 goals and tally 183 assists in 121 regular-season games over two years. Each year he was named Canada’s top junior player. He also joined the Canadian National Junior Hockey team and became the youngest player to score a goal for the national team.\nIn 2005 the Penguins selected the 18-year-old Crosby as the top pick in that year’s NHL draft. Expectations were high for the young player, who drew numerous comparisons to Gretzky (Crosby was dubbed “The Next One,” a variation on Gretzky’s nickname “The Great One”). By the end of his first season (2005–06), Crosby had become the youngest NHL player to score at least 100 points (goals plus assists) in a single season.\nCrosby’s second season saw him break more records. For scoring 120 points in 79 games, he won the Art Ross Trophy, becoming its youngest recipient. He was the youngest player since Gretzky (in 1980) to register a six-point game, and he became the second youngest player ever (again behind Gretzky) to receive the Hart Trophy, as the NHL’s most valuable player. Crosby was named captain of the Penguins in 2007, making him the youngest captain in NHL history. During the 2007–08 season Crosby helped lead the Penguins to the Stanley Cup finals, though the team lost to the Detroit Red Wings in six games. The following year Crosby finished third in the NHL with 103 points; the Penguins once again advanced to the finals against the Red Wings, this time winning the championship in seven games.\nIn 2011 his career nearly came to a premature end when he suffered a concussion after an on-ice hit in January. Crosby missed the remainder of the 2010–11 NHL season, and there was speculation that postconcussion problems might prevent him from returning to hockey. After a prolonged rehabilitation, Crosby rejoined the Penguins’ lineup in November 2011, but he played for just two weeks before he was again sidelined by a recurrence of concussion-like symptoms. He returned in March 2012. His highly publicized injury led to increased public discussion about—and agitation to improve—player safety in the NHL.\nCrosby missed 12 games of the lockout-shortened 2012–13 NHL season because of a broken jaw, and he still managed to lead the league with 1.56 points per game. However, the top-seeded Penguins were swept out of the following postseason by the Boston Bruins in the conference finals, a series in which Crosby failed to score a point. He won his second career Art Ross Trophy for leading the NHL in points (104) during the 2013–14 season, a feat that also earned him a second Hart Trophy. His postseason scoring troubles continued, however, as he tallied a single goal and eight assists in 13 playoff games, while the Penguins were upset in the conference semifinals. Crosby tallied 84 points in 2014–15—the lowest total of his career in a season not shortened by injury or labour issues—but still managed to lead the NHL with 1.09 points per game. The Penguins again had a disappointing season, however, as the talent-laden team barely qualified for the playoffs and was quickly eliminated in the first round.\nCrosby helped the Penguins return to the upper echelon of the NHL in 2015–16 while scoring 85 points over the course of the regular season. He then tallied 19 points in 24 postseason games while leading the Penguins to their second Stanley Cup victory of his captaincy, earning the Conn Smythe Trophy as the postseason’s most valuable player for his efforts. Crosby led the NHL in goals scored (44) during the 2016–17 season, as the Penguins posted the second best record in the league. In the following postseason he helped Pittsburgh again advance to the Stanley Cup finals, where the team faced the Nashville Predators. The Penguins defeated the Predators in six games, and Crosby—who had scored eight goals and tallied 19 assists over the course of the playoffs—won a second consecutive Conn Smythe Trophy. Crosby kept the Penguins among the top teams in the NHL during the 2017–18 season, but the franchise’s run of Stanley Cup victories ended with a loss in the second round of the playoffs. He continued his excellent individual play in 2018–19, scoring 100 points during the season, but the Penguins were swept in the first round of the playoffs. The next two seasons were shortened because of the COVID-19 pandemic. Crosby helped the team qualify for the postseason in 2020–21, but the Penguins again lost in the opening round. Before the start of the 2021–22 season, he had wrist surgery that caused him to miss the opening month of play.\nIn addition to his NHL accomplishments, Crosby was a key member of the Canadian men’s hockey team at the 2010 Olympic Winter Games in Vancouver. Canada took the gold medal as Crosby scored the game-winning overtime goal in the final against the United States. He added a second Olympic gold medal at the 2014 Winter Games in Sochi, Russia.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/place/Pennsylvania-state",
    "content": "Pennsylvania, constituent state of the United States of America, one of the original 13 American colonies. The state is approximately rectangular in shape and stretches about 300 miles (480 km) from east to west and 150 miles (240 km) from north to south. It is bounded to the north by Lake Erie and New York state; to the east by New York and New Jersey; to the south by Delaware, Maryland, and West Virginia; and to the west by the panhandle of West Virginia and by Ohio. Harrisburg, nestled in the foothills of the Appalachian Mountains, is the capital.\nPennsylvania is classified as a Middle Atlantic state, along with New York, New Jersey, Delaware, and Maryland. Its central location on the Eastern Seaboard is sometimes said to be the source of its nickname, the Keystone State. It does not, however, touch the Atlantic Ocean at any point. Water nonetheless has been nearly as crucial in the state’s growth as the wealth of its earth. The Delaware River forms the boundary between Pennsylvania and New Jersey. In the northwest a small panhandle separates Ohio and New York and forms a 40-mile (65-km) waterfront on Lake Erie, giving the state access to the iron ore barges and other commerce of the Great Lakes.\nThe state has two great metropolitan areas. Philadelphia is a part of the East Coast population belt stretching from Boston to Norfolk, Virginia. It is a major harbour on the Delaware River and one of the world’s busiest shipping centres. In the west, Pittsburgh lies on the eastern edge of the great industrial region extending along the Great Lakes plains to Chicago. Area 46,054 square miles (119,280 square km). Population (2020) 13,002,700; (2023 est.) 12,961,683.\nThe landforms of Pennsylvania had their origin about 500 million years ago, when a vast interior sea, up to several hundred miles wide, occupied the area from New England to Alabama. For about 250 million years, the rivers originating from an extensive mountain chain on the east poured sediments into the great Appalachian downwarp basin. Great swamps prevailed in southwestern Pennsylvania for millions of years and provided the vegetation that ultimately became the coal beds of the area.\nBeginning about 250 million years ago, plate-tectonic movement folded the flat-lying sediment into upwarps and downwarps. The heat created by this pressure also metamorphosed the rocks, changing the sandstone into quartzite, limestone into marble, and granite into gneiss. The pressure from the plate movement was confined to southeastern Pennsylvania, creating the Piedmont and Ridge and Valley provinces. The rocks of the Appalachian Plateau remained essentially flat-lying, and the dissection of the plateau has been created by erosion.\nPennsylvania includes parts of large physiographic regions that extend beyond its borders; those regions crossing the eastern and central parts of the state parallel one another along a sweeping northeast-southwest diagonal orientation. In the southeastern part of the state is a section of the Atlantic Coastal Plain, a narrow strip of sandy low-lying land immediately adjacent to the Delaware River. This region has played a major role in Pennsylvania history. It was the site of William Penn’s settlement and the initial city of Philadelphia. Immediately inland from the Coastal Plain is the Piedmont province, a gently rolling, well-drained plain that is rarely more than 500 feet (150 metres) above sea level; the eastern part is the Piedmont Upland. The boundary between the Piedmont and the Coastal Plain is known as the fall line, with hard rock to the west and soft rock to the east. The Piedmont Lowland parallels the Piedmont Upland to its northwest. It is made of sedimentary rocks into which volcanic rocks have been intruded. Some of these volcanic rocks make ridges. The Battle of Gettysburg was fought there, the Northern army on the high ridges having the advantage over the Southern forces on the plains. The limestone rocks have weathered into fertile lowlands such as the Conestoga Lowlands of Lancaster county. Farther to the northwest lie two segments of a larger mountain range. The southern prong, extending to the Carlisle area, is the northernmost extension of the Blue Ridge system. The northern portion, known as the Reading Prong, is a small section of the larger New England topographic region. There is a major gap between these prongs.\nInland from the Blue Ridge is one of the country’s most distinctive topographic regions, the Ridge and Valley Province. It consists of long, narrow valleys and parallel ridges aligned over a long distance. As seen from space, it appears as if an enormous rake had been dragged along the backbone of the Appalachians from northeast to southwest. None of the ridges rises above the valley floor more than 1,000 feet (300 metres), and nowhere does the elevation reach 3,000 feet (900 metres). On the east is the Great Valley, which stretches more than 1,200 miles (1,930 km) from Pennsylvania to Alabama. To the west and north of the Ridge and Valley Province is the Appalachian Plateau, an area of nearly 30,000 square miles (77,700 square km). The Allegheny Front, more than 1,500 feet (450 metres) high, divides the two provinces. With no passes, it is the most formidable obstacle to east-west transportation in Pennsylvania. Almost everywhere the plateau surface has been dissected by rivers into a chaos of valleys and hills. Mount Davis is the highest point in the state at 3,213 feet (979 metres). However, elevations range from about 1,000 to 3,000 feet. On the northwest is the narrow Lake Erie Plain, which rises in a series of steps from the lakeshore to the high escarpment of the Appalachian Plateau.\nPennsylvania has three major river systems. In the east is the Delaware River, fed mainly by the Lehigh and Schuylkill rivers. In the central part of the state is the Susquehanna, draining the largest section of the state; it is a wide, shallow stream that meanders finally into Maryland and Chesapeake Bay. In the west is the Ohio River—formed by the confluence of the Allegheny (north) and Monongahela (south) rivers at Pittsburgh—from where it flows westward to the Mississippi River. Minor systems lead into Lake Erie in the northwest and the Potomac River from the southwest.\nPennsylvania generally has a humid continental climate characterized by wide fluctuations in seasonal temperatures, with prevailing winds from the west. The average temperature in July is about 70 °F (21 °C) and in January about 28 °F (−2 °C). The growing season varies from nearly 200 days in the southeast to only 90 days in the north-central part of the state. On average, about 40 inches (1,000 mm) of precipitation fall in the state annually. The daily weather is influenced by the passage of cyclone fronts in the westerly wind system.\nAt the time of the first European settlement in 1682, the land surface of Pennsylvania was covered entirely by trees. By about 1900 some three-fourths of the land had been cleared of forests, principally for farmland. Since then, vast areas of farmland have been abandoned, and much of that land has returned to forest cover. About half of the state is now wooded, although only small areas are still virgin forest. Pennsylvania occupies a transition zone between the northern and southern forests of the United States. In the north are beech, maple, birch, pine, and hemlock trees, while in the south oak, hickory, yellow poplar, walnut, and elm dominate.\nPennsylvania’s abundant wildlife makes it a leading state for hunting. Not only is there abundant small game—rabbits, pheasants, and squirrels—but tens of thousands of deer and a few hundred black bears are killed by hunters every year. The streams are stocked with fish—trout, walleye, and others—each spring to support sportfishing.\nScattered groups of Native Americans, small in number, lived in the Pennsylvania area at the time of European settlement. With the disappearance of the last recognizable Native American groups by the mid-19th century, Native Americans have become an inconspicuous part of the state’s population, numbering only some 15,000 in the early 21st century.\nBecause of the state’s rugged topography, settlement in Pennsylvania proceeded slowly. From Philadelphia, people moved west and north. However, it took about 80 years for settlement to extend west to the Ridge and Valley area of central Pennsylvania. In the western part of the state, the first settlers arrived from Virginia, traveling west by way of the Potomac River and north along the Monongahela River, reaching Pittsburgh in the 1750s. In 1840 settlers reached the last remaining unexploited area—the rugged north-central portion of the state. Thus, it took 160 years from the first settlement for the final pioneer area to be occupied.\nThere were a few Swedish, Dutch, and Finnish settlers in Pennsylvania prior to William Penn’s arrival. Initially, English Quakers (adherents of the Society of Friends) were the most important group to occupy the Delaware valley. Philadelphia, along with nearby Chester and Bucks counties, became the first thriving agricultural commercial region.\nPenn’s practice of religious toleration and his experiments with democratic forms of government encouraged other groups to settle in Pennsylvania. Germans were the first major group to immigrate to Pennsylvania. Almost entirely Protestant, they belonged to a wide array of denominations, from mainstream Lutheranism and Calvinism to various pietistic groups, including the Amish, Mennonites, Moravians, Schwenkfelders, and Dunkers. By the time of the American Revolution, the German groups (by then known as Pennsylvania Dutch, or, more correctly, Pennsylvania Germans), constituted one-third of the population.\nThe next major group to settle in Pennsylvania comprised Scotch-Irish from Northern Ireland. To find farmland they moved westward beyond the English and the Germans to the western Piedmont and the Ridge and Valley region. By the time of the Revolution, they constituted one-fourth of the total population of the colony. The fourth major group, the Irish, emigrated from their homeland in the 1840s and ’50s because of the Irish Potato Famine.\nThe Industrial Revolution spurred the development of a dynamic economy in Pennsylvania. Because the domestic population was inadequate to supply the needed labour, the state became the centre of a massive migration of Italians, Poles, Russians, Ukrainians, and people from the Balkan region, among others. Between 1890 and 1900 the population of the state rose to more than a million, largely because of immigration to the mining areas and new industrial centres. In the 20th century, African Americans began to move into the state from the South; they now constitute about one-tenth of the state’s total population. Asians, Hispanics, and Native Americans collectively constitute only a small fraction of the population.\nThe population of Pennsylvania has been nearly stable since reaching 10.5 million in 1950, having increased by only a few million in subsequent decades. Through the late 20th and early 21st centuries, the state’s annual population growth was less than 1 percent.\nThe economy of Pennsylvania has evolved through three distinct eras since the time of the first settlement. From 1682 to about 1830 a rural agricultural economy dominated. From the 1830s to about 1920 Pennsylvania developed one of the world’s great industrial economies, based on the production of iron and steel, machinery, fabricated metals, leather, textiles, and apparel. Since the 1920s service activities have increased drastically and have come to dominate employment. Of the total workforce, only a tiny fraction is now employed in the primary sector (agriculture, mining, and lumbering). About one-fifth is employed in manufacturing and construction, and the remaining workers are in the service sector.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/biography/John-Nash",
    "content": "John Nash (born June 13, 1928, Bluefield, West Virginia, U.S.—died May 23, 2015, near Monroe Township, New Jersey) was an American mathematician who was awarded the 1994 Nobel Prize for Economics for his landmark work, first begun in the 1950s, on the mathematics of game theory. He shared the prize with John C. Harsanyi and Reinhard Selten. In 2015, Nash won (with Louis Nirenberg) the Abel Prize for his contributions to the study of partial differential equations.\nNash enrolled in chemical engineering at the Carnegie Institute of Technology (later Carnegie Mellon University) in Pittsburgh before he switched to chemistry and then to mathematics, in which he finally received both bachelor’s and master’s degrees in 1948. Two years later, at age 22, he completed a doctorate at Princeton University. In 1951 he joined the faculty of the Massachusetts Institute of Technology (MIT), where he pursued research into partial differential equations. He resigned in the late 1950s after bouts of mental illness. He then began an informal association with Princeton, where he became a senior research mathematician in 1995.\nWhile he was still in graduate school, Nash published (April 1950) his first paper, “The Bargaining Problem,” in the journal Econometrica. He expanded on his mathematical model for bargaining in his influential doctoral thesis, “Non-Cooperative Games,” which appeared in September 1951 in the journal Annals of Mathematics. Nash thus established the mathematical principles of game theory, a branch of mathematics that examines the rivalries between competitors with mixed interests. Nash showed that for any finite game, all the players can arrive at an optimal outcome, known as the Nash equilibrium or the Nash solution, when considering the possible actions of the other players. Despite its practical limitations, the Nash equilibrium was widely applied by business strategists.\nNash’s research into differential equations at MIT led to his seminal paper “Real Algebraic Manifolds,” which was published in Annals of Mathematics in November 1952. His other influential work in mathematics included the Nash-Moser inverse function theorem, the Nash–De Giorgi theorem (a solution to David Hilbert’s 19th problem, which Nash undertook at the suggestion of Nirenberg), and the Nash embedding (or imbedding) theorems, which the Norwegian Academy of Science and Letters described as “among the most original results in geometric analysis of the twentieth century”; the academy awarded Nash the Abel Prize. His other honours included the John von Neumann Theory Prize (1978) and the American Mathematical Society’s Leroy P. Steele Prize for a Seminal Contribution to Research (1999).\nNash’s research into game theory and his long struggle with paranoid schizophrenia became well known to the general public because of the Academy Award-winning motion picture A Beautiful Mind (2001), which was based on Sylvia Nasar’s 1998 biography of the same name. A more factually accurate exploration of Nash’s struggle with mental illness was offered by the public television documentary A Brilliant Madness (2002).\nA Beautiful Mind, American biographical film, released in 2001, that told the story of American Nobel Prize winner John Nash, whose innovative work on game theory in mathematics was in many ways overshadowed by decades of mental illness. Parts of the film, which is set largely on the campus of Princeton University against a backdrop of Cold War intrigue, are seen from Nash’s delusional perspective. The movie, directed by Ron Howard and based loosely on Sylvia Nasar’s 1998 biography of Nash, won four Academy Awards, including that for best picture.\nThe movie begins in 1947 at Princeton, where Nash (played by Russell Crowe) has arrived as a graduate student, together with Martin Hansen (Josh Lucas), Richard Sol (Adam Goldberg), Ainsley (Jason Gray-Stanford), and Bender (Anthony Rapp). Nash is arrogant and dismissive of his classmates but gets along with his roommate Charles (Paul Bettany). Nash generally pursues his studies alone but, when Charles suggests that he take a break and go to a bar, Nash agrees. At the bar, a discussion with his classmates as to the most successful way for them to approach a group of women leads to Nash’s breakthrough paper on game theory.\nNash later receives an appointment to the Massachusetts Institute of Technology, where Sol and Bender become his assistants. A few years later, he is asked to the Pentagon to decrypt coded Russian communications. His success impresses William Parcher (Ed Harris), a high-level agent in the Department of Defense. While teaching at MIT, Nash begins dating a student, Alicia (Jennifer Connelly). Parcher visits Nash to enlist him in a group of workers who scan newspapers and magazines to find hidden Russian codes embedded in the text. Nash is to leave deciphered codes in a secret drop box for Parcher. The clandestine work makes Nash nervous, but he is cheered when he reunites with his former roommate Charles. He marries Alicia soon thereafter. Some time later, Nash gets caught up in a gun battle between Parcher and several Russian agents. Terrified, he asks Parcher to be relieved of his assignment, but Parcher tells him that he would be killed if he were to quit. While giving a lecture at Harvard University, Nash sees Charles in the audience but then spots Russian agents as well, and he flees.\nNash is captured, sedated, and sent to a psychiatric facility under the care of Dr. Rosen (Christopher Plummer). Dr. Rosen tells Alicia that Nash suffers from schizophrenia and that Parcher and Charles exist only in Nash’s mind. Alicia is not convinced until she sees the inside of Nash’s office and also finds the drop box, which is full of unopened missives. Nash receives therapy, and Nash, Alicia, and their son move to Princeton. The medication makes Nash lethargic, however, and eventually he stops taking his pills. After he knocks Alicia to the ground when Parcher urges him to kill her, he and Alicia decide to find a way to live with his illness. After that, although Nash continues to see Parcher and Charles, he no longer interacts with them. Eventually, he is able to return to teaching, and in 1994 he receives the Nobel Prize.\nA Beautiful Mind was criticized by some viewers for glossing over some of the darker elements of Nash’s life story, including the facts that Nash fathered a child with a different woman before marrying Alicia and that he was arrested in 1954 for indecent exposure. The mathematician’s symptoms in fact did not begin until 1959, after he had written his dissertation. Although the film depicted Nash’s hallucinations as largely visual, Nash himself reported that his delusions were mostly auditory and mental. The screenwriter, Akiva Goldsman, conceived of the visual hallucinations as a method for giving the viewer the sensation of experiencing delusions. Though in reality Nash apparently had a remission of his mental illness, in the movie the character says that he is taking a newer medicine. This was a choice made by the director in order not to give the impression that abandoning medication was an appropriate method of dealing with schizophrenia.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/topic/University-of-Pittsburgh",
    "content": "University of Pittsburgh, coeducational state system of higher learning in Pennsylvania, U.S., comprising a main campus in Pittsburgh and branches in Bradford, Greensburg, Johnstown, and Titusville. The Pittsburgh campus is a comprehensive research institution of higher learning and includes 16 schools that offer more than 360 degree programs. Among these schools are those of Medicine, Dental Medicine, Law, Engineering, and Social Work. The university offers a broad range of undergraduate, graduate, and professional degree programs. Research facilities affiliated with the main campus include the Learning Research and Development Center and the Pymatuning Laboratory of Ecology. The Johnstown campus is a four-year college with a liberal arts and sciences and engineering curriculum. The branches in Bradford and Greensburg are both four-year liberal arts colleges. The Titusville branch is a junior college. Total enrollment for the system is approximately 31,400.\nThe university began in 1787, in a three-room log schoolhouse, as Pittsburgh Academy. In 1819 it became the Western University of Pittsburgh. The School of Medicine, originally chartered in 1886 as the Western Pennsylvania Medical College, joined the university in 1892. The school’s name was changed to the University of Pittsburgh in 1908. The central feature of the main campus is its Cathedral of Learning, a 42-story Gothic skyscraper. Notable alumni include dancer Gene Kelly, filmmaker Werner Herzog, and philanthropist Andrew Mellon. Jonas Salk conducted his polio-vaccine research while on the medical school faculty.\nPittsburgh, city, seat (1788) of Allegheny county, southwestern Pennsylvania, U.S. The city is located at the confluence of the Allegheny and Monongahela rivers, which unite at the point of the “Golden Triangle” (the business district) to form the Ohio River. A city of hills, parks, and valleys, it is the centre of an urban industrial complex that includes the surrounding cities of Aliquippa (northwest), New Kensington (northeast), McKeesport (southeast), and Washington (southwest) and the borough of Wilkinsburg (east). Inc. borough, 1794; city, 1816. Area city, 58 square miles (150 square km). Pop. (2010) 305,704; Pittsburgh Metro Area, 2,356,285; (2020) 302,971; Pittsburgh Metro Area, 2,370,930.\nAlgonquian- and Iroquoian-speaking peoples were early inhabitants of the region. The conflict between the British and French over territorial claims in the area was settled in 1758 when General John Forbes and his British and colonial army expelled the French from Fort Duquesne (built 1754). Forbes named the site for the British statesman William Pitt the Elder. The British built Fort Pitt (1761) to ensure their dominance at the source of the Ohio. Settlers began arriving after Native American forces led by Ottawa chief Pontiac were defeated in 1763; an agreement subsequently was made between Native American groups and the Penn family, and a boundary dispute was ended between Pennsylvania and Virginia. Pittsburgh was laid out (1764) by John Campbell in the area around the fort (now the Golden Triangle). Following the American Revolution, the town became an outfitting point for settlers traveling westward down the Ohio River.\nPittsburgh’s strategic location and wealth of natural resources spurred its commercial and industrial growth in the 19th century. A blast furnace, erected by George Anschutz about 1792, was the forerunner of the iron and steel industry that for more than a century was the city’s economic mainstay; by 1850 Pittsburgh was known as the “Iron City.” The Pennsylvania Canal and the Portage Railroad, both completed in 1834, opened vital markets for trade and shipping. The city suffered a great loss in 1845 when some 24 blocks of businesses, homes, churches, and other buildings were destroyed by fire.\nAfter the American Civil War, great numbers of European immigrants swelled Pittsburgh’s population, and industrial magnates such as Andrew Carnegie, Henry Clay Frick, and Thomas Mellon built their steel empires there. The city became the focus of historic friction between labour and management, and the American Federation of Labor was born there in 1881.\nBy 1900 the city’s population had reached 321,616. Growth continued nearly unabated through World War II, the war years bringing a particularly great boon for the economy. The population crested at more than 675,000 in 1950, after which it steadily declined; by the end of the century, it had returned almost to the 1900 level. Most citizens were still of European ancestry, but the growing African American proportion of the population exceeded one-fourth. During the period of economic and population growth, Pittsburgh had come to epitomize the grimy, polluted industrial city. After the war, however, the city undertook an extensive redevelopment program that emphasized smoke-pollution control, flood prevention, and sewage disposal. In 1957 it became the first American city to generate electricity by nuclear power.\nBy the late 1970s and early ’80s, the steel industry had virtually disappeared—a result of foreign competition and decreased demand. Many of the surrounding mill towns were laid to waste by unemployment, becoming a symbol of the notorious Rust Belt, the U.S. region where steelmaking and manufacturing once thrived before succumbing to widespread unemployment and poverty. Pittsburgh, however, successfully diversified its economy through more emphasis on light industries—though metalworking, chemicals, and plastics remained important—and on such high-technology industries as computer software, industrial automation (robotics), and biomedical and environmental technologies. Numerous industrial research laboratories were established in the area, and the service sector became increasingly important. Pittsburgh long has been one of the nation’s largest inland ports, and it remains a leading transportation centre.\nMuch of the Golden Triangle has been rebuilt and includes Point State Park (containing Fort Pitt Blockhouse and Fort Pitt Museum), the Gateway Center (site of several skyscrapers and a garden), and the David L. Lawrence Convention Center. The University of Pittsburgh was chartered in 1787. Other educational institutions include Carnegie Mellon (1900), Duquesne (1878), Point Park (1960), Chatham (1869), and Carlow (1929) universities and two campuses of the Community College of Allegheny County (1966).\nCentral to the city’s cultural life is the Carnegie Museums of Pittsburgh (formerly Carnegie Institute), an umbrella organization consisting of a number of institutions. Its museums include those for the fine arts and natural history (both founded in 1895), the Carnegie Science Center (1991), which now also houses the Henry Buhl, Jr., Planetarium and Observatory (1939), and the Andy Warhol Museum (1994), which exhibits the works of the Pittsburgh-born artist and filmmaker. Other institutions affiliated with the organization are the Carnegie Library of Pittsburgh, which contains more than 3.3 million volumes, and the Carnegie Music Hall. The Pittsburgh Symphony Orchestra performs at Heinz Hall, a restored movie theatre.\nPhipps Conservatory and Botanical Gardens (1893), which covers 15 acres (6 hectares), is noted for its extensive greenhouses. The city’s zoo, in the northeastern Highland Park neighbourhood, includes an aquarium. Two new sports venues opened in 2001 on the north bank of the Allegheny opposite the Golden Triangle: PNC Park is home of the Pirates, the city’s professional baseball team, and Acrisure Stadium houses the Steelers, its professional football team. The Penguins, Pittsburgh’s professional ice hockey team, plays at PPG Paints Arena. Popular summertime attractions include riverboat excursions on Pittsburgh’s waterways and Kennywood, an amusement park southeast of the city in West Mifflin.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/story/25-decade-defining-events-in-us-history",
    "content": "Dividing history into decades is an arbitrary but sometimes very useful way of trying to understand the arcs and significance of events. Trying to identify any single event as crucial to the understanding of a given decade may be even more arbitrary. It is certainly subjective. Nevertheless, that attempt can at the very least be a catalyst for discussion. What follows is an attempt to identify decade-defining moments in the history of the United States since the country’s inception.\nThe centrality of the Declaration of Independence (1776) to the developments of the 1770s is self-evident. From the Boston Tea Party to the “shot heard round the world,” Washington’s crossing of the Delaware River, and the Valley Forge winter, the American Revolution’s pursuit of liberty was made meaningful by the founding document of the great American experiment in democracy.\nWith the war won, independence secured, and the Articles of Confederation proving inadequate, the Founding Fathers laid down the law by which the new country would be governed in the elegantly crafted Constitution, which, depending on one’s perspective, was meant either to evolve to meet changing circumstances or to be strictly interpreted to adhere to the Founders’ “original intent.”\nAs the new country began finding its feet, U.S. President George Washington sent troops to western Pennsylvania in 1794 to quell the Whiskey Rebellion, an uprising by citizens who refused to pay a liquor tax that had been imposed by Secretary of Treasury Alexander Hamilton to raise money for the national debt and to assert the power of the national government. Federalists cheered the triumph of national authority, while members of Thomas Jefferson’s Republican (later Democratic-Republican) Party were appalled by what they saw as government overreach. More than two centuries later, the names and faces have changed, but the story is ongoing.\nThe Louisiana Territory, the huge swath of land (more than 800,000 square miles) that made up the western Mississippi River basin, passed from French colonial rule to Spanish colonial rule and then back to the French before U.S. President Thomas Jefferson pried it away from Napoleon in 1803 for a final price of some $27 million. Out of it were carved—in their entirety—the states of Louisiana, Missouri, Arkansas, Iowa, North Dakota, South Dakota, Nebraska, and Oklahoma along with most of Kansas, Colorado, Wyoming, Montana, and Minnesota. Exploring the land acquired through the Louisiana Purchase gave Lewis and Clark something to do for two years.\nOn January 8, 1815, a ragtag army under the command of Andrew Jackson decisively defeated British forces in the Battle of New Orleans, even though the War of 1812 had actually already ended. News of the Treaty of Ghent (December 24, 1814) had yet to reach the combatants. The American victory made a national figure of future president Jackson and contributed to the widespread perception that the U.S. had won the war, but in truth the conflict was effectively a draw, and the issues that had brought it on were largely unresolved.\nThe Era of Good Feelings (roughly 1815–25), a period of American prosperity and isolationism, was in full swing when U.S. President James Monroe articulated a set of principles in 1823 that decades later would be called the Monroe Doctrine. According to the policy, the United States would not intervene in European affairs, but likewise it would not tolerate further European colonization in the Americas or European interference in the governments of the American hemisphere. It is questionable whether the U.S. at the time had the might to back up its swagger, but later, as a world power, it would implement a broad interpretation of the doctrine in its “sphere of influence.”\nAndrew Jackson, U.S. president from 1829 to 1837, was said to have ushered in the Era of the Common Man. But while suffrage had been broadly expanded beyond men of property, it was not a result of Jackson’s efforts. Despite the careful propagation of his image as a champion of popular democracy and as a man of the people, he was much more likely to align himself with the influential than with the have-nots, with the creditor than with the debtor. Jacksonian democracy talked a good game for people on the street but delivered little.\nSigned on February 2, 1848, the Treaty of Guadalupe Hidalgo brought to a close the Mexican-American War (1846–48) and seemingly fulfilled the Manifest Destiny of the United States championed by President James K. Polk by adding 525,000 square miles of formerly Mexican land to the U.S. territory.\nThe 1850s were awash in harbingers of the American Civil War—from the Compromise of 1850, which temporarily forestalled North-South tensions, to John Brown’s Harpers Ferry Raid, which ramped them up. Arguably, though, by stoking abolitionist indignation in an increasingly polarized country, the U.S. Supreme Court’s Dred Scott decision set the table for the 1860 election of Abraham Lincoln as president, which ultimately precipitated secession and war.\nIn July 1863, the year of the Emancipation Proclamation, in the small Pennsylvania crossroads town of Gettysburg, Robert E. Lee’s invading Army of Northern Virginia sustained a defeat so devastating that it sealed the fate of the Confederacy and its “peculiar institution.” Within two years the war was over, and before the end of the decade the South was temporarily transformed by Reconstruction.\nWhile the country celebrated its anniversary at the Philadelphia Centennial Exposition, on June 25, 1876, the 7th Cavalry under the command of Lieutenant Colonel George Armstrong Custer was vanquished by Lakota and Northern Cheyenne warriors led by Sitting Bull in the Battle of the Little Bighorn. Although it was a major victory for the Northern Plains people against U.S. expansionism, the battle marked the beginning of the end of Native American sovereignty over the West.\nThe wealth-concentrating practices of the “robber barons,” who oversaw the burst of industrial activity and corporate growth during the Gilded Age of the late 19th century, was countered by the rise of organized labor led by the Knights of Labor. However, when a protest meeting related to one of the nearly 1,600 strikes conducted during 1886 was disrupted by the explosion of a bomb that killed seven policemen at the Haymarket Riot in Chicago, many people blamed the violence on organized labor, which went into decline until the turn of the century.\nWith the end of Reconstruction in the 1870s, the enactment of Jim Crow laws enforced racial segregation in the South. In its 7–1 decision in the Plessy v. Ferguson case in May 1896, the U.S. Supreme Court gave constitutional sanction to laws designed to achieve racial segregation by means of separate and supposedly equal public facilities and services for African Americans and whites, thus providing a controlling judicial precedent that would endure until the 1950s.\nIn 1902 U.S. President Theodore Roosevelt pursued the Progressive goal of curbing the enormous economic and political power of the giant corporate trusts by resurrecting the nearly defunct Sherman Antitrust Act to bring a lawsuit that led to the breakup of a huge railroad conglomerate, the Northern Securities Company (ordered by the U.S. Supreme Court in 1904). Roosevelt pursued this policy of “trust-busting” by initiating suits against 43 other major corporations during the next seven years.\nAs World War I raged in Europe, most Americans, including U.S. President Woodrow Wilson, remained determined to avoid involvement and committed to neutrality, though the U.S. economy had benefited greatly from supplying food, raw material, and guns and ammunition to the Allies. More than any other single event, the sinking of the unarmed British ocean liner, the Lusitania, by a German submarine on May 7, 1915 (killing, among others, 128 Americans), prompted the U.S. to join the war on the side of the Allies. Leaving behind its isolationism, the U.S. became a global superpower, though by decade’s end it would recoil from membership in the fledgling League of Nations.\n“The chief business of the American people is business,” U.S. President Calvin Coolidge said in 1925. And with the American economy humming during the “Roaring Twenties” (the Jazz Age), peace and prosperity reigned in the United States…until it didn’t. The era came to a close in October 1929 when the stock market crashed, setting the stage for years of economic deprivation and calamity during the Great Depression.\nIn 1933 at least one-fourth of the U.S. workforce was unemployed when the administration of President Franklin D. Roosevelt first took on the ravages of the Great Depression with the New Deal, a federal government program that sought to bring about immediate economic relief as well as reforms in industry, agriculture, finance, labor, and housing. On March 12, 1933, Roosevelt gave the first in a long series (1933–44) of straightforward informal radio addresses, the fireside chats, which were initially intended to garner support for the New Deal but eventually contributed to reformulating the American social mentality from one of despair to one of hope during a time of multiple crises, including the Great Depression and World War II.\nHaving again stayed out of the initial stages of a worldwide conflict, the U.S. entered World War II on the side of the Allies following the Japanese attack on Pearl Harbor (December 1941). In August 1945, with the war in Europe over and U.S. forces advancing on Japan, U.S. President Harry S. Truman ushered in the nuclear era by choosing to drop atomic bombs on Hiroshima and Nagasaki, Japan, in the hope that the terrible destruction unleashed would prevent the even greater loss of life that seemed likely with a protracted island-by-island invasion of Japan.\nWith the Cold War as a backdrop, U.S. Senator Joseph McCarthy gave his name to an era (McCarthyism) by fanning the flames of anticommunist hysteria with sensational but unproven charges of communist subversion in high government circles, while the House Un-American Activities Committee investigated alleged communist activities in the entertainment industry. McCarthy’s influence waned in 1954 when a nationally televised 36-day hearing on his charges of subversion by U.S. Army officers and civilian officials exposed his brutal interrogative tactics.\nAt the center of the widespread social and political upheaval of the 1960s were the civil rights movement, opposition to the Vietnam War, the emergence of a youth-oriented counterculture, and the establishment and reactionary elements that pushed back against change. The April 4, 1968, assassination of Martin Luther King, Jr., the most prominent civil rights leader, revealed the tragic, violent consequences that could result from a country’s political polarization.\nOn August 9, 1974—facing likely impeachment for his role in covering up the scandal surrounding the break-in at the Democratic National Committee (DNC) headquarters in the Watergate complex in Washington, D.C., in June 1972—Republican Richard Nixon became the only U.S. president to resign. The loss of faith in government officials that resulted from the scandal suffused both popular and political culture with paranoia and disillusionment for the remainder of the decade.\nU.S. President Ronald Reagan’s triumph over the strike by the Professional Air Traffic Controllers Organization (PATCO) in August 1981 played a pivotal role in the long-term weakening of the power of labor unions and helped set the tenor for his administration. Reagan’s ascent to the presidency in 1980 had much to do with his rhetorical ability to break the cloud of gloom caused by Watergate. This abetted his efforts to implement supply-side (monetarist) economic policies predicated on the notion that lower taxes on wealthy “job creators” would create a rising tide that would lift all boats. Critics argued that the wealth created during the decade never “trickled down” to the rank and file.\n(Read Monica Lewinsky's Britannica essay on cyberbullying.)\nAlthough terrorist attacks had been directed at the United States at the end of the 20th century, a new sense of vulnerability was introduced into American life on September 11, 2001, when Islamist terrorists crashed hijacked planes into the World Trade Center in New York City, the Pentagon in Washington, D.C., and the Pennsylvania countryside, resulting in the deaths of nearly 3,000 people.\nSince at least the 1980s, the U.S. had been politically polarized by so-called culture wars that symbolically divided the country into Republican-dominated “red states” (typically characterized as conservative, God-fearing, and opposed to big government, abortion, and same-sex marriage) and Democrat-dominated “blue states” (theoretically liberal, secular, politically correct, and for legal abortion access). The 2016 election of Republican Donald Trump—whose presidential campaign was grounded in nationalism and anti-immigrant rhetoric—could been seen then as a reaction against the seeming triumph of blue-state values during the two-term presidency (2009–17) of the United States’ first African American president, Democrat Barack Obama.\nIn early 2020, life in the United States and around the world was turned upside down by the arrival of the COVID-19 global pandemic. For more than two years, preventive measures such as lockdowns, social distancing, mask wearing, and vaccine passports became the “new normal,” as local and national governments sought to prevent the spread of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), which causes COVID-19, the potentially deadly disease that claimed more than one million American lives by May 2022.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/Geography-Travel",
    "content": "Quite a few islands around the world are extremely large, and many of them are countries. \n7 Wonders of America\nJust How Many Oceans Are There?\n6 of the World's Smallest Kingdoms\nWhere Is \"Old Zealand\"?\nAntarctica is not only the world's southernmost continent. It is also the world's highest, driest, windiest, coldest, and iciest continent. How icy? There's about 7 million cubic miles of it (or 29 million cubic km), representing about 90 percent of the world's ice and 80 percent of its fresh water.\nAll About the World's Southernmost Continent\nBelize\nMumbai\nGreat Wall of China\nAngkor\nColorado River\nMount Fuji",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/place/Aliquippa",
    "content": "Aliquippa, city, Beaver county, western Pennsylvania, U.S. It lies along the Ohio River just northwest of Pittsburgh. Settled about 1750 as a post for trade with Delaware, Iroquois, and Shawnee Indians, it was first known as Logstown and was later renamed for “Queen” Aliquippa, probably a Seneca. After the French and Indian War (1754–63), the Native American peoples lost their title to the land, and Logstown was deserted. White settlers came again in the 1770s, and sawmilling and gristmilling were early industries. During the winter of 1793–94, Gen. Anthony (“Mad Anthony”) Wayne trained his troops at a site across the river from the adjacent borough of Woodlawn before moving into western Ohio to defeat the British-supported Northwest Indian Confederation at the Battle of Fallen Timbers (August 20, 1794).\nAliquippa was incorporated as a borough in 1892. It grew rapidly after 1900 with the establishment of steel mills in the area, and much of the workforce was engaged in steel production until the contraction of the steel industry in the late 1970s and early ’80s. Although some structural steel is still being produced, most of the steelmaking operations have been torn down, leaving the city with restricted employment opportunities and an aging population. Aliquippa consolidated in 1928. Pop. (2000) 11,734; (2010) 9,438.\nOhio River, major river artery of the east-central United States. Formed by the confluence of the Allegheny and Monongahela rivers at Pittsburgh, it flows northwest out of Pennsylvania, then in a general southwesterly direction to join the Mississippi River at Cairo, Illinois (see photograph), after a course of 981 miles (1,579 km). It marks several state boundaries: the Ohio–West Virginia, Ohio–Kentucky, Indiana–Kentucky, and Illinois–Kentucky. The Ohio River contributes more water to the Mississippi than does any other tributary and drains an area of 203,900 square miles (528,100 square km). The river’s valley is narrow, with an average width of less than 0.5 mile (0.8 km) between Pittsburgh and Wheeling (West Virginia), a little more than 1 mile (1.6 km) from Cincinnati (Ohio) to Louisville (Kentucky) and somewhat greater below Louisville.\nThe Ohio is navigable, and, despite seasonal fluctuations that occasionally reach flood proportions, its fairly uniform flow has supported important commerce since settlement first began. Following destructive floods at Johnstown, Pennsylvania, in 1889 and Portsmouth, Ohio, in 1937, the federal government built a series of flood-control dams. While not developed for hydropower in Ohio, the river, kept at a navigable depth of 9 feet (3 metres), carries cargoes of coal, oil, steel, and manufactured articles. It has a total fall of only 429 feet (130 metres), the one major hazard to navigation being the Falls of the Ohio at Louisville, where locks control a descent of about 24 feet (7 metres) within a distance of 2.5 miles (4 km).\nThe Ohio’s tributaries include the Tennessee, Cumberland, Kanawha, Big Sandy, Licking, Kentucky, and Green rivers from the south and the Muskingum, Miami, Wabash, and Scioto rivers from the north. Chief cities along the river, in addition to Pittsburgh, Cairo, Wheeling, and Louisville, are Steubenville, Marietta, Gallipolis, Portsmouth, and Cincinnati in Ohio; Madison, New Albany, Evansville, and Mount Vernon in Indiana; Parkersburg and Huntington in West Virginia; and Ashland, Covington, Owensboro, and Paducah in Kentucky.\nRené-Robert Cavelier, Sieur de La Salle, is said to have been the first European to see the Ohio, in 1669, and he descended it until obstructed by a waterfall (presumably the Falls at Louisville). In the 1750s the river’s strategic importance (especially the fork at Pittsburgh) in the struggle between the French and the English for possession of the interior of the continent became fully recognized. By the treaty of 1763 ending the French and Indian Wars, the English finally gained undisputed control of the territory along its banks. When (by an ordinance of 1787) the area was opened to settlement, most of the settlers entered the region down the headwaters of the Ohio.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/Arts-Culture",
    "content": "Art movements break up tens of thousands of years of art history into time periods or categories that have common techniques, themes, or philosophies.\nThe author of Don Quixote, often credited with inventing the modern novel, had an eventful afterlife.\n12 Novels Considered the Greatest Ever Written\nWe Are the World\n10 Greatest Basketball Players of All Time\nWhy Is Alcohol Measured by Proof?\nImpressionism comprises the work produced between about 1867 and 1886 by a group of artists, headlined by Claude Monet and Pierre Auguste Renoir, who shared a set of related approaches and techniques. The most conspicuous characteristic was an attempt to accurately and objectively record visual reality in terms of transient effects of light and color. \nImpressionism: A Timeline\nVincent van Gogh\nParis\nLeonardo da Vinci",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/topic/Duquesne-University",
    "content": "Duquesne University, private, coeducational institution of higher learning in Pittsburgh, Pennsylvania, U.S. Duquesne is affiliated with the Roman Catholic church. The university consists of the College of Liberal Arts and the schools of Business Administration, Natural and Environmental Sciences, Education, Music, Health Sciences, Nursing, and Pharmacy. Master’s and doctoral degree programs are offered in some areas, and the Law School awards the Doctor of Jurisprudence degree. Campus facilities include the Tamburitzan Cultural Center and the Simon Silverman Phenomenology Center. Total enrollment exceeds 9,000.\nThe university was founded in 1878 by the Rev. Joseph Strub, of the Congregation of the Holy Ghost, and was named Pittsburgh Catholic College of the Holy Ghost. In 1911 the name was changed to Duquesne University of the Holy Ghost. Its law and business schools opened in 1911 and 1913, respectively.\nPittsburgh, city, seat (1788) of Allegheny county, southwestern Pennsylvania, U.S. The city is located at the confluence of the Allegheny and Monongahela rivers, which unite at the point of the “Golden Triangle” (the business district) to form the Ohio River. A city of hills, parks, and valleys, it is the centre of an urban industrial complex that includes the surrounding cities of Aliquippa (northwest), New Kensington (northeast), McKeesport (southeast), and Washington (southwest) and the borough of Wilkinsburg (east). Inc. borough, 1794; city, 1816. Area city, 58 square miles (150 square km). Pop. (2010) 305,704; Pittsburgh Metro Area, 2,356,285; (2020) 302,971; Pittsburgh Metro Area, 2,370,930.\nAlgonquian- and Iroquoian-speaking peoples were early inhabitants of the region. The conflict between the British and French over territorial claims in the area was settled in 1758 when General John Forbes and his British and colonial army expelled the French from Fort Duquesne (built 1754). Forbes named the site for the British statesman William Pitt the Elder. The British built Fort Pitt (1761) to ensure their dominance at the source of the Ohio. Settlers began arriving after Native American forces led by Ottawa chief Pontiac were defeated in 1763; an agreement subsequently was made between Native American groups and the Penn family, and a boundary dispute was ended between Pennsylvania and Virginia. Pittsburgh was laid out (1764) by John Campbell in the area around the fort (now the Golden Triangle). Following the American Revolution, the town became an outfitting point for settlers traveling westward down the Ohio River.\nPittsburgh’s strategic location and wealth of natural resources spurred its commercial and industrial growth in the 19th century. A blast furnace, erected by George Anschutz about 1792, was the forerunner of the iron and steel industry that for more than a century was the city’s economic mainstay; by 1850 Pittsburgh was known as the “Iron City.” The Pennsylvania Canal and the Portage Railroad, both completed in 1834, opened vital markets for trade and shipping. The city suffered a great loss in 1845 when some 24 blocks of businesses, homes, churches, and other buildings were destroyed by fire.\nAfter the American Civil War, great numbers of European immigrants swelled Pittsburgh’s population, and industrial magnates such as Andrew Carnegie, Henry Clay Frick, and Thomas Mellon built their steel empires there. The city became the focus of historic friction between labour and management, and the American Federation of Labor was born there in 1881.\nBy 1900 the city’s population had reached 321,616. Growth continued nearly unabated through World War II, the war years bringing a particularly great boon for the economy. The population crested at more than 675,000 in 1950, after which it steadily declined; by the end of the century, it had returned almost to the 1900 level. Most citizens were still of European ancestry, but the growing African American proportion of the population exceeded one-fourth. During the period of economic and population growth, Pittsburgh had come to epitomize the grimy, polluted industrial city. After the war, however, the city undertook an extensive redevelopment program that emphasized smoke-pollution control, flood prevention, and sewage disposal. In 1957 it became the first American city to generate electricity by nuclear power.\nBy the late 1970s and early ’80s, the steel industry had virtually disappeared—a result of foreign competition and decreased demand. Many of the surrounding mill towns were laid to waste by unemployment, becoming a symbol of the notorious Rust Belt, the U.S. region where steelmaking and manufacturing once thrived before succumbing to widespread unemployment and poverty. Pittsburgh, however, successfully diversified its economy through more emphasis on light industries—though metalworking, chemicals, and plastics remained important—and on such high-technology industries as computer software, industrial automation (robotics), and biomedical and environmental technologies. Numerous industrial research laboratories were established in the area, and the service sector became increasingly important. Pittsburgh long has been one of the nation’s largest inland ports, and it remains a leading transportation centre.\nMuch of the Golden Triangle has been rebuilt and includes Point State Park (containing Fort Pitt Blockhouse and Fort Pitt Museum), the Gateway Center (site of several skyscrapers and a garden), and the David L. Lawrence Convention Center. The University of Pittsburgh was chartered in 1787. Other educational institutions include Carnegie Mellon (1900), Duquesne (1878), Point Park (1960), Chatham (1869), and Carlow (1929) universities and two campuses of the Community College of Allegheny County (1966).\nCentral to the city’s cultural life is the Carnegie Museums of Pittsburgh (formerly Carnegie Institute), an umbrella organization consisting of a number of institutions. Its museums include those for the fine arts and natural history (both founded in 1895), the Carnegie Science Center (1991), which now also houses the Henry Buhl, Jr., Planetarium and Observatory (1939), and the Andy Warhol Museum (1994), which exhibits the works of the Pittsburgh-born artist and filmmaker. Other institutions affiliated with the organization are the Carnegie Library of Pittsburgh, which contains more than 3.3 million volumes, and the Carnegie Music Hall. The Pittsburgh Symphony Orchestra performs at Heinz Hall, a restored movie theatre.\nPhipps Conservatory and Botanical Gardens (1893), which covers 15 acres (6 hectares), is noted for its extensive greenhouses. The city’s zoo, in the northeastern Highland Park neighbourhood, includes an aquarium. Two new sports venues opened in 2001 on the north bank of the Allegheny opposite the Golden Triangle: PNC Park is home of the Pirates, the city’s professional baseball team, and Acrisure Stadium houses the Steelers, its professional football team. The Penguins, Pittsburgh’s professional ice hockey team, plays at PPG Paints Arena. Popular summertime attractions include riverboat excursions on Pittsburgh’s waterways and Kennywood, an amusement park southeast of the city in West Mifflin.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/story/us-death-toll-during-major-events",
    "content": "This infographic shows the death toll caused by major events during the history of the United States, including the COVID-19 pandemic, in the form of a bar chart that compares the magnitude of each number.\nThe attack on Pearl Harbor killed 2,404 Americans.\nHurricane Maria in 2017 killed 2,982 Americans.\nThe September 11 attacks killed 2,977 Americans.\nThe American Revolution killed 4,435 Americans.\nThe war on terrorism started after the September 11 attacks killed 7,024 Americans.\nAn estimated 8,000 Americans were killed in 1900 when a hurricane struck Galveston, Texas.\nThe H1N1 flu pandemic killed 12,469 Americans.\nThe Korean War killed 36,568 Americans.\nThe Vietnam War killed 58,200 Americans.\nAn estimated 100,000 Americans were killed during the 1968 flu pandemic.\nAn estimated 116,000 Americans were killed during the 1957–58 flu pandemic.\nWorld War I killed 116,516 Americans.\nWorld War II killed 405,399 Americans.\nAn estimated 675,000 Americans were killed during the 1918–19 flu pandemic.\nAn estimated 752,000 Americans were killed during the Civil War.\nAfter the glitzy red-carpet arrivals, the feel-good montages, and the host’s opening roast, the Oscars ceremony switches to its raison d’être: revealing the previous year’s highest achievers in cinema. One after another, the presenters list the nominees, open a lavish envelope, and reveal the winner in each category. The champions breathlessly accept their awards and, over and over, effusively thank their mothers, God, and the academy. The academy has so much gratitude bestowed on it throughout the ceremony that many of us watching at home may start to wonder: What is this obscure body? The answer is less enthralling than one might think. The academy—that is, the Academy of Motion Picture Arts and Sciences—is the organization that votes for the Oscar winners. Perhaps what’s more interesting is figuring out who the members of the academy are and how they vote.\nThe academy is an exclusive Hollywood institution that has its own governing body (aptly named the Board of Governors), 17 separate branches, and a thorough rule book on membership eligibility and voting processes. Since 2016, when the board announced that it would diversify its membership, the academy has grown to about 8,000 members. It doesn’t publicize the names of all those members, but each spring it releases a list of the individuals it has invited to join its ranks. Invitees have included Mindy Kaling, Rashida Jones, Kendrick Lamar, Melissa Etheridge, and J.K. Rowling. The academy also posts the names of members of its current Board of Governors, which includes elected representatives of the 17 branches. These branches represent the various fields of cinema: acting, directing, writing, sound editing, and others. In 2019 the academy’s Website showed that the Board of Governors’ president was cinematographer John Bailey, its first vice president was makeup artist Lois Burwell, and its other officers included actors Laura Dern, Whoopi Goldberg, and Alfred Molina.\nThe rest of the academy members are not listed, but we can guess who a few are by looking at some of the requirements to join the institution. To qualify, an individual must work in the film industry. This means that neither individuals who work exclusively in television nor members of the press may join. Oscar nominees are often considered for membership automatically, while other candidates must be sponsored by two active members of the branch they wish to join. Each branch also has its own specific requirements. Directors, for example, must have a minimum of two directing credits, at least one of them within the past 10 years. So we can be pretty sure that such Hollywood treasures as Meryl Streep, Jack Nicholson, Steven Spielberg, and Tom Hanks, who have each been nominated several times and have won Oscars, are members of the academy. New members may choose only one of the branches to join. This means that Sofia Coppola and Alfonso Cuarón, for example, who were each nominated for and won Oscars, could sign on to either the directing branch or the writing branch but not both.\nWhile the membership of the academy is largely obscure, the voting process is perhaps only slightly clearer. It involves two phases: first, nominating the Oscar candidates and, second, voting for the winners. In the first phase, members receive a ballot that lists qualifying movies. To be considered for nomination, a movie must be feature-length and must have been publicly screened for paid admission for at least one week at a commercial theater in Los Angeles county between January 1 and December 31 of the award year. Documentaries and foreign films have their own eligibility requirements. Members may nominate only for awards within their branch and for best picture. Emma Stone may thus suggest nominees for best actress, actor, supporting actress, and supporting actor, but she may not nominate candidates in the best sound editing or best sound mixing categories. Each member of the academy picks up to five candidates for each of their designated categories and lists them by preference.\nTo determine the nominees in each category, the ballots are tallied by certified public accountants from a firm designated by the academy’s president in a somewhat arcane system that might seem like a sacred ritual to an outsider. To ensure that nominees have broad, rather than just popular, support, the academy uses instant runoff voting, sometimes called preferential voting, which involves several rounds and a “magic” number, wherein a candidate must receive a predetermined number of votes to be considered a nominee. A few weeks after the nominees are announced in January, the second phase of voting begins. For the final voting, all active or lifetime academy members are allowed to cast ballots in any category, but they are discouraged from voting in categories where they lack expertise. Accountants once again tally the ballots, using the preferential system to determine the winner for best picture but using the popular vote for all other categories.\nAfter all the voting and tallying, the winners are finally determined, but they are not reported to anyone. Only two accountants see the final results, and they are responsible for keeping those results secret until the awards ceremony. The accountants memorize the names of the winners, stuff two sets of envelopes, and pack and store two briefcases at an undisclosed location until the day of the ceremony. At the ceremony, neither the members of the academy nor the producers of the awards show know who will receive an Oscar. It is a complete mystery until the presenter utters one of the most famous lines in Hollywood: “And the Oscar goes to....”",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/browse/Lifestyles-Social-Issues",
    "content": "",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/question/Should-universities-pay-college-athletes",
    "content": "Whether university athletes should be paid is widely debated. Some argue the NCAA, colleges, and universities profit unfairly and exorbitantly from the work and likenesses of college athletes, who are risking their bodies as well as their future careers and earning potential while often living below the poverty line. Others argue that the scholarships given to student athletes are fair compensation for their services, especially since so few college athletes actually \"go pro,\" and that the real problem is not greater compensation for student-athletes but an incompetent amateur sports system for feeding talent to professional sports leagues. For more on the debate over paying college athletes, visit ProCon.org.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/chatbot",
    "content": "Improve your online learning experience with the Britannica AI Chatbot, powered by our comprehensive database of encyclopedic articles. Ask questions and receive reliable answers across a variety of topics, from history and science to arts and culture.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/editor/The-Editors-of-Encyclopaedia-Britannica/4419",
    "content": "Britannica has been a global leader in information since 1768. We’ve gone from publishing encyclopedia sets to selling CD-ROMs (which may seem even more quaint today than books) to going fully digital. What hasn’t changed is our commitment to being clear, fair, accurate, and relevant.\nThat’s a very fair question, and the fact that you thought to ask it means that you know that not all information is created equal. So consider: \nUnlike the days when the encyclopedia was printed and couldn’t be changed for upwards of a year, today editors at Britannica are continuously updating and revising content. \nIf after reading all this, you’re still not sure about our process, know that we make it transparent to readers and users how, why, and when we revise articles. Just click on Article History in any article to see what has been done—from adding media to fixing the way an article appears on your phone to updating for developments and more.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/place/PNC-Park",
    "content": "…Allegheny opposite the Golden Triangle: PNC Park is home of the Pirates, the city’s professional baseball team, and Acrisure Stadium houses the Steelers, its professional football team. The Penguins, Pittsburgh’s professional ice hockey team, plays at PPG Paints Arena. Popular summertime attractions include riverboat excursions\n…of the Pirates’ new stadium, PNC Park.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/videos",
    "content": "We've been celebrating Women's Day since 1909.\nWatch more",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/procon",
    "content": "",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/about-britannica-ai",
    "content": "Britannica AI answers your questions with information and insights from our wide-ranging collection of articles. For generations, knowledge-seekers have turned to Britannica for answers they can trust. Britannica AI offers a new way to engage with Britannica-created content.\nBritannica AI includes unique features to improve accuracy and relevance:\nWe are committed to using technology responsibly, offering new ways to engage with our content. Britannica AI is built to enhance rather than replace the educational process — to inspire curiosity and the joy of learning. We also understand that even Britannica AI will make mistakes as the technology evolves. It is a complement to – not a substitute for – exploring Britannica’s enormous base of knowledge. As always, please verify all important information using Britannica articles.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/money/Andrew-Carnegie",
    "content": "Andrew Carnegie (born November 25, 1835, Dunfermline, Fife, Scotland—died August 11, 1919, Lenox, Massachusetts, U.S.) was a Scottish-born American industrialist who led the enormous expansion of the American steel industry in the late 19th century. He was also one of the most important philanthropists of his era.\nCarnegie’s father, William Carnegie, a handloom weaver, was a Chartist and marcher for workingman’s causes; his maternal grandfather, Thomas Morrision, also an agitator, had been a friend of William Cobbett. During the young Carnegie’s childhood the arrival of the power loom in Dunfermline and a general economic downturn impoverished his father, inducing the Carnegies to immigrate in 1848 to the United States, where they joined a Scottish colony of relatives and friends in Allegheny, Pennsylvania (now part of Pittsburgh). Young Andrew began work at age 12 as a bobbin boy in a cotton factory. He quickly became enthusiastically Americanized, educating himself by reading and writing and attending night school.\nAt age 14 Carnegie became a messenger in a telegraph office, where he eventually caught the notice of Thomas Scott, a superintendent of the Pennsylvania Railroad Company, who made Carnegie his private secretary and personal telegrapher in 1853. Carnegie’s subsequent rise was rapid, and in 1859 he succeeded Scott as superintendent of the railroad’s Pittsburgh division. While in this post he invested in the Woodruff Sleeping Car Company (the original holder of the Pullman patents) and introduced the first successful sleeping car on American railroads. He had meanwhile begun making shrewd investments in such industrial concerns as the Keystone Bridge Company, the Superior Rail Mill and Blast Furnaces, the Union Iron Mills, and the Pittsburgh Locomotive Works. He also profitably invested in a Pennsylvania oilfield, and he took several trips to Europe, selling railroad securities. By the age of 30 he had an annual income of $50,000.\nDuring his trips to Britain he came to meet steelmakers. Foreseeing the future demand for iron and steel, Carnegie left the Pennsylvania Railroad in 1865 and started managing the Keystone Bridge Company. From about 1872–73, at about age 38, he began concentrating on steel, founding near Pittsburgh the J. Edgar Thomson Steel Works, which would eventually evolve into the Carnegie Steel Company. In the 1870s Carnegie’s new company built the first steel plants in the United States to use the new Bessemer steelmaking process, borrowed from Britain. Other innovations followed, including detailed cost- and production-accounting procedures that enabled the company to achieve greater efficiencies than any other manufacturing industry of the time. Any technological innovation that could reduce the cost of making steel was speedily adopted, and in the 1890s Carnegie’s mills introduced the basic open-hearth furnace into American steelmaking. Carnegie also obtained greater efficiency by purchasing the coke fields and iron-ore deposits that furnished the raw materials for steelmaking, as well as the ships and railroads that transported these supplies to his mills. The vertical integration thus achieved was another milestone in American manufacturing. Carnegie also recruited extremely capable subordinates to work for him, including the administrator Henry Clay Frick, the steelmaster and inventor Captain Bill Jones, and his own brother Thomas M. Carnegie.\nIn 1889 Carnegie’s vast holdings were consolidated into the Carnegie Steel Company, a limited partnership that henceforth dominated the American steel industry. In 1890 the American steel industry’s output surpassed that of Great Britain’s for the first time, largely owing to Carnegie’s successes. The Carnegie Steel Company continued to prosper even during the depression of 1892, which was marked by the bloody Homestead strike. (Although Carnegie professed support for the rights of unions, his goals of economy and efficiency may have made him favour local management at the Homestead plant, which used Pinkerton guards to try to break the Amalgamated Association of Iron, Steel, and Tin Workers.)\nIn 1900 the profits of Carnegie Steel (which became a corporation) were $40,000,000, of which Carnegie’s share was $25,000,000. Carnegie sold his company to J.P. Morgan’s newly formed United States Steel Corporation for $480,000,000 in 1901. He subsequently retired and devoted himself to his philanthropic activities, which were themselves vast.\nCarnegie wrote frequently about political and social matters, and his most famous article, “Wealth,” appearing in the June 1889 issue of the North American Review, outlined what came to be called the Gospel of Wealth. This doctrine held that a man who accumulates great wealth has a duty to use his surplus wealth for “the improvement of mankind” in philanthropic causes. A “man who dies rich dies disgraced.”\nCarnegie’s own distributions of wealth came to total about $350,000,000, of which $62,000,000 went for benefactions in the British Empire and $288,000,000 for benefactions in the United States. His main “trusts,” or charitable foundations, were (1) the Carnegie Trust for the Universities of Scotland (Edinburgh), founded in 1901 and intended for the improvement and expansion of the four Scottish universities and for Scottish student financial aid, (2) the Carnegie Dunfermline Trust, founded in 1903 and intended to aid Dunfermline’s educational institutions, (3) the Carnegie United Kingdom Trust (Dunfermline), founded in 1913 and intended for various charitable purposes, including the building of libraries, theatres, child-welfare centres, and so on, (4) the Carnegie Institute of Pittsburgh, founded in 1896 and intended to improve Pittsburgh’s cultural and educational institutions, (5) the Carnegie Institution of Washington, founded in 1902 and contributing to various areas of scientific research, (6) the Carnegie Endowment for International Peace, founded in 1910 and intended to disseminate (usually through publications) information to promote peace and understanding among nations, (7) the Carnegie Corporation of New York, the largest of all Carnegie foundations, founded in 1911 and intended for “the advancement and diffusion of knowledge and understanding among the people of the United States” and, from 1917, Canada and the British colonies. The Carnegie Corporation of New York has aided colleges and universities and libraries, as well as research and training in law, economics, and medicine.\nChief among Carnegie’s writings are Triumphant Democracy (1886; rev. ed. 1893), The Gospel of Wealth, a collection of essays (1900), The Empire of Business (1902), Problems of To-day (1908), and Autobiography (1920).\nCarnegie married Louise Whitfield in 1887. Until World War I, the Carnegies alternated between Skibo Castle in northern Scotland, their home in New York City, and their summer house “Shadowbrook” in Lenox, Massachusetts.\nFor historical and social background, see Louis M. Hacker, The World of Andrew Carnegie: 1865–1901 (1968); and George S. Bobinski, Carnegie Libraries (1969). Biographies include Joseph Frazier Wall, Andrew Carnegie (1970, reissued 1989), a massive definitive study; and Harold C. Livesay, Andrew Carnegie and the Rise of Big Business (1975, reissued 1988). Carnegie’s many writings are discussed in George Swetnam, Andrew Carnegie (1980).",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/editor/Amy-Tikkanen/6393",
    "content": "Amy Tikkanen is Managing Editor at Encyclopaedia Britannica. She has worked at Britannica for more than two decades, handling a wide range of topics that include Hollywood, politics, books, and anything related to the Titanic.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/topic/Carnegie-Mellon-University/additional-info",
    "content": "",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/technology/blast-furnace",
    "content": "blast furnace, a vertical shaft furnace that produces liquid metals by the reaction of a flow of air introduced under pressure into the bottom of the furnace with a mixture of metallic ore, coke, and flux fed into the top. Blast furnaces are used to produce pig iron from iron ore for subsequent processing into steel, and they are also employed in processing lead, copper, and other metals. Rapid combustion is maintained by the current of air under pressure.\nBlast furnaces produce pig iron from iron ore by the reducing action of carbon (supplied as coke) at a high temperature in the presence of a fluxing agent such as limestone. Ironmaking blast furnaces consist of several zones: a crucible-shaped hearth at the bottom of the furnace; an intermediate zone called a bosh between the hearth and the stack; a vertical shaft (the stack) that extends from the bosh to the top of the furnace; and the furnace top, which contains a mechanism for charging the furnace. The furnace charge, or burden, of iron-bearing materials (e.g., iron ore pellets and sinter), coke, and flux (e.g., limestone) descends through the shaft, where it is preheated and reacts with ascending reducing gases to produce liquid iron and slag that accumulate in the hearth. Air that has been preheated to temperatures from 900 to 1,250 °C (1,650 and 2,300 °F), together with injected fuel such as oil or natural gas, is blown into the furnace through multiple tuyeres (nozzles) located around the circumference of the furnace near the top of the hearth; these nozzles may number from 12 to as many as 40 on large furnaces. The preheated air is, in turn, supplied from a bustle pipe, a large-diameter pipe encircling the furnace. The preheated air reacts vigorously with the preheated coke, resulting in both the formation of the reducing gas (carbon monoxide) that rises through the furnace and a very high temperature of about 1,650 °C (3,000 °F) that produces the liquid iron and slag.\nThe bosh is the hottest part of the furnace because of its close proximity to the reaction between air and coke. Molten iron accumulates in the hearth, which has a taphole to draw off the molten iron and, higher up, a slag hole to remove the mixture of impurities and flux. The hearth and bosh are thick-walled structures lined with carbon-type refractory blocks, while the stack is lined with high-quality fireclay brick to protect the furnace shell. To keep these refractory materials from burning out, plates, staves, or sprays for circulating cool water are built into them.\nThe stack is kept full with alternating layers of coke, ore, and limestone admitted at the top during continuous operation. Coke is ignited at the bottom and burned rapidly with the forced air from the tuyeres. The iron oxides in the ore are chemically reduced to molten iron by carbon and carbon monoxide from the coke. The slag formed consists of the limestone flux, ash from the coke, and substances formed by the reaction of impurities in the ore with the flux; it floats in a molten state on the top of the molten iron. Hot gases rise from the combustion zone, heating fresh material in the stack and then passing out through ducts near the top of the furnace.\nBlast furnaces may have the following ancillary facilities: a stock house where the furnace burden is prepared prior to being elevated to the furnace top by skip cars or a belt conveyor system; a top-charging system consisting of a vertical set of double bells (cones) or rotating chutes to prevent the release of furnace gas during charging; stoves that use the furnace off-gases to preheat the air delivered to the tuyeres; and a cast house, consisting of troughs that distribute liquid iron and slag to appropriate ladles for transfer to steelmaking furnaces and slag-reclamation areas.\nIn Europe, the blast furnace developed gradually over the centuries from small furnaces operated by the Romans, in which charcoal was used for reducing ore to a semisolid mass of iron containing a relatively small amount of carbon and slag. The iron mass was then hammered to remove the slag, yielding wrought iron. Increases in the height of the furnace, coupled with mechanical bellows for introducing greater amounts of air into it, allowed the higher temperatures needed to produce a high-carbon iron known as cast, or pig, iron. This mode of production was used in central Europe by the mid-14th century and was introduced into England about 1500. Charcoal was the only furnace fuel until the 17th century, when the depletion of forests that provided the charcoal in England led to experiments with coke, which is produced from coal. Coke had been widely adopted for use in blast furnaces by the mid-18th century, and the principle of heating air before it entered the furnace was introduced in the early 19th century.\nModern blast furnaces range in size from 20 to 110 metres (70 to 360 feet), have hearth diameters of 6 to 15 metres (20 to 50 feet), and can produce from 1,000 to about 15,000 tons of pig iron daily. Steelmaking accounts for about 8 percent of global greenhouse gas emissions—the most of any industry—and these emissions contribute to global warming. Some blast furnaces have been fitted with carbon capture technology to decrease the amount of carbon dioxide emitted. However, some steelmakers plan to use a hydrogen-based process that would not require blast furnaces and would emit only water as a by-product.\niron processing, use of a smelting process to turn the ore into a form from which products can be fashioned. Included in this article also is a discussion of the mining of iron and of its preparation for smelting.\nIron (Fe) is a relatively dense metal with a silvery white appearance and distinctive magnetic properties. It constitutes 5 percent by weight of the Earth’s crust, and it is the fourth most abundant element after oxygen, silicon, and aluminum. It melts at a temperature of 1,538° C (2,800° F).\nIron is allotropic—that is, it exists in different forms. Its crystal structure is either body-centred cubic (bcc) or face-centred cubic (fcc), depending on the temperature. In both crystallographic modifications, the basic configuration is a cube with iron atoms located at the corners. There is an extra atom in the centre of each cube in the bcc modification and in the centre of each face in the fcc. At room temperature, pure iron has a bcc structure referred to as alpha-ferrite; this persists until the temperature is raised to 912° C (1,674° F), when it transforms into an fcc arrangement known as austenite. With further heating, austenite remains until the temperature reaches 1,394° C (2,541° F), at which point the bcc structure reappears. This form of iron, called delta-ferrite, remains until the melting point is reached.\nThe pure metal is malleable and can be easily shaped by hammering, but apart from specialized electrical applications it is rarely used without adding other elements to improve its properties. Mostly it appears in iron-carbon alloys such as steels, which contain between 0.003 and about 2 percent carbon (the majority lying in the range of 0.01 to 1.2 percent), and cast irons with 2 to 4 percent carbon. At the carbon contents typical of steels, iron carbide (Fe3C), also known as cementite, is formed; this leads to the formation of pearlite, which in a microscope can be seen to consist of alternate laths of alpha-ferrite and cementite. Cementite is harder and stronger than ferrite but is much less malleable, so that vastly differing mechanical properties are obtained by varying the amount of carbon. At the higher carbon contents typical of cast irons, carbon may separate out as either cementite or graphite, depending on the manufacturing conditions. Again, a wide range of properties is obtained. This versatility of iron-carbon alloys leads to their widespread use in engineering and explains why iron is by far the most important of all the industrial metals.\nThere is evidence that meteorites were used as a source of iron before 3000 BC, but extraction of the metal from ores dates from about 2000 BC. Production seems to have started in the copper-producing regions of Anatolia and Persia, where the use of iron compounds as fluxes to assist in melting may have accidentally caused metallic iron to accumulate on the bottoms of copper smelting furnaces. When iron making was properly established, two types of furnace came into use. Bowl furnaces were constructed by digging a small hole in the ground and arranging for air from a bellows to be introduced through a pipe or tuyere. Stone-built shaft furnaces, on the other hand, relied on natural draft, although they too sometimes used tuyeres. In both cases, smelting involved creating a bed of red-hot charcoal to which iron ore mixed with more charcoal was added. Chemical reduction of the ore then occurred, but, since primitive furnaces were incapable of reaching temperatures higher than 1,150° C (2,100° F), the normal product was a solid lump of metal known as a bloom. This may have weighed up to 5 kilograms (11 pounds) and consisted of almost pure iron with some entrapped slag and pieces of charcoal. The manufacture of iron artifacts then required a shaping operation, which involved heating blooms in a fire and hammering the red-hot metal to produce the desired objects. Iron made in this way is known as wrought iron. Sometimes too much charcoal seems to have been used, and iron-carbon alloys, which have lower melting points and can be cast into simple shapes, were made unintentionally. The applications of this cast iron were limited because of its brittleness, and in the early Iron Age only the Chinese seem to have exploited it. Elsewhere, wrought iron was the preferred material.\nAlthough the Romans built furnaces with a pit into which slag could be run off, little change in iron-making methods occurred until medieval times. By the 15th century, many bloomeries used low shaft furnaces with water power to drive the bellows, and the bloom, which might weigh over 100 kilograms, was extracted through the top of the shaft. The final version of this kind of bloomery hearth was the Catalan forge, which survived in Spain until the 19th century. Another design, the high bloomery furnace, had a taller shaft and evolved into the 3-metre- (10-foot-) high Stückofen, which produced blooms so large they had to be removed through a front opening in the furnace.\nThe blast furnace appeared in Europe in the 15th century when it was realized that cast iron could be used to make one-piece guns with good pressure-retaining properties, but whether its introduction was due to Chinese influence or was an independent development is unknown. At first, the differences between a blast furnace and a Stückofen were slight. Both had square cross sections, and the main changes required for blast-furnace operation were an increase in the ratio of charcoal to ore in the charge and a taphole for the removal of liquid iron. The product of the blast furnace became known as pig iron from the method of casting, which involved running the liquid into a main channel connected at right angles to a number of shorter channels. The whole arrangement resembled a sow suckling her litter, and so the lengths of solid iron from the shorter channels were known as pigs.\nDespite the military demand for cast iron, most civil applications required malleable iron, which until then had been made directly in a bloomery. The arrival of blast furnaces, however, opened up an alternative manufacturing route; this involved converting cast iron to wrought iron by a process known as fining. Pieces of cast iron were placed on a finery hearth, on which charcoal was being burned with a plentiful supply of air, so that carbon in the iron was removed by oxidation, leaving semisolid malleable iron behind. From the 15th century on, this two-stage process gradually replaced direct iron making, which nevertheless survived into the 19th century.\nBy the middle of the 16th century, blast furnaces were being operated more or less continuously in southeastern England. Increased iron production led to a scarcity of wood for charcoal and to its subsequent replacement by coal in the form of coke—a discovery that is usually credited to Abraham Darby in 1709. Because the higher strength of coke enabled it to support a bigger charge, much larger furnaces became possible, and weekly outputs of 5 to 10 tons of pig iron were achieved.\nNext, the advent of the steam engine to drive blowing cylinders meant that the blast furnace could be provided with more air. This created the potential problem that pig iron production would far exceed the capacity of the finery process. Accelerating the conversion of pig iron to malleable iron was attempted by a number of inventors, but the most successful was the Englishman Henry Cort, who patented his puddling furnace in 1784. Cort used a coal-fired reverberatory furnace to melt a charge of pig iron to which iron oxide was added to make a slag. Agitating the resultant “puddle” of metal caused carbon to be removed by oxidation (together with silicon, phosphorus, and manganese). As a result, the melting point of the metal rose so that it became semisolid, although the slag remained quite fluid. The metal was then formed into balls and freed from as much slag as possible before being removed from the furnace and squeezed in a hammer. For a short time, puddling furnaces were able to provide enough iron to meet the demands for machinery, but once again blast-furnace capacity raced ahead as a result of the Scotsman James Beaumont Nielsen’s invention in 1828 of the hot-blast stove for preheating blast air and the realization that a round furnace performed better than a square one.\nThe eventual decline in the use of wrought iron was brought about by a series of inventions that allowed furnaces to operate at temperatures high enough to melt iron. It was then possible to produce steel, which is a superior material. First, in 1856, Henry Bessemer patented his converter process for blowing air through molten pig iron, and in 1861 William Siemens took out a patent for his regenerative open-hearth furnace. In 1879 Sidney Gilchrist Thomas and Percy Gilchrist adapted the Bessemer converter for use with phosphoric pig iron; as a result, the basic Bessemer, or Thomas, process was widely adopted on the continent of Europe, where high-phosphorus iron ores were abundant. For about 100 years, the open-hearth and Bessemer-based processes were jointly responsible for most of the steel that was made, before they were replaced by the basic oxygen and electric-arc furnaces.\nApart from the injection of part of the fuel through tuyeres, the blast furnace has employed the same operating principles since the early 19th century. Furnace size has increased markedly, however, and one large modern furnace can supply a steelmaking plant with up to 10,000 tons of liquid iron per day.\nThroughout the 20th century, many new iron-making processes were proposed, but it was not until the 1950s that potential substitutes for the blast furnace emerged. Direct reduction, in which iron ores are reduced at temperatures below the metal’s melting point, had its origin in such experiments as the Wiberg-Soderfors process introduced in Sweden in 1952 and the HyL process introduced in Mexico in 1957. Few of these techniques survived, and those that did were extensively modified. Another alternative iron-making method, smelting reduction, had its forerunners in the electric furnaces used to make liquid iron in Sweden and Norway in the 1920s. The technique grew to include methods based on oxygen steelmaking converters using coal as a source of additional energy, and in the 1980s it became the focus of extensive research and development activity in Europe, Japan, and the United States.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/question/What-is-a-university",
    "content": "A university is an institution of higher education, usually comprising a college of liberal arts and sciences and graduate and professional schools and having the authority to confer degrees in various fields of study. A university differs from a college in that it is usually larger, has a broader curriculum, and offers graduate and professional degrees in addition to undergraduate degrees. Although universities did not arise in the West until the Middle Ages in Europe, they existed in some parts of Asia and Africa in ancient times.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/list/the-10-greatest-basketball-players-of-all-time",
    "content": "It’s time for Britannica—OK, one editor at Britannica—to jump into the perpetually unresolved “greatest ever in [insert sport here]” fray. We’re not technically a sports website, but this editor (hi!) has been obsessively watching sports for nearly three decades and arguing about them with friends for nearly as long. This list is, naturally, incredibly subjective, and shouldn’t be taken too seriously. Unless you agree with me, in which case this was the most meaningful thing I’ve ever written.\nThis list was originally published in April 2014. LeBron James overtook Kareem Abdul-Jabbar as the NBA’s leading scorer in February 2023.\n(Read James Naismith's 1929 Britannica essay on his invention of basketball.)\nWhat? The leading scorer in National Basketball Association (NBA) history is just the 10th-best player ever? Indeedy. While Kareem put up a whopping 38,387 points during his playing days, I can’t look past the fact that he spent a good chunk of his career receiving passes from Oscar Robertson and Magic Johnson, the two greatest point guards of all time. Also, his career totals were inflated by the fact that he played roughly 10,000 years in the NBA. (Or 20. Whatever.) Nevertheless, he was an awesome force who dominated the sport for two decades and perfected the sky hook, one of the most gorgeous shots the game has ever seen. Plus, he was hilarious in Airplane! and fought Bruce Lee in Game of Death, so his cool quotient is easily the highest of anyone on this list.\nRead Kareem Abdul-Jabbar’s Britannica essay on the New York Rens.\nI have a confession to make: even though I’m a lifelong devotee to all Seattle sports team, I had a fan-fling with the San Antonio Spurs teams of the late ‘90s and early 2000s. Yes, they played the kind of slow-paced ball that put most fans to sleep by the third quarter, but there was often a thing of pure beauty hidden among the tire-fire of a 78–71 final score: Tim Duncan’s bank shot. In his prime, Duncan, nicknamed “The Big Fundamental” by no less a nickname authority than Shaquille O’Neal, was one of the most sound players of all time. While his famously vanilla playing style and quiet demeanor kept him from having a cultural impact similar to the other greats, his four championships, 14 All-Star Games, and two NBA MVP awards are indisputable evidence of his fantastic ability.\nAt the opposite end of the “attractive play” spectrum from Duncan is Shaquille O’Neal. Where Timmy would work his way around an opponent in the post with his superb footwork, Shaq would often use his extraordinary bulk (7’1” and 315 pounds) to bully his way to the basket. Once there, he would finish with an emphatic dunk, a foolproof strategy that helped O’Neal lead the NBA in field-goal percentage 10 times in his career. But O’Neal wasn’t just pure physicality—he was surprisingly graceful for such a massive man and he had a deft touch with his close-range jump shots. His free-throw shooting, on the other hand…\nDon’t be fooled by his humble small-college provenance and the “Hick from French Lick” nickname—Larry Bird was one of the fiercest competitors and greatest smack-talkers in NBA history. The uber-confident Bird had arguably the quickest release of anyone to ever play basketball and he’d often let his defender know that the shot was going in soon after it left his hands. He racked up three championship rings and 12 All-Star appearances in his injury-shortened 13-year career. Moreover, his rivalry with Magic Johnson—who, spoiler alert, you’ll see a bit later in this list—during the 1980s launched basketball into an unprecedented level of national popularity that the sport has never fallen from.\nRussell was the ultimate winner in the history of the NBA. He won a league title in all but two of his 13 seasons as a member of the Boston Celtics. Yes, the NBA consisted of just 8 to 14 teams during this period, so capturing championships was a statistically easier feat for a single franchise, but even that fact doesn’t minimize Russell’s historic accomplishments. The Celtics had played for 10 seasons before Russell joined the team, never once reaching a championship series in that time. But in his rookie year, Russell completely changed the franchise’s course and established the Celtics as the winningest team in the NBA. But he didn’t earn his place on this list though some sort of vague, ethereal “winningness.” Russell was one of the fiercest defenders of all time and he redefined the value of blocking shots, in addition to averaging an incredible 22.5 rebounds per game over his career.\nOh jeez, this guy. While I’m too young to have ever seen him play, his statistics are so mind-blowing that I wish I had a time machine for the express purpose of going back and seeing him in action. During the 1961–62 season, “The Big O” averaged a triple-double with 30.8 points, 12.5 rebounds, and 11.4 assists per game. Oh, and the 12-time All-Star also helped initiate true free agency into the NBA through a landmark antitrust suit, an accomplishment just as impressive as his jaw-dropping on-court exploits.\nAdmittedly, Chamberlain played at a time when post players were significantly smaller and basketball wasn’t drawing the types of athletic marvels we see today, but the man was so incredibly dominant that he deserves a spot in the top five regardless of context. The four highest all-time NBA single-season scoring averages all belong to Chamberlain…in his first four professional seasons. The most notable of his scoring feats came on March 2, 1962, when he put up an astounding 100 points in a game, an NBA record that will likely never be broken. In addition to his unprecedented prowess at putting up points, Chamberlain was also the only person to grab more rebounds per game than Bill Russell (22.9), all while averaging more minutes played per game than any player in league history (45.8). The one time in his 14-year career that he was not an All-Star was in 1970, a season in which an injured Chamberlain was limited to just 12 regular-season games and yet he still managed to will his team to the NBA finals upon his return.\nOne of the most ebullient personalities to ever play in the NBA, Johnson’s charm was a major factor in the massive increase in the league’s popularity during the 1980s. But he was so much more than a dazzling smile. Johnson’s otherworldly passing set the stage for the “Showtime” L.A. Lakers teams that captured five championships during his 13 years with the franchise. The 6’9” Johnson (making him the NBA’s tallest point guard) not only posted the best assists-per-game mark in league history (11.2) but had a tremendous all-around game, as well. Famously, he played center in place of the injured Abdul-Jabbar in the title-clinching game six of the 1980 NBA finals as a 20-year-old rookie. Oh, and while this has nothing to do with his ranking on this list, it’s still incredibly awesome and noteworthy that he has successfully fought off HIV for over two decades, helped de-stigmatize AIDS through his high-profile advocacy, and launched a second career as an entrepreneur who opens businesses predominantly in poverty-stricken areas in efforts to spur urban revitalization. So, yeah, Magic Johnson—neat guy.\nI know I risk getting run out of my beloved Chicago on a rail for daring to suggest His Airness isn’t the best player ever but, well, I just don’t think he is. Most famous player ever? Absolutely. Most important player ever? Quite possibly. Most obsessively competitive to the detriment of ever having normal human relations with anybody? Oh my, yes. The man’s desire to be the best is legendary and propelled him to six championships, five MVP awards, All-Star appearances in every full season he played, and the status as possibly the best defender ever. Plus his 30.1 points per game is the NBA’s highest career scoring average. But, well, he played alongside another top-25 talent in Scottie Pippen and was coached by the strategic genius Phil Jackson during his most productive years. He was amazing, but he had a lot of help, at least more than the last guy on this list. And, frankly, it’s sort of fun tweaking all the Chicagoans who are surprisingly defensive about their sporting accomplishments. Related: did you know that the 2013 Seattle Seahawks had the best defense in NFL history?\nYes, the man many fans (ignorantly) consider the most overrated choke artist in the league is actually the best player to ever set foot on a court. LeBron James just does things that shouldn’t be humanly possible. He’s bigger than a good portion of the players in the NFL and yet he still moves as gracefully as the most nimble guards in basketball. Moreover, he not only faced the incredible pressure of being anointed “The Chosen One” by Sports Illustrated as a teenager, but he’s actually exceeded the lofty expectations set for him. As magnificent as earlier players were, they never had to deal with the 24/7 stresses of 21st century media, which James has with aplomb. Through the publication of this list, James has averaged a Robertson-esque 27.5 points, 7.2 rebounds, and 6.9 steals per game and—unlike the Big O—he was doing so against teams stocked with elite athletes and not players who smoked cigarettes at halftime. When people knocked him for not winning championships early in his career, they overlooked that he almost single-handedly took an overmatched Cleveland Cavaliers squad to the 2007 NBA finals at just 22 years old. And, of course, he’s since won two titles (and counting?) as a member of the Miami Heat. Not only does he routinely pull off feats that I’ve never before seen, he has consistently evolved his game to fix the relative weak spots he was previously criticized for. Pretty much all you can ask for from the best ever, no?",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/story/holi-festival-of-colors",
    "content": "Every spring, people across India and around the world celebrate the Hindu festival Holi, throwing colored water and powders on one another in joyous celebration. On this one day—the full-moon day of the Hindu month of Phalguna—societal rankings such as caste, gender, age, and status are eschewed in the spirit of making merry together, and everyone is fair game to be doused with color.\nHoli’s traditions vary throughout the country and have their roots in Indian mythology. In many places the festival is associated with the legend of Hiranyakashipu, a demon king in ancient India. Hiranyakashipu enlisted the help of his sister, Holika, to kill his son, Prahlada, a devoted worshipper of Vishnu. In an attempt to burn Prahlada, Holika sat with him on a pyre while wearing a cloak that protected her from the fire. But the cloak protected Prahlada instead, and Holika burned. Later that night Vishnu succeeded in killing Hiranyakashipu, and the episode was heralded as a triumph of good over evil. In many places in India, a large pyre is lit on the night before Holi to celebrate this occasion.\nIn other places, the story of Krishna and Radha is central. The story goes that Krishna, a Hindu deity who is considered a manifestation of Vishnu, fell in love with the milkmaid Radha, but he was embarrassed that his skin was dark blue and hers fair. In order to rectify this, he playfully colored her face during a game with her and the other milkmaids. This is thought to be an origin of the colored water and powder throwing. The general merrymaking is also seen as characteristic of Krishna, who is known for his pranks and play.\nYou may know your own religion’s traditions, but do you know who celebrates these holidays and festivals?\nAfter the glitzy red-carpet arrivals, the feel-good montages, and the host’s opening roast, the Oscars ceremony switches to its raison d’être: revealing the previous year’s highest achievers in cinema. One after another, the presenters list the nominees, open a lavish envelope, and reveal the winner in each category. The champions breathlessly accept their awards and, over and over, effusively thank their mothers, God, and the academy. The academy has so much gratitude bestowed on it throughout the ceremony that many of us watching at home may start to wonder: What is this obscure body? The answer is less enthralling than one might think. The academy—that is, the Academy of Motion Picture Arts and Sciences—is the organization that votes for the Oscar winners. Perhaps what’s more interesting is figuring out who the members of the academy are and how they vote.\nThe academy is an exclusive Hollywood institution that has its own governing body (aptly named the Board of Governors), 17 separate branches, and a thorough rule book on membership eligibility and voting processes. Since 2016, when the board announced that it would diversify its membership, the academy has grown to about 8,000 members. It doesn’t publicize the names of all those members, but each spring it releases a list of the individuals it has invited to join its ranks. Invitees have included Mindy Kaling, Rashida Jones, Kendrick Lamar, Melissa Etheridge, and J.K. Rowling. The academy also posts the names of members of its current Board of Governors, which includes elected representatives of the 17 branches. These branches represent the various fields of cinema: acting, directing, writing, sound editing, and others. In 2019 the academy’s Website showed that the Board of Governors’ president was cinematographer John Bailey, its first vice president was makeup artist Lois Burwell, and its other officers included actors Laura Dern, Whoopi Goldberg, and Alfred Molina.\nThe rest of the academy members are not listed, but we can guess who a few are by looking at some of the requirements to join the institution. To qualify, an individual must work in the film industry. This means that neither individuals who work exclusively in television nor members of the press may join. Oscar nominees are often considered for membership automatically, while other candidates must be sponsored by two active members of the branch they wish to join. Each branch also has its own specific requirements. Directors, for example, must have a minimum of two directing credits, at least one of them within the past 10 years. So we can be pretty sure that such Hollywood treasures as Meryl Streep, Jack Nicholson, Steven Spielberg, and Tom Hanks, who have each been nominated several times and have won Oscars, are members of the academy. New members may choose only one of the branches to join. This means that Sofia Coppola and Alfonso Cuarón, for example, who were each nominated for and won Oscars, could sign on to either the directing branch or the writing branch but not both.\nWhile the membership of the academy is largely obscure, the voting process is perhaps only slightly clearer. It involves two phases: first, nominating the Oscar candidates and, second, voting for the winners. In the first phase, members receive a ballot that lists qualifying movies. To be considered for nomination, a movie must be feature-length and must have been publicly screened for paid admission for at least one week at a commercial theater in Los Angeles county between January 1 and December 31 of the award year. Documentaries and foreign films have their own eligibility requirements. Members may nominate only for awards within their branch and for best picture. Emma Stone may thus suggest nominees for best actress, actor, supporting actress, and supporting actor, but she may not nominate candidates in the best sound editing or best sound mixing categories. Each member of the academy picks up to five candidates for each of their designated categories and lists them by preference.\nTo determine the nominees in each category, the ballots are tallied by certified public accountants from a firm designated by the academy’s president in a somewhat arcane system that might seem like a sacred ritual to an outsider. To ensure that nominees have broad, rather than just popular, support, the academy uses instant runoff voting, sometimes called preferential voting, which involves several rounds and a “magic” number, wherein a candidate must receive a predetermined number of votes to be considered a nominee. A few weeks after the nominees are announced in January, the second phase of voting begins. For the final voting, all active or lifetime academy members are allowed to cast ballots in any category, but they are discouraged from voting in categories where they lack expertise. Accountants once again tally the ballots, using the preferential system to determine the winner for best picture but using the popular vote for all other categories.\nAfter all the voting and tallying, the winners are finally determined, but they are not reported to anyone. Only two accountants see the final results, and they are responsible for keeping those results secret until the awards ceremony. The accountants memorize the names of the winners, stuff two sets of envelopes, and pack and store two briefcases at an undisclosed location until the day of the ceremony. At the ceremony, neither the members of the academy nor the producers of the awards show know who will receive an Oscar. It is a complete mystery until the presenter utters one of the most famous lines in Hollywood: “And the Oscar goes to....”",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/science/nuclear-energy",
    "content": "nuclear energy, energy that is released in significant amounts in processes that affect atomic nuclei, the dense cores of atoms. It is distinct from the energy of other atomic phenomena such as ordinary chemical reactions, which involve only the orbital electrons of atoms. One method of releasing nuclear energy is by controlled nuclear fission in devices called reactors, which now operate in many parts of the world for the production of electricity. Another method for obtaining nuclear energy, controlled nuclear fusion, holds promise but has not been perfected by 2020. Nuclear energy has been released explosively by both nuclear fusion and nuclear fission. See also nuclear power.\nIn nuclear fission the nucleus of an atom, such as that of uranium or plutonium. breaks up into two lighter nuclei of roughly equal mass. The process may take place spontaneously in some cases or may be induced by the excitation of the nucleus with a variety of particles (e.g., neutrons, protons, deuterons, or alpha particles) or with electromagnetic radiation in the form of gamma rays. In the fission process a large quantity of energy is released, radioactive products are formed, and several neutrons are emitted. These neutrons can induce fission in a nearby nucleus of fissionable material and release more neutrons that can repeat the sequence, causing a chain reaction in which a large number of nuclei undergo fission and an enormous amount of energy is released. If controlled in a nuclear reactor, such a chain reaction can provide power for society’s benefit. If uncontrolled, as in the case of the so-called atomic bomb, it can lead to an explosion of awesome destructive force.\nNuclear fusion is the process by which nuclear reactions between light elements form heavier elements. In cases where the interacting nuclei belong to elements with low atomic numbers (e.g., hydrogen [atomic number 1] or its isotopes deuterium and tritium), substantial amounts of energy are released. The vast energy potential of nuclear fusion was first exploited in thermonuclear weapons, or hydrogen bombs, which were developed in the decade immediately following World War II. The potential peaceful applications of nuclear fusion, especially in view of the essentially limitless supply of fusion fuel on Earth, have encouraged an immense effort to harness this process for the production of power. Although practical fusion reactors have not been built yet, the necessary conditions of plasma temperature and heat insulation have been largely achieved, suggesting that fusion energy for electric-power production is now a serious possibility. Commercial fusion reactors promise an inexhaustible source of electricity for countries worldwide.\nnuclear power, electricity generated by power plants that derive their heat from fission in a nuclear reactor. Except for the reactor, which plays the role of a boiler in a fossil-fuel power plant, a nuclear power plant is similar to a large coal-fired power plant, with pumps, valves, steam generators, turbines, electric generators, condensers, and associated equipment.\nNuclear power provides almost 15 percent of the world’s electricity. The first nuclear power plants, which were small demonstration facilities, were built in the 1960s. These prototypes provided “proof-of-concept” and laid the groundwork for the development of the higher-power reactors that followed.\nThe nuclear power industry went through a period of remarkable growth until about 1990, when the portion of electricity generated by nuclear power reached a high of 17 percent. That percentage remained stable through the 1990s and began to decline slowly around the turn of the 21st century, primarily because of the fact that total electricity generation grew faster than electricity from nuclear power while other sources of energy (particularly coal and natural gas) were able to grow more quickly to meet the rising demand. This trend appears likely to continue well into the 21st century. The Energy Information Administration (EIA), a statistical arm of the U.S. Department of Energy, has projected that world electricity generation between 2005 and 2035 will roughly double (from more than 15,000 terawatt-hours to 35,000 terawatt-hours) and that generation from all energy sources except petroleum will continue to grow.\nIn 2012 more than 400 nuclear reactors were in operation in 30 countries around the world, and more than 60 were under construction. The United States has the largest nuclear power industry, with more than 100 reactors; it is followed by France, which has more than 50. Of the top 15 electricity-producing countries in the world, all but two, Italy and Australia, utilize nuclear power to generate some of their electricity. The overwhelming majority of nuclear reactor generating capacity is concentrated in North America, Europe, and Asia. The early period of the nuclear power industry was dominated by North America (the United States and Canada), but in the 1980s that lead was overtaken by Europe. The EIA projects that Asia will have the largest nuclear capacity by 2035, mainly because of an ambitious building program in China.\nA typical nuclear power plant has a generating capacity of approximately one gigawatt (GW; one billion watts) of electricity. At this capacity, a power plant that operates about 90 percent of the time (the U.S. industry average) will generate about eight terawatt-hours of electricity per year. The predominant types of power reactors are pressurized water reactors (PWRs) and boiling water reactors (BWRs), both of which are categorized as light water reactors (LWRs) because they use ordinary (light) water as a moderator and coolant. LWRs make up more than 80 percent of the world’s nuclear reactors, and more than three-quarters of the LWRs are PWRs.\nCountries may have a number of motives for deploying nuclear power plants, including a lack of indigenous energy resources, a desire for energy independence, and a goal to limit greenhouse gas emissions by using a carbon-free source of electricity. The benefits of applying nuclear power to these needs are substantial, but they are tempered by a number of issues that need to be considered, including the safety of nuclear reactors, their cost, the disposal of radioactive waste, and a potential for the nuclear fuel cycle to be diverted to the development of nuclear weapons. All of these concerns are discussed below.\nThe safety of nuclear reactors has become paramount since the Fukushima accident of 2011. The lessons learned from that disaster included the need to (1) adopt risk-informed regulation, (2) strengthen management systems so that decisions made in the event of a severe accident are based on safety and not cost or political repercussions, (3) periodically assess new information on risks posed by natural hazards such as earthquakes and associated tsunamis, and (4) take steps to mitigate the possible consequences of a station blackout.\nThe four reactors involved in the Fukushima accident were first-generation BWRs designed in the 1960s. Newer Generation III designs, on the other hand, incorporate improved safety systems and rely more on so-called passive safety designs (i.e., directing cooling water by gravity rather than moving it by pumps) in order to keep the plants safe in the event of a severe accident or station blackout. For instance, in the Westinghouse AP1000 design, residual heat would be removed from the reactor by water circulating under the influence of gravity from reservoirs located inside the reactor’s containment structure. Active and passive safety systems are incorporated into the European Pressurized Water Reactor (EPR) as well.\nTraditionally, enhanced safety systems have resulted in higher construction costs, but passive safety designs, by requiring the installation of far fewer pumps, valves, and associated piping, may actually yield a cost saving.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/list/weapons-of-world-war-i",
    "content": "Depth charges were first developed by the Royal Navy during World War I to combat German submarines.\nWorld War I was a crucible for military aircraft development. Between 1914 and 1918, planes advanced from barely airworthy craft to effective weapons platforms.\nInfantry weapons underwent a massive change in the late 19th century, as repeating rifles entered widespread use. The World War I infantryman could produce a volume of fire that dwarfed that of his mid-19th-century predecessors.\nGerman airships achieved moderate success in long-range bombing operations, as Zeppelins could attain higher altitudes than the airplanes of the era.\nChemical weapons, such as diphosgene and mustard gas, were employed extensively on the Western Front.\nArtillery literally shaped the battlefield in World War I. It ranged in size from the French 75-mm field gun to the massive 420-mm Big Bertha and the 210-mm Paris Gun.\nDespite the advances in technology, cavalry retained a significant role in World War I, and horses died by the millions in the conflict.\nThe age of the battleship reached its apotheosis in World War I, as even the Dreadnought, the archetypal “big-gun” ship, found itself outgunned. Super dreadnoughts, such as the HMS Orion, ruled the waves; their reign was short, however, as developments in naval aviation would soon render such ships obsolete.\nMachine guns were an exceptionally lethal addition to the battlefield in World War I. Heavy guns, such as the Maxim and Hotchkiss, made “no man's land” a killing zone, and Isaac Newton Lewis's light machine gun saw widespread use at the squad level and as an aircraft armament.\nTanks were used primarily in a supporting role. The armoured vehicle would not truly come into its own until the doctrines of J.F.C. Fuller and Basil Liddell Hart were more widely adopted in World War II.\nThe French government estimates that millions of unexploded shells from World War I remain buried or undiscovered in the French countryside. Every year, bomb-clearing units remove more than 40 tons of unexploded munitions from the Verdun area alone.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/topic/university",
    "content": "university, institution of higher education, usually comprising a college of liberal arts and sciences and graduate and professional schools and having the authority to confer degrees in various fields of study. A university differs from a college in that it is usually larger, has a broader curriculum, and offers graduate and professional degrees (master’s and doctorates), professional degrees, in addition to undergraduate degrees (such as the bachelor’s degree). Although universities did not arise in the West until the Middle Ages in Europe, they existed in some parts of Asia and Africa in ancient times.\nThe modern Western university evolved from the medieval schools known as studia generalia; they were generally recognized places of study open to students from all parts of Europe. The earliest studia arose out of efforts to educate clerks and monks beyond the level of the cathedral and monastic schools. The inclusion of scholars from foreign countries constituted the primary difference between the studia and the schools from which they grew.\nThe earliest Western institution that can be called a university was a famous medical school that arose at Salerno, Italy, in the 9th century and drew students from all over Europe. It remained merely a medical school, however. The first true university in the West was founded at Bologna late in the 11th century. It became a widely respected school of canon and civil law. The first university to arise in northern Europe was the University of Paris, founded between 1150 and 1170. It became noted for its teaching of theology, and it served as a model for other universities in northern Europe such as the University of Oxford in England, which was well established by the end of the 12th century. The Universities of Paris and Oxford were composed of colleges, which were actually endowed residence halls for scholars.\nThese early universities were corporations of students and masters, and they eventually received their charters from popes, emperors, and kings. The University of Naples, founded by Emperor Frederick II (1224), was the first to be established under imperial authority, while the University of Toulouse, founded by Pope Gregory IX (1229), was the first to be established by papal decree. These universities were free to govern themselves, provided they taught neither atheism nor heresy. Students and masters together elected their own rectors (presidents). As the price of independence, however, universities had to finance themselves. So teachers charged fees, and, to assure themselves of a livelihood, they had to please their students. These early universities had no permanent buildings and little corporate property, and they were subject to the loss of dissatisfied students and masters who could migrate to another city and establish a place of study there. The history of the University of Cambridge began in 1209 when a number of disaffected students moved there from Oxford, and 20 years later Oxford profited by a migration of students from the University of Paris.\nFrom the 13th century on, universities were established in many of the principal cities of Europe. Universities were founded at Montpellier (beginning of the 13th century) and Aix-en-Provence (1409) in France, at Padua (1222), Rome (1303), and Florence (1321) in Italy, at Salamanca (1218) in Spain, at Prague (1348) and Vienna (1365) in central Europe, at Heidelberg (1386), Leipzig (1409), Freiburg (1457), and Tübingen (1477) in what is now Germany, at Louvain (1425) in present-day Belgium, and at Saint Andrews (1411) and Glasgow (1451) in Scotland.\nUntil the end of the 18th century, most Western universities offered a core curriculum based on the seven liberal arts: grammar, logic, rhetoric, geometry, arithmetic, astronomy, and music. Students then proceeded to study under one of the professional faculties of medicine, law, and theology. Final examinations were grueling, and many students failed.\nThe Protestant Reformation of the 16th century and the ensuing Counter-Reformation affected the universities of Europe in different ways. In the German states, new Protestant universities were founded and older schools were taken over by Protestants, while many Roman Catholic universities became staunch defenders of the traditional learning associated with the Catholic church. By the 17th century, both Protestant and Catholic universities had become overly devoted to defending correct religious doctrines and hence remained resistant to the new interest in science that had begun to sweep through Europe. The new learning was discouraged, and thus many universities underwent a period of relative decline. New schools continued to be founded during this time, however, including ones at Edinburgh (1583), Leiden (1575), and Strasbourg (university status, 1621).\nThe first modern university in Europe was that of Halle, founded by Lutherans in 1694. This school was one of the first to renounce religious orthodoxy of any kind in favour of rational and objective intellectual inquiry, and it was the first where teachers lectured in German (i.e., a vernacular language) rather than in Latin. Halle’s innovations were adopted by the University of Göttingen (founded 1737) a generation later and subsequently by most German and many American universities.\nIn the later 18th and 19th centuries religion was gradually displaced as the dominant force as European universities became institutions of modern learning and research and were secularized in their curriculum and administration. These trends were typified by the University of Berlin (1809), in which laboratory experimentation replaced conjecture; theological, philosophical, and other traditional doctrines were examined with a new rigour and objectivity; and modern standards of academic freedom were pioneered. The German model of the university as a complex of graduate schools performing advanced research and experimentation proved to have a worldwide influence.\nThe first universities in the Western Hemisphere were established by the Spaniards: the University of Santo Domingo (1538) in what is now the Dominican Republic and the University of Michoacán (1539) in Mexico. The earliest American institutions of higher learning were the four-year colleges of Harvard (1636), William and Mary (1693), Yale (1701), Princeton (1746), and King’s College (1754; now Columbia). Most early American colleges were established by religious denominations, and most eventually evolved into full-fledged universities. One of the oldest universities in Canada is that at Toronto, chartered as King’s College in 1827.\nAs the frontier of the United States moved westward, hundreds of new colleges were founded. American colleges and universities tended to imitate German models, seeking to combine the Prussian ideal of academic freedom with the native tradition of educational opportunity for the many. The growth of such schools in the United States was greatly spurred by the Morrill Act of 1862, which granted each state tracts of land with which to finance new agricultural and mechanical schools. Many “land-grant colleges” arose from this act, and there developed among these the Massachusetts Institute of Technology (MIT), Cornell University, and the state universities of Illinois, Wisconsin, and Minnesota.\nSeveral European countries in the 19th century reorganized and secularized their universities, notably Italy (1870), Spain (1876), and France (1896). Universities in these and other European countries became mostly state-financed. Women began to be admitted to universities in the second half of the 19th century. Meanwhile, universities’ curricula also continued to evolve. The study of modern languages and literatures was added to, and in many cases supplanted, the traditional study of Latin, Greek, and theology. Such sciences as physics, chemistry, biology, and engineering achieved a recognized place in curricula, and by the early 20th century the newer disciplines of economics, political science, psychology, and sociology were also taught.\nIn the late 19th and 20th centuries Great Britain and France established universities in many of their colonies in South and Southeast Asia and Africa. Most of the independent countries that emerged from these colonies in the mid-20th century expanded their university systems along the lines of their European or American models, often with major technical and economic assistance from former colonial rulers, industrialized countries, and international agencies such as the World Bank. Universities in Japan, China, and Russia also evolved in response to pressures for modernization. In India some preindependence universities, such as Banaras Hindu University (1916) and Rabindranath Tagore’s Visva-Bharati (1921), were founded as alternatives to Western educational principles. The state universities of Moscow (1755) and St. Petersburg (1819) retained their preeminence in Russia. Tokyo (1877) and Kyōto (1897) universities were the most prestigious ones in Japan, as was Peking University (1898) in China.\nModern universities may be financed by national, state, or provincial governments, or they may depend largely on tuition fees paid by their students. The typical modern residential university may enroll 30,000 or more students and educate both undergraduates and graduate students in the entire range of the arts and humanities, mathematics, the social sciences, the physical, biological, and earth sciences, and various fields of technology. Nonresidential, virtual, and open universities, some of which are modeled after Britain’s Open University (1969), may enroll 200,000 or more students, who pursue both degree-credit and noncredit courses of study. Universities are the main providers of graduate-level training in most professional fields.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/money/unemployment",
    "content": "unemployment, the condition of one who is capable of working, actively seeking work, but unable to find any work. It is important to note that to be considered unemployed a person must be an active member of the labour force and in search of remunerative work.\nUnderemployment is the term used to designate the situation of those who are able to find employment only for shorter than normal periods—part-time workers, seasonal workers, or day or casual workers. The term may also describe the condition of workers whose education or training make them overqualified for their jobs.\nStatistics on unemployment are collected and analyzed by government labour offices in most countries and have come to be considered a chief indicator of economic health. Trends in unemployment and statistical differences among groups in the population are studied for what they may reveal of general economic trends and as bases for possible governmental action. Full employment has been a stated goal of many governments since World War II, and a variety of programs have been devised to attain it. It should be pointed out that full employment is not necessarily synonymous with a zero unemployment rate, for at any given time the unemployment rate will include some number of persons who are between jobs and not unemployed in any long-term sense.\nOn the first Friday of every month, Wall Street zeroes in on the big “jobs report”—officially called the Employment Situation Summary. The main headlines? Unemployment rate, growth in nonfarm payrolls, and average hourly earnings. However, there’s a key statistic that often gets overlooked: the labor force participation rate (LFPR). (Some economists call it “civilian labor participation” because it typically excludes active duty members of the military.)\nUnemployment data includes, not the entire population, but rather people who are considered part of the labor force. This includes those who are actively employed, those unemployed but actively seeking work, and those who are “underemployed”—working, but perhaps not as much as they would like. In general, the labor force does not include children, college students, retirees, or those “detached” from the labor force, such as prisoners, people in institutions, and anyone who is unemployed but has given up looking for a job (economists call them “discouraged” workers).\nSo, although the LFPR often flies under the radar, it doesn’t just complete the big picture. It can actually shift the meaning of the other headline numbers and, by extension, the entire report.\nThe labor force participation rate measures the percentage of the civilian working-age population who are employed or actively looking for work (see the detailed definition above). It’s published on the first Friday of each month by the U.S. Bureau of Labor Statistics (BLS).\nHere’s how the labor force participation rate is calculated:\n(Labor force / total working-age population) x 100\nFor example, if 3,000 people were working in a small town of 5,000 civilian working-age adults, the calculation would look like this: (3,000 / 5,000) x 100 = 60%.\nIf you’re checking the unemployment rate as a key economic bellwether, it helps to know how many people are actually working or looking for work. Here’s why.\nImagine two hypothetical scenarios in which the unemployment rate is 4% (which has historically been considered at or near full employment).\nTo get a clearer picture of an economy’s employment situation and productivity potential, it helps to factor in the labor force participation rate alongside unemployment data, jobs growth, and wage trends. For a society to function, it needs to produce enough to cover the basic needs of all its members—including those not in the labor force.\nFigure 1 shows the labor force participation rate versus the unemployment rate from the end of World War II to 2024. The shaded areas represent recessionary periods.\nBetween 1950 and 1965, around 59% of working-age Americans were active in the workforce. Then workforce participation steadily surged, peaking in 2000 when over 67% of the civilian working age population was active. Over the next two decades, the rate declined. After the pandemic in 2020, the labor force participation rate settled at around 62% by 2024.\nLFPR surge: 1965–2000\nSeveral things contributed to the rise in labor force participation:\nLFPR decline: After 2000\nAlthough many factors contributed to the decline of the LFPR after 2000, one of the biggest was the aging of the baby boomer generation. Their exit from the workforce caused a gradual yet significant decline.\nTechnology, automation, and globalization also played a role. Automation displaced many manufacturing workers and replaced them with machines and software. Workers who couldn’t adapt to the new tech-driven economy struggled to find employment. Globalization added to these pressures, as companies began moving both skilled and unskilled jobs overseas to cut costs.\nSome economists argue that the so-called “wealth effect,” brought about by a long period of growth in the stock and real estate markets, also contributed to the shrinking labor force by providing millions of early retirees sufficient assets to cover a longer retirement.\nThe labor force participation rate can help aid policymakers in making forward-looking decisions:\nAmong fundamental indicators, the LFPR isn’t a market mover (in the short term, anyway). But without it, a comprehensive picture of the labor market may be incomplete. The LFPR is a key piece of the labor puzzle—and key indicator of long-term economic trends.\nConsider using the labor force participation rate as your big-picture framework when analyzing jobs data—especially the unemployment rate. And if you want to analyze longer-term labor trends, look beyond the headline numbers—dig deep into the demographic and structural changes that may be driving the rate up or down.\nThe labor force participation rate may not be the star of the jobs reports, but it’s critical to getting a fuller view of the labor market, particularly longer-term trends. Participation trends (whether rising, falling, or variable) can either exacerbate or mute the effects of unemployment trends.\nFor example, the unemployment spike during the 2007–09 Great Recession was particularly hard-felt because it came about just as the first baby boomers were entering their retirement years and moving out of the workforce—either by choice or by necessity. And although the official unemployment rate topped 14.8% during the COVID-19 economic shutdowns, the precipitous fall in the LFPR—perhaps due to a rise in labor force detachment—made the employment picture even more pronounced than the numbers would indicate.\nIf you actively “participate” in fundamental analysis, when the jobs report comes out each month, keep an eye on the LFPR.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/topic/AFL-CIO",
    "content": "AFL–CIO, American federation of autonomous labour unions formed in 1955 by the merger of the AFL (founded 1886), which originally organized workers in craft unions, and the CIO (founded 1935), which organized workers by industries.\nFounded in 1881, the Federation of Organized Trades was the precursor of the American Federation of Labor (AFL, or AF of L), which, late in the 19th century, replaced the Knights of Labor (KOL) as the most powerful industrial union of the era. In seeking to absorb the existing craft unions, the KOL had reduced their autonomy and involved them in social and political disputes that did not represent the unions’ own direct interests. Consequently, the craft unions revolted. In 1886, under the leadership of Samuel Gompers, they organized themselves as the AFL, a loose federation that remained for half a century the sole unifying agency of the American labour movement.\nIn its beginnings, the American Federation of Labor was dedicated to the principles of craft unionism. Its approximately 100 national and international unions retained full autonomy over their own affairs. In return, each union received “exclusive jurisdiction” over a craft. Although this provoked some bitter jurisdictional disputes between unions affiliated with the federation, union membership still grew. The AFL, unlike the KOL, did not focus on national political issues. Instead, it concentrated on gaining the right to bargain collectively for wages, benefits, hours, and working conditions.\nThe 1920s marked the first period of economic prosperity that lacked a parallel expansion of unionism. During the Great Depression and into the early 1930s, growth in union enrollments slowed. The administration of Pres. Franklin D. Roosevelt, however, brought new opportunities for labour. The new political climate, marked by the passage of the 1935 Wagner Act, prevented employers from interfering with union activities and created the National Labor Relations Board to foster union organization and collective bargaining. As a result, the U.S. labour movement entered a new era of unprecedented growth.\nAn enduring question—whether union organization should be based on craft (skill) or industry (workplace)—became a divisive issue at the American Federation of Labor’s 1935 convention. An industry-based resolution, which stated that “in the great mass production industries … industrial organization is the only solution,” was defeated, which prompted defection. In November 1935, representatives of eight unions announced the formation of the Committee for Industrial Organization (CIO). Two more unions joined later. The AFL retaliated by suspending all 10 unions, but the CIO built momentum by organizing the key steel, rubber, and automobile industries, reaching agreements with such large corporations as U.S. Steel and General Motors. In the following year the CIO and the AFL battled for leadership of American labour, often trying to organize the same workers.\nThe CIO held its first convention in Pittsburgh, Pennsylvania, in November 1938, adopting a new name (Congress of Industrial Organizations) and a constitution as well as electing John L. Lewis as its president. Lewis had organized the first successful strike against General Motors (a “sit-down” tactic) in 1937. This action spurred several other organizing efforts and drew new members.\nLewis pledged to resign as CIO president if Roosevelt, whom he had previously supported, was reelected in 1940. He kept his promise and was succeeded that year by Philip Murray, who had served under Lewis in the United Mine Workers of America (UMWA) union. In the following year the CIO organized the employees of the Ford Motor Company, steel companies (including Bethlehem, Republic, Inland, and Youngstown), and other big industrial corporations that previously had refused to sign agreements with it.\nThe passage of the Taft-Hartley Act in 1947 and the growing conservatism in U.S. national labour policies implicit in the statute aroused unions to renewed political activity. The CIO joined the AFL in opposition to the new law, but political unity was only gradually translated into union solidarity. After Murray’s death late in 1952, Walter P. Reuther, head of the CIO’s United Automobile Workers, became president of the CIO. Three years later, in 1955, the AFL and the CIO merged, with George Meany, former head of the AFL, becoming president of the new federation (a post he held until November 1979, a few months before his death). Membership in the new labour entity included about one-third of all nonagricultural workers in 1955. Membership declined steadily thereafter.\nIn 1957 the union federation expressed ethical concerns when it expelled the Teamsters Union after disclosures of corruption and labour racketeering in what was then the nation’s largest union. (Not until 1987 was the Teamsters Union readmitted to the AFL-CIO.)\nThe conservative Meany and the liberal Reuther never achieved more than an icy cordiality, and in 1968 Meany succeeded in getting Reuther and several other CIO leaders expelled from the federation’s executive board. Thereupon, Reuther’s United Automobile Workers (UAW) promptly withdrew from the AFL-CIO, allying with the Teamsters from 1968 to 1972. Reuther died in 1970, and, two years after Meany’s retirement and Lane Kirkland’s accession to the presidency of the AFL-CIO in 1979, the UAW reaffiliated with the AFL-CIO. During Kirkland’s presidency (1979–95) the percentage of workers represented by organized labour declined from 19 to 15 percent.\nWhen Kirkland retired on August 1, 1995, he named his secretary-treasurer, Thomas R. Donahue, to fill the remainder of his term. At the organization’s 1995 convention, Donahue was defeated for the presidency by John J. Sweeney in what marked the first competitive election in AFL-CIO history. Sweeney, former president of the Service Employees International Union (SEIU), led a dissident slate committed to reversing the federation’s declining membership and waning political power. Also in 1995, the first person of colour was elected to an AFL-CIO executive office when Linda Chavez-Thompson became executive vice president. Sweeney pledged to increase union membership through aggressive organizing campaigns and political lobbying.\nHowever, because of an increasing decline in union membership, five international labour unions—the Laborers’ International Union of North America (LIUNA), the SEIU, and the United Brotherhood of Carpenters, as well as the Union of Needletrades, Industrial and Textile Employees (UNITE) and the Hotel Employees and Restaurant Employees Union (HERE), which later merged to form UNITE HERE—joined together in 2003 to form the New Unity Partnership (NUP), an informal coalition that advocated reform of the AFL-CIO, emphasizing organizing efforts to promote union growth. Following the dissolution of the NUP in 2005, its former member unions—which by then also included the United Food and Commercial Workers (UFCW) and the Teamsters—disaffiliated from the AFL-CIO and launched Change to Win, a formal coalition that afforded an alternative to the AFL-CIO.\nIn 2009 Sweeney stepped down as AFL-CIO president. He was succeeded by Richard Trumka, who had previously served as the president of the UMWA and as the AFL-CIO’s secretary-treasurer. Trumka held the post until his death in 2021.\nLocal union delegates, allocated in proportion to their membership, elect the president to a four-year term. The executive council, which meets at least twice a year, consists of the president, executive vice president, secretary-treasurer, and about 50 vice presidents—most of them presidents of national unions affiliated with the AFL-CIO. An executive committee of six vice presidents selected by the council meets more often with the president and secretary-treasurer to discuss policy matters. Moreover, a general board, which includes the executive council and a principal officer of each affiliated union, meets at least once a year to address policy matters.\nThe federation is supported by a per capita tax levied on affiliated unions and organizing committees. The federation engages in organizing efforts, educational campaigns on behalf of the labour movement, and political support of legislation deemed beneficial to labour.\ncollective bargaining, the ongoing process of negotiation between representatives of workers and employers to establish the conditions of employment. The collectively determined agreement may cover not only wages but hiring practices, layoffs, promotions, job functions, working conditions and hours, worker discipline and termination, and benefit programs.\nCollective bargaining existed before the end of the 18th century in Britain; its development occurred later on the European continent and in the United States, where Samuel Gompers developed its common use during his leadership of the American Federation of Labor. Collective agreements are probably least significant in developing countries that have large labour populations from which to draw.\nThe degree of centralization in the bargaining process and the functions performed by collective agreements vary. Contract negotiation may occur at the national, regional, or local level, depending on the structure of industry within a country. National agreements, which are more common in smaller countries, usually settle general matters, leaving more detailed issues for local consideration. An agreement may, for example, set actual wage rates, or it might simply establish minimum wage rates.\nCollective agreements are not legally binding in all countries. In Britain their application depends on the goodwill of the signatories. In some countries—including Germany, France, and Australia—the government may require that the terms of negotiated settlements be extended to all firms in an industry. In the United States similar results have been achieved, albeit less formally, by unions that select a target employer in a particular industry: the negotiation of a new agreement with the targeted employer then sets the pattern for other labour contracts in the same industry.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/",
    "content": "This weekend marks Daylight Saving Time (DST) in many countries, when clocks will jump ahead one hour. Here’s a quick look at why it began and how it has changed.\nThe practice was first suggested in an essay by Benjamin Franklin in 1784. In 1907 an Englishman, William Willett, campaigned for setting the clock ahead by 80 minutes in four moves of 20 minutes each during April and the reverse in September.\nSeveral countries—including Australia, Great Britain, Germany, and the United States—adopted summer DST during World War I to conserve fuel by reducing the need for artificial light. During World War II clocks were kept continuously advanced by an hour in some countries.\nIn the U.S., DST once began on the last Sunday in April and ended in October. In 1986 the start of DST was moved to the first Sunday in April. In 2007 DST changed again, as the start date was moved to the second Sunday in March and the end date to the first Sunday in November. In most countries in western Europe, DST starts on the last Sunday in March and ends on the last Sunday in October.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/list/14-questions-about-government-in-the-united-states-answered",
    "content": "The people of the United States live under a variety of governments—the federal government, state governments, and local governments—all with their own powers and responsibilities. This list answers 14 questions about how these governments work.\nEarlier versions of these questions and answers first appeared in the second edition of The Handy Answer Book for Kids (and Parents) by Gina Misiroglu (2010).\nThe United States government carries out a census every ten years, as is set forth in Article 1 of the U.S. Constitution. The census survey asks questions about people in the United States. The survey gathers information about how many people are in a household, how old they are, and how they self-identify their gender, race, and ethnicity. In 1790 the first census counted 4 million people living in the United States. According to the 2020 census, there are more than 331 million.\nThe census count tells a state how many people it can send to represent it in the U.S. Congress, which includes both the Senate and the House of Representatives. Every state sends two people to the Senate. But the House of Representatives is different—the number of people a state sends to the House is based on that state’s population. According to the 2020 census, the state with the smallest population is Wyoming, while the state with the largest population is California. Wyoming can send only one person to the House of Representatives, but California can send 52 people to the House.\nThe census also provides information about everyday things, so that the government can tell which states and regions of the country need improvements. If a town has more children living in it than it had ten years ago (when the last census count was taken), this might be a good place to build another school. Or, if workers in a city are spending too much time getting to work, this might be a good place to build more roads or increase public transportation, including subways and buses.\nThe census, in other words, provides the data on which government—whose focus is the people of the United States—and its potential activity are built.\nGovernment in the United States is the institution that creates and enforces public policies. Public policies are all the things that the government, at all levels, decides to do, such as impose an income tax, service its armed forces, protect and manage the environment, and hold businesses to certain standards. In the democratic United States, voters elect people to government to represent them. Those who exercise the powers of the government include legislators, who make laws; executives and administrators, who administer and enforce those laws; and judges, who interpret the law. Each year, Congress enacts about 500 laws, and state legislators enact about 25,000 laws. Local governments enact thousands and thousands of ordinances, or city laws.\nThe U.S. federal government is the national government of the United States of America. It includes the executive, legislative, and judicial branches. The executive branch is responsible for enforcing the laws of the United States. Its main components include the president, the vice president, and government departments and agencies. The president is the leader of the country and commander in chief of the armed forces; the vice president is the president of the Senate and the first in line for the presidency should the president be unable to serve; the departments and their heads (called Cabinet members) advise the president on decisions that affect the country; and agencies help carry out the president’s policies and provide special services. The legislative branch is the lawmaking branch of the federal government. It is made up of a bicameral (or two-chamber) Congress: the Senate and the House of Representatives. The judicial branch, made up of the Supreme Court and other federal courts, is responsible for interpreting the meaning of laws, how they are applied, and whether or not they violate the Constitution.\nThe chief duty of the president of the United States is to protect the Constitution and enforce the laws made by Congress. However, the president also has a host of other responsibilities tied to this job description. They include: recommending legislation (laws) to Congress, calling special sessions of Congress, delivering messages to Congress, signing or vetoing legislation, nominating federal judges, appointing heads of federal departments and agencies and other principal federal officials, appointing representatives to foreign countries, carrying on official business with foreign nations, acting as commander in chief of the armed forces, and granting pardons for offenses against the United States.\nThe U.S. Congress consists of the Senate and the House of Representatives. Both senators and representatives are responsible for representing the people of the states who elect them. This involves voting on and writing bills in the U.S. Congress. There are, however, some major differences between a U.S. senator and a representative. While both senators and representatives are permitted to introduce bills, senators are restricted from introducing bills that raise revenue, such as tax bills. There are 100 senators in Congress; two senators are allotted for each state. This number is independent of each state’s population. However, the number of U.S. representatives a state has is determined by the population of that particular state. There are 435 representatives in Congress, and each state has at least one representative. Another difference involves the length of time a senator and a representative are permitted to serve. A senator represents their state for a six-year term. A representative, on the other hand, serves for a two-year term.\nLike the U.S. federal government, state governments have three branches: the executive, legislative, and judicial. Each branch functions and works a lot like the corresponding branch at the federal level. The chief executive of a state is the governor, who is elected by popular vote, typically (with notable exceptions) for a four-year term. Except for Nebraska, which has a single legislative body, all states have a bicameral (two-house) legislature, with the upper house usually called the Senate and the lower house called the House of Representatives, the House of Delegates, or the General Assembly. The sizes of these two legislatures vary. Typically, the upper house is made up of between 30 and 50 members; the lower house is made up of between 100 and 150 members.\nA governor of a U.S. state is responsible for the well-being of their state. The details of this job include many hands-on tasks and leadership duties. The governor’s executive powers include the appointment and removal of state officials, the supervision of executive branch staff, the formulation of the state budget, and the leadership of the state militia as its commander in chief. Law-making powers include the power to recommend legislation, to call special sessions of the legislature, and to veto measures passed by the legislature. In more than 40 states, governors have the power to veto (or reject) several parts of a bill without rejecting it altogether. The governor can also pardon (excuse) a criminal or reduce a criminal’s sentence.\nThe mayor-council is the oldest form of city government in the United States. Its structure is similar to that of the state and federal governments, with an elected mayor as chief of the executive branch and an elected council that represents the various neighborhoods, forming the legislative branch. The mayor usually appoints heads of city departments and other officials. The mayor also has the power to veto the laws of the city (called ordinances), and prepares the city’s budget. The council passes city laws, sets the tax rates on property, and decides how the city departments spend their money.\nThe town meeting is one aspect of local government that still exists today, after having been created in the early years of the United States. At least once a year the registered voters of the town meet in open session to elect officers, debate local issues, and pass laws for operating a government. As a group, or body, they decide on road construction and repair, construction of public buildings and facilities (such as libraries and parks), tax rates, and the town budget. Having existed for more than two centuries, the town meeting is often called the purest form of direct democracy because governmental power is not delegated but instead exercised directly by the people. However, town meetings cannot be found in every area of the United States. They are mostly conducted in the small towns of New England, where the first colonies were established.\nThe Bill of Rights is made up of the first 10 amendments to the U.S. Constitution. The Bill of Rights guarantees rights and liberties to the American people. These amendments were proposed by Congress in 1789, and ratified (approved) by three-fourths of the states on December 15, 1791, thereby officially becoming part of the Constitution. The first eight amendments outline many individual rights, while the Ninth and Tenth Amendments are general rules of interpretation of the relationship among the people, the state governments, and the federal government.\nThe Bill of Rights limits the ability of the government to intrude upon certain individual liberties, guaranteeing freedom of speech, press, assembly, and religion to the people of the United States. Nearly two-thirds of the Bill of Rights was written to safeguard the rights of those suspected or accused of a crime, providing for due process of law, fair trials, freedom from self-incrimination and from cruel and unusual punishment, and protection against being tried twice in court for the same crime. Since the adoption of the Bill of Rights, 17 additional amendments have been added to the Constitution. While a number of these amendments revised how the federal government is structured and operates, many expanded individual rights and freedoms.\nLaws are enforced by the courts and the judicial system. If someone breaks a law or a business or organization does something illegal, they go to the judicial branch of government for review of their actions. The judicial branch is made up of different courts. The court leader, or judge, interprets the meaning of laws, how they are applied, and whether they break the rules of the Constitution. If a person or group is found guilty of breaking a law, the judicial system decides how they should be punished.\nIn the United States, laws have been written to protect the rights of someone accused of committing a crime. That person is considered innocent until proven guilty in a court of law. Someone suspected of a crime is usually arrested and taken into custody by a police officer. Sometimes, the case is presented before a grand jury (a group of citizens who examines the accusations made). The grand jury files an indictment, or a formal charge, if there appears to be enough evidence for a trial. In many criminal cases, however, there is no grand jury. While awaiting trial, the accused may be temporarily released, sometimes on bail (which is the amount of money meant to guarantee that the person will return for trial instead of leaving the country), or kept in a local jail. Trials are usually held before a judge and a jury of 12 citizens. The government presents its case against an accused person, or defendant, through a district attorney, and another attorney defends the accused. If the defendant is judged innocent, they are released. If they are found guilty of committing a crime, the judge decides the punishment or sentence, using established guidelines. The convicted person may be forced to pay a fine, pay damages, or go to prison.\nCitizens of the United States enjoy all of the freedoms, protections, and legal rights that the Constitution promises. However, living in the United States doesn’t automatically make a person an American citizen. People born in the United States or born to U.S. citizens in foreign countries are granted citizenship in the United States. Persons born in other countries who want to become citizens must apply for and pass a citizenship test. Those who become citizens in this way are called naturalized citizens.\nRepublics such as the United States are based upon a voting population. If the citizens of the country do not vote, then politicians do not necessarily need to heed their interests. It is necessary for the people of a democratic country to constitutionally voice their opinions, and they do this by voting for their city, state, and country’s leaders and on issues and initiatives presented to them in elections.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/dictionary/crested",
    "content": "",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/money/Henry-Clay-Frick",
    "content": "Henry Clay Frick (born December 19, 1849, West Overton, Pennsylvania, U.S.—died December 2, 1919, New York City) was a U.S. industrialist, art collector, and philanthropist who helped build the world’s largest coke and steel operations.\nFrick began building and operating coke ovens in 1870, and the following year he organized Frick and Company. Taking advantage of the difficult times following the financial panic of 1873, he acquired extensive coal deposits and supplied Pittsburgh with the coke required for its steel and iron industry.\nIn 1889 Frick was made chairman of Carnegie Brothers and Company to reorganize their steel business. He initiated far-reaching improvements and bought out Carnegie’s chief competitor, the Duquesne Steel Works. He was responsible for building Carnegie into the largest manufacturer of steel and coke in the world. As a result of his leading role in the dispute during the Homestead (Pennsylvania) steel strike of 1892, he was shot and stabbed by Alexander Berkman, an anarchist, but survived.\nFrick played a major role in the formation of the United States Steel Corporation in 1901 and later became a director. He also served as a director of a number of railroads.\nUpon his death Frick bequeathed $15,000,000 and his Fifth Avenue mansion to New York City to establish the Frick Collection, a trove of paintings, bronzes, and enamels he had collected over a 40-year period. It is generally considered one of the great privately owned museums of the world. His other gifts include a 150-acre (61-hectare) park and a $2,000,000 endowment to the city of Pittsburgh, as well as liberal contributions to Princeton University.\nAndrew Carnegie (born November 25, 1835, Dunfermline, Fife, Scotland—died August 11, 1919, Lenox, Massachusetts, U.S.) was a Scottish-born American industrialist who led the enormous expansion of the American steel industry in the late 19th century. He was also one of the most important philanthropists of his era.\nCarnegie’s father, William Carnegie, a handloom weaver, was a Chartist and marcher for workingman’s causes; his maternal grandfather, Thomas Morrision, also an agitator, had been a friend of William Cobbett. During the young Carnegie’s childhood the arrival of the power loom in Dunfermline and a general economic downturn impoverished his father, inducing the Carnegies to immigrate in 1848 to the United States, where they joined a Scottish colony of relatives and friends in Allegheny, Pennsylvania (now part of Pittsburgh). Young Andrew began work at age 12 as a bobbin boy in a cotton factory. He quickly became enthusiastically Americanized, educating himself by reading and writing and attending night school.\nAt age 14 Carnegie became a messenger in a telegraph office, where he eventually caught the notice of Thomas Scott, a superintendent of the Pennsylvania Railroad Company, who made Carnegie his private secretary and personal telegrapher in 1853. Carnegie’s subsequent rise was rapid, and in 1859 he succeeded Scott as superintendent of the railroad’s Pittsburgh division. While in this post he invested in the Woodruff Sleeping Car Company (the original holder of the Pullman patents) and introduced the first successful sleeping car on American railroads. He had meanwhile begun making shrewd investments in such industrial concerns as the Keystone Bridge Company, the Superior Rail Mill and Blast Furnaces, the Union Iron Mills, and the Pittsburgh Locomotive Works. He also profitably invested in a Pennsylvania oilfield, and he took several trips to Europe, selling railroad securities. By the age of 30 he had an annual income of $50,000.\nDuring his trips to Britain he came to meet steelmakers. Foreseeing the future demand for iron and steel, Carnegie left the Pennsylvania Railroad in 1865 and started managing the Keystone Bridge Company. From about 1872–73, at about age 38, he began concentrating on steel, founding near Pittsburgh the J. Edgar Thomson Steel Works, which would eventually evolve into the Carnegie Steel Company. In the 1870s Carnegie’s new company built the first steel plants in the United States to use the new Bessemer steelmaking process, borrowed from Britain. Other innovations followed, including detailed cost- and production-accounting procedures that enabled the company to achieve greater efficiencies than any other manufacturing industry of the time. Any technological innovation that could reduce the cost of making steel was speedily adopted, and in the 1890s Carnegie’s mills introduced the basic open-hearth furnace into American steelmaking. Carnegie also obtained greater efficiency by purchasing the coke fields and iron-ore deposits that furnished the raw materials for steelmaking, as well as the ships and railroads that transported these supplies to his mills. The vertical integration thus achieved was another milestone in American manufacturing. Carnegie also recruited extremely capable subordinates to work for him, including the administrator Henry Clay Frick, the steelmaster and inventor Captain Bill Jones, and his own brother Thomas M. Carnegie.\nIn 1889 Carnegie’s vast holdings were consolidated into the Carnegie Steel Company, a limited partnership that henceforth dominated the American steel industry. In 1890 the American steel industry’s output surpassed that of Great Britain’s for the first time, largely owing to Carnegie’s successes. The Carnegie Steel Company continued to prosper even during the depression of 1892, which was marked by the bloody Homestead strike. (Although Carnegie professed support for the rights of unions, his goals of economy and efficiency may have made him favour local management at the Homestead plant, which used Pinkerton guards to try to break the Amalgamated Association of Iron, Steel, and Tin Workers.)\nIn 1900 the profits of Carnegie Steel (which became a corporation) were $40,000,000, of which Carnegie’s share was $25,000,000. Carnegie sold his company to J.P. Morgan’s newly formed United States Steel Corporation for $480,000,000 in 1901. He subsequently retired and devoted himself to his philanthropic activities, which were themselves vast.\nCarnegie wrote frequently about political and social matters, and his most famous article, “Wealth,” appearing in the June 1889 issue of the North American Review, outlined what came to be called the Gospel of Wealth. This doctrine held that a man who accumulates great wealth has a duty to use his surplus wealth for “the improvement of mankind” in philanthropic causes. A “man who dies rich dies disgraced.”\nCarnegie’s own distributions of wealth came to total about $350,000,000, of which $62,000,000 went for benefactions in the British Empire and $288,000,000 for benefactions in the United States. His main “trusts,” or charitable foundations, were (1) the Carnegie Trust for the Universities of Scotland (Edinburgh), founded in 1901 and intended for the improvement and expansion of the four Scottish universities and for Scottish student financial aid, (2) the Carnegie Dunfermline Trust, founded in 1903 and intended to aid Dunfermline’s educational institutions, (3) the Carnegie United Kingdom Trust (Dunfermline), founded in 1913 and intended for various charitable purposes, including the building of libraries, theatres, child-welfare centres, and so on, (4) the Carnegie Institute of Pittsburgh, founded in 1896 and intended to improve Pittsburgh’s cultural and educational institutions, (5) the Carnegie Institution of Washington, founded in 1902 and contributing to various areas of scientific research, (6) the Carnegie Endowment for International Peace, founded in 1910 and intended to disseminate (usually through publications) information to promote peace and understanding among nations, (7) the Carnegie Corporation of New York, the largest of all Carnegie foundations, founded in 1911 and intended for “the advancement and diffusion of knowledge and understanding among the people of the United States” and, from 1917, Canada and the British colonies. The Carnegie Corporation of New York has aided colleges and universities and libraries, as well as research and training in law, economics, and medicine.\nChief among Carnegie’s writings are Triumphant Democracy (1886; rev. ed. 1893), The Gospel of Wealth, a collection of essays (1900), The Empire of Business (1902), Problems of To-day (1908), and Autobiography (1920).\nCarnegie married Louise Whitfield in 1887. Until World War I, the Carnegies alternated between Skibo Castle in northern Scotland, their home in New York City, and their summer house “Shadowbrook” in Lenox, Massachusetts.\nFor historical and social background, see Louis M. Hacker, The World of Andrew Carnegie: 1865–1901 (1968); and George S. Bobinski, Carnegie Libraries (1969). Biographies include Joseph Frazier Wall, Andrew Carnegie (1970, reissued 1989), a massive definitive study; and Harold C. Livesay, Andrew Carnegie and the Rise of Big Business (1975, reissued 1988). Carnegie’s many writings are discussed in George Swetnam, Andrew Carnegie (1980).",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/list/11-egyptian-gods-and-goddesses",
    "content": "Egypt had one of the largest and most complex pantheons of gods of any civilization in the ancient world. Over the course of Egyptian history hundreds of gods and goddesses were worshipped. The characteristics of individual gods could be hard to pin down. Most had a principle association (for example, with the sun or the underworld) and form. But these could change over time as gods rose and fell in importance and evolved in ways that corresponded to developments in Egyptian society. Here are a few of the most important deities to know.\nOsiris, one of Egypt’s most important deities, was god of the underworld. He also symbolized death, resurrection, and the cycle of Nile floods that Egypt relied on for agricultural fertility.\nAccording to the myth, Osiris was a king of Egypt who was murdered and dismembered by his brother Seth. His wife, Isis, reassembled his body and resurrected him, allowing them to conceive a son, the god Horus. He was represented as a mummified king, wearing wrappings that left only the green skin of his hands and face exposed.\nThe origins of Isis are obscure. Unlike many gods, she can’t be tied to a specific town, and there are no certain mentions of her in the earliest Egyptian literature. Over time she grew in importance, though, eventually becoming the most important goddess in the pantheon. As the devoted wife who resurrected Osiris after his murder and raised their son, Horus, Isis embodied the traditional Egyptian virtues of a wife and mother.\nAs the wife of the god of the underworld, Isis was also one of the main deities concerned with rites for the dead. Along with her sister Nephthys, Isis acted as a divine mourner, and her maternal care was often depicted as extending to the dead in the underworld.\nIsis was one of the last of the ancient Egyptian gods to still be worshipped. In the Greco-Roman period she was identified with the Greek goddess Aphrodite and her cult spread as far west as Great Britain and as far east as Afghanistan. It is believed that depictions of Isis with the infant Horus influenced Christian imagery of Mary with the infant Jesus.\nDepicted as a falcon or as a man with a falcon’s head, Horus was a sky god associated with war and hunting. He was also the embodiment of the divine kingship, and in some eras the reigning king was considered to be a manifestation of Horus.\nAccording to the Osiris myth, Horus was the son of Isis and Osiris, magically conceived after the murder of Osiris by his brother Seth. Horus was raised to avenge his father’s murder. One tradition holds that Horus lost his left eye fighting with Seth, but his eye was magically healed by the god Thoth. Because the right and left eyes of Horus were associated, respectively, with the sun and the moon, the loss and restoration of Horus’s left eye gave a mythical explanation for the phases of the moon.\nSeth was the god of chaos, violence, deserts, and storms. In the Osiris myth, he is the murderer of Osiris (in some versions of the myth, he tricks Osiris into laying down in a coffin and then seals it shut.)\nSeth’s appearance poses a problem for Egyptologists. He is often depicted as an animal or as a human with the head of an animal. But they can’t figure out what animal he’s supposed to be. He usually has a long snout and long ears that are squared at the tips. In his fully animal form, he has a thin doglike body and a straight tail with a tuft on the end. Many scholars now believe that no such animal ever existed and that the Seth animal is some sort of mythical composite.\nPtah was the head of a triad of gods worshipped at Memphis. The other two members of the triad were Ptah’s wife, the lion-headed goddess Sekhmet, and the god Nefertem, who may have been the couple’s son.\nPtah’s original association seems to have been with craftsmen and builders. The 4th-dynasty architect Imhotep was deified after his death as a son of Ptah.\nScholars have suggested that the Greek word Aiguptos—the source of the name Egypt—may have started as a corruption of Hwt-Ka-Ptah, the name of one of Ptah’s shrines.\nOne of several deities associated with the sun, the god Re was usually represented with a human body and the head of a hawk. It was believed that he sailed across the sky in a boat each day and then made a passage through the underworld each night, during which he would have to defeat the snake god Apopis in order to rise again.\nRe’s cult was centered in Heliopolis, now a suburb of Cairo. Over time, Re came to be syncretized with other sun deities, especially Amon.\nThe goddess Hathor was usually depicted as a cow, as a woman with the head of a cow, or as a woman with cow’s ears. Hathor embodied motherhood and fertility, and it was believed that she protected women in childbirth. She also had an important funerary aspect, being known as “the lady of the west.” (Tombs were generally built on the west bank of the Nile.) In some traditions, she would welcome the setting sun every night; living people hoped to be welcomed into the afterlife in the same way.\nAnubis was concerned with funerary practices and the care of the dead. He was usually represented as a jackal or as a man with the head of a jackal. The association of jackals with death and funerals likely arose because Egyptians would have observed jackals scavenging around cemeteries.\nIn the Old Kingdom (c. 2575–2130 BCE), before Osiris rose to prominence as the lord of the underworld, Anubis was considered the principal god of the dead. According to the Osiris myth, Anubis embalmed and wrapped the body of the murdered king, becoming the patron god for embalmers.\nThoth, the god of writing and wisdom, could be depicted in the form of a baboon or a sacred ibis or as a man with the head of an ibis. He was believed to have invented language and the hieroglyphic script and to serve as a scribe and adviser for the gods. As the god of wisdom, Thoth was said to possess knowledge of magic and secrets unavailable to the other gods.\nIn underworld scenes showing the judgment undergone by the deceased after their deaths, Thoth is depicted as weighing the hearts of the deceased and reporting the verdict to Osiris, the god of the dead.\nIn her earliest forms, the cat goddess Bastet was represented as a woman with the head of a lion or a wild cat. She took the less ferocious form of a domestic cat in the first millennium BCE.\nIn later periods she was often represented as a regal-looking seated cat, sometimes wearing rings in her ears or nose. In the Ptolemaic period she came to be associated with the Greek goddess Artemis, the divine hunter and goddess of the moon.\nBefore rising to national importance in the New Kingdom (c. 1539–1292 BCE), the god Amon was worshipped locally in the southern city of Thebes. Amon was a god of the air, and the name probably means the “Hidden One.” He was usually represented as a man wearing a crown with two vertical plumes. His animal symbols were the ram and the goose.\nAfter the rulers of Thebes rebelled against a dynasty of foreign rulers known as the Hyksos and reestablished native Egyptian rule throughout Egypt, Amon received credit for their victory. In a form merged with the sun god Re, he became the most powerful deity in Egypt, a position he retained for most of the New Kingdom.\nToday the massive temple complex devoted to Amon-Re at Karnak is one of the most visited monuments in Egypt.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/biography/Dana-Scott",
    "content": "Dana Scott (born October 11, 1932, Berkeley, California, U.S.) is an American mathematician, logician, and computer scientist who was co-winner of the 1976 A.M. Turing Award, the highest honour in computer science. Scott and the Israeli American mathematician and computer scientist Michael O. Rabin were cited in the award for their early joint paper “Finite Automata and Their Decision Problem,” which introduced the idea of nondeterministic machines to the field of automata theory, and for their subsequent independent work.\nScott earned a bachelor’s degree (1954) in mathematics from the University of California, Berkeley, and a doctorate (1958) in mathematics from Princeton University. He began his academic career at the University of Chicago (1958–60), followed by the University of California, Berkeley (1960–63), Stanford University (1963–69), Princeton University (1969–72), and the University of Oxford (1972–81). In 1981 he joined the faculty at Carnegie Mellon University, where he became Hillman University Professor of Mathematical Logic, Computer Science, and Philosophy the following year. Scott retired as professor emeritus in 2003.\nScott’s last position, at Carnegie Mellon, gives some inkling of the remarkable diversity of his academic interests. In addition to contributing his seminal work on automata theory, Scott collaborated in the 1970s with the British computer scientist Christopher Strachey to lay the foundations of the mathematical (or denotational) semantics of computer programming languages. The outgrowth of that work led to Scott’s introduction of domain theory, providing, in particular, mathematical models for the λ-calculus, or lambda calculus (a formal mathematical-logical system invented in 1936 by the American logician Alonzo Church), and many other related theories. Scott was the first editor in chief of Logical Methods in Computer Science, an online open-access journal founded in 2005.\nScott was elected to the American Association for the Advancement of Science, the American Academy of Arts and Sciences, and the U.S. National Academy of Sciences. His awards include the 1972 LeRoy P. Steele Prize from the American Mathematical Society, the 1997 Rolf Shock Prize in Logic and Philosophy from the Royal Swedish Academy of Sciences, and the 2009 Gold Medal from the Russian Academy of Sciences.\ndomain name, address of a computer, organization, or other entity on the Internet. Domain names are typically in a three-level “server.organization.type” format. The top level, called the top-level domain, has usually denoted the type of organization, such as “com” (for commercial sites) or “edu” (for educational sites). However, in 2011 the Internet Corporation for Assigned Names and Numbers (ICANN) announced that it would greatly increase the number of top-level domains by allowing nearly any new top-level domain name in any language. The second level is the top level plus the name of the organization (e.g., “britannica.com” for Encyclopædia Britannica). The third level identifies a specific host server at the address, such as the “www” (World Wide Web) host server for “www.britannica.com.” A domain name is ultimately mapped to an IP address, but two or more domain names can be mapped to the same IP address. A domain name must be unique on the Internet and must be assigned by a registrar accredited by ICANN. See also URL.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/list/new-seven-wonders-of-the-world",
    "content": "In 2000 a Swiss foundation launched a campaign to determine the New Seven Wonders of the World. Given that the original Seven Wonders list was compiled in the 2nd century BCE—and that only one entrant is still standing (the Pyramids of Giza)—it seemed time for an update. And people around the world apparently agreed, as more than 100 million votes were cast on the Internet or by text messaging. The final results, which were announced in 2007, were met with cheers as well as some jeers—a number of prominent contenders, such as Athens’s Acropolis, failed to make the cut. Do you agree with the new list?\nGreat might be an understatement. One of the world’s largest building-construction projects, the Great Wall of China is widely thought to be about 5,500 miles (8,850 km) long; a disputed Chinese study, however, claims the length is 13,170 miles (21,200 km). Work began in the 7th century BCE and continued for two millennia. Although called a “wall,” the structure actually features two parallel walls for lengthy stretches. In addition, watchtowers and barracks dot the bulwark. One not-so-great thing about the wall, however, was its effectiveness. Although it was built to prevent invasions and raids, the wall largely failed to provide actual security. Instead, scholars have noted that it served more as “political propaganda.”\nChichén Itzá is a Mayan city on the Yucatán Peninsula in Mexico, which flourished in the 9th and 10th centuries CE. Under the Mayan tribe Itzá—who were strongly influenced by the Toltecs—a number of important monuments and temples were built. Among the most notable is the stepped pyramid El Castillo (“The Castle”), which rises 79 feet (24 meters) above the Main Plaza. A testament to the Mayans’ astronomical abilities, the structure features a total of 365 steps, the number of days in the solar year. During the spring and autumnal equinoxes, the setting sun casts shadows on the pyramid that give the appearance of a serpent slithering down the north stairway; at the base is a stone snake head. Life there was not all work and science, however. Chichén Itzá is home to the largest tlachtli (a type of sporting field) in the Americas. On that field the residents played a ritual ball game popular throughout pre-Columbian Mesoamerica.\nThe ancient city of Petra, Jordan, is located in a remote valley, nestled among sandstone mountains and cliffs. It was purported to be one of the places where Moses struck a rock and water gushed forth. Later the Nabataeans, an Arab tribe, made it their capital, and during this time it flourished, becoming an important trade center, especially for spices. Noted carvers, the Nabataeans chiseled dwellings, temples, and tombs into the sandstone, which changed color with the shifting sun. In addition, they constructed a water system that allowed for lush gardens and farming. At its height, Petra reportedly had a population of 30,000. The city began to decline, however, as trade routes shifted. A major earthquake in 363 CE caused more difficulty, and after another tremor hit in 551, Petra was gradually abandoned. Although rediscovered in 1912, it was largely ignored by archaeologists until the late 20th century, and many questions remain about the city.\nThis Incan site near Cuzco, Peru, was “discovered” in 1911 by Hiram Bingham, who believed it was Vilcabamba, a secret Incan stronghold used during the 16th-century rebellion against Spanish rule. Although that claim was later disproved, the purpose of Machu Picchu has confounded scholars. Bingham believed it was home to the “Virgins of the Sun,” women who lived in convents under a vow of chastity. Others think that it was likely a pilgrimage site, while some believe it was a royal retreat. (One thing it apparently should not be is the site of a beer commercial. In 2000 a crane being used for such an ad fell and cracked a monument.) What is known is that Machu Picchu is one of the few major pre-Columbian ruins found nearly intact. Despite its relative isolation high in the Andes Mountains, it features agricultural terraces, plazas, residential areas, and temples.\nChrist the Redeemer, a colossal statue of Jesus, stands atop Mount Corcovado in Rio de Janeiro. Its origins date to just after World War I, when some Brazilians feared a “tide of godlessness.” They proposed a statue, which was ultimately designed by Heitor da Silva Costa, Carlos Oswald, and Paul Landowski. Construction began in 1926 and was completed five years later. The resulting monument stands 98 feet (30 meters) tall—not including its base, which is about 26 feet (8 meters) high—and its outstretched arms span 92 feet (28 meters). It is the largest Art Deco sculpture in the world. Christ the Redeemer is made of reinforced concrete and is covered in approximately six million tiles. Somewhat disconcertingly, the statue has often been struck by lightning, and in 2014 the tip of Jesus’s right thumb was damaged during a storm.\nThe Colosseum in Rome was built in the first century by order of the Emperor Vespasian. A feat of engineering, the amphitheater measures 620 by 513 feet (189 by 156 meters) and features a complex system of vaults. It was capable of holding 50,000 spectators, who watched a variety of events. Perhaps most notable were gladiator fights, though men battling animals was also common. In addition, water was sometimes pumped into the Colosseum for mock naval engagements. However, the belief that Christians were martyred there—namely, by being thrown to lions—is debated. According to some estimates, about 500,000 people died in the Colosseum. Additionally, so many animals were captured and then killed there that certain species reportedly became extinct.\nThis mausoleum complex in Agra, India, is regarded as one of the world’s most iconic monuments and is perhaps the finest example of Mughal architecture. It was built by Emperor Shah Jahān (reigned 1628–58) to honor his wife Mumtāz Maḥal (“Chosen One of the Palace”), who died in 1631 giving birth to their 14th child. It took about 22 years and 20,000 workers to construct the complex, which includes an immense garden with a reflecting pool. The mausoleum is made of white marble that features semiprecious stones in geometric and floral patterns. Its majestic central dome is surrounded by four smaller domes. According to some reports, Shah Jahān wished to have his own mausoleum made out of black marble. However, he was deposed by one of his sons before any work began.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/summary/Pittsburgh",
    "content": "Pittsburgh, City (pop., 2020: 302,971), southwestern Pennsylvania, U.S. It is situated at the confluence of the Allegheny and Monongahela rivers, where they form the Ohio River. In 1758 the French Fort Duquesne was captured there by the British, and the site was renamed Pitt. It was incorporated as a borough in 1794 and as a city in 1816. In the 19th century it developed rapidly as a steel-manufacturing centre. The American Federation of Labor began there in 1881 (see AFL-CIO). The second largest city in the state, it is the centre of an urban industrial complex that includes several neighbouring cities. There are more than 150 industrial research laboratories in the area. It is home to the University of Pittsburgh, Carnegie Mellon University, and other educational institutions.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/place/Rust-Belt",
    "content": "Rust Belt, geographic region of the United States that was long the country’s manufacturing, steelmaking, and coal-producing heartland but that underwent dramatic industrial decline that resulted in widespread unemployment, increased poverty, decay, and population loss. Because the Rust Belt is defined by similar economic experience rather than by natural borders, its boundaries are debated; however, it is generally viewed as encompassing a large part of the Midwest (Indiana, Illinois, Michigan, Missouri, Ohio, and Wisconsin) along with Pennsylvania, West Virginia, and portions of New York. Some schemes extend it east to include the former textile-manufacturing cities of Massachusetts, west to eastern Iowa,  and south to the coalfields of Kentucky. The region’s prosperity sprang from its abundance of natural resources, including coal and iron ore reserves, as well as rivers and lakes that abetted transport and travel. From the 1950s to the 1980s it experienced significant economic decline and dislocation largely because of foreign competition, increased labour costs, failure to modernize equipment and infrastructure, and technological advancements that replaced workers.\nThe term “Rust Belt” is derived from a statement made to a gathering of steelworkers in Cleveland by Democratic presidential candidate Walter Mondale during the 1984 election campaign. Responding to what he viewed as the myopic optimism of incumbent Republican Pres. Ronald Reagan’s rhetoric, Mondale alluded to the iconic desolation of the Depression-era Dust Bowl and claimed that Reagan’s policies were “turning our industrial Midwest into a rust bowl.” The choice of the word rust evoked the image of the abandoned decaying factories that were becoming increasingly common features of the region’s landscape. Riffing on the trend of using belt to characterize distinctive regions of the country (e.g., Sun Belt and Corn Belt), journalists quickly transformed rust bowl into Rust Belt and began using the term as a mostly pejorative label for the country’s traditional industrial heartland.\nThat industrial heartland—which has analogues in Germany’s Ruhr region, the Donbas of Ukraine, the north of England, and southern Wales—sprouted in the second half of the 19th century and continued expanding in the first half of 20th century. The access to plentiful iron ore and coal reserves provided by the Great Lakes and waterways such as the Ohio River and the Erie Canal were essential to the development of the region’s dominant steelmaking industry. Iron ore from the Mesabi Range of Minnesota, the Marquette range of the Upper Peninsula of Michigan, and the Menominee and Gogebic ranges straddling the Michigan-Wisconsin border was shipped to ports on the southern shore of Lake Michigan, where furnaces in steel plants on the South Side of Chicago and in Gary and Hammond, Indiana, were fired by coal from southern Illinois. Freighters also carried iron ore to Lake Erie port cities such as Cleveland and Buffalo to be used in steel plants there or transported to mills in cities such as Youngstown, Ohio, and Pittsburgh, which was connected by the Ohio, Allegheny, and Monongahela rivers to a network of smaller cities where steel mills and metals-related manufacturing flourished, all benefiting from their access to coal from Pennsylvania and West Virginia.\nThe growth of the steel industry in turn facilitated the development of the machine and machine part industries, the weapons industry, and the automotive and automotive parts industries. Detroit famously became the “Motor City,” epicentre of the American automotive industry (and the “Arsenal of Democracy” during World War II), but car and truck factories thrived throughout the burgeoning industrial heartland, from Janesville and Kenosha, Wisconsin, to Fort Wayne, Indiana, and Lordstown, Ohio. Akron, Ohio, was crowned the world’s tire and rubber capital. The manufacture of farm equipment and heavy machinery became big business in Chicago and smaller Illinois cities such as Moline and Peoria. Both immigrants and domestic migrants(see Great Migration) looking for work flocked to the towns and cities of the industrial heartland, whose populations grew exponentially. Local economies hummed, and rich diverse urban cultures blossomed. Fortunes were made by a privileged few (e.g., John D. Rockefeller and Andrew Carnegie), but the rise of organized labour contributed to the creation of good-paying blue-collar jobs and a growing middle class (though persistent racism limited access and equity for people of colour).\nThe reversal of the region’s prosperity is most often dated to the 1970s, but some historians and economists find its roots as early as in the 1950s. In either case, most observers emphasize the role of comparative advantages enjoyed by foreign industrial competitors, especially in Asia. Lower labour costs were pivotal to those advantages, and one response by Rust Bowl manufacturers was to relocate their operations to the American South, where labour unions were not as established and wages were lower. Complacency born of the post-World War II economic boom experienced by U.S. industry is also often identified as being responsible for the widespread failure to update industrial machinery and infrastructure, which also contributed to a decline in competitiveness with foreign rivals. Some economists point to currency exchange rates and a strong U.S. dollar that made American products relatively more expensive on the world market in the early 1980s. Still other economists cite the expanding world market, economic globalization, and the removal of trade barriers as important contributing factors to the region’s declining fortunes. In that vein the North American Free Trade Agreement (NAFTA) is frequently villainized.\nWhatever the cause, the result of industrial decline in the Rust Belt was shuttered factories, mills, and mines; devastating unemployment; shrinking tax bases; growing poverty; and dislocation. Workers fled the region in droves in search of economic opportunity elsewhere. Cleveland (the population of which declined from about 876,000 in 1960 to about 505,000 in 1990) was mocked as the “Mistake on the Lake”; Michiganders who flooded Texas in the 1970s in the hope of finding work were disparaged as “black tag” people (after the black Michigan license plates on their cars).\nAnne Trubek, founder of Belt Magazine, identified September 19, 1977, “Black Monday,” the day on which it was announced that the Campbell Works would be closed (sealing the fate of Youngstown Sheet and Tube, once the largest corporation in Ohio), as the most symbolic date in Rust Belt history, and, in his Rust Belt anthem “Youngstown,” Bruce Springsteen movingly evokes the prideful heartbreak and disillusioned sense of abandonment that resulted from the transformation of that Ohio city and scores like it. Billy Joel paints a similar portrait of industrial upheaval in eastern Pennsylvania in another popular song, “Allentown.”\nIn the 21st century many of the towns and cities of the Rust Belt continue to struggle to regain their former prominence, but others have reinvented themselves, embracing service industries and new technologies or capitalizing on the public and private colleges and universities that are a vital legacy of the region’s longtime commitment to excellence in education.\nglobal city, an urban centre that enjoys significant competitive advantages and that serves as a hub within a globalized economic system. The term has its origins in research on cities carried out during the 1980s, which examined the common characteristics of the world’s most important cities. However, with increased attention being paid to processes of globalization during subsequent years, these world cities came to be known as global cities. Linked with globalization was the idea of spatial reorganization and the hypothesis that cities were becoming key loci within global networks of production, finance, and telecommunications. In some formulations of the global city thesis, then, such cities are seen as the building blocks of globalization. Simultaneously, these cities were becoming newly privileged sites of local politics within the context of a broader project to reconfigure state institutions.\nEarly research on global cities concentrated on key urban centres such as London, New York City, and Tokyo. With time, however, research has been completed on emerging global cities outside of this triad, such as Amsterdam, Frankfurt, Houston, Los Angeles, Mexico City, Paris, São Paulo, Sydney, and Zürich. Such cities are said to knit together to form a global city network serving the requirements of transnational capital across broad swathes of territory.\nThe rise of global cities has been linked with two globalization-related trends: first, the expansion of the role of transnational corporations (TNCs) in global production patterns and, second, the decline of mass production along Fordist lines and the concomitant rise of flexible production centred within urban areas. These two trends explain the emergence of networks of certain cities serving the financial and service requirements of TNCs while other cities suffer the consequences of deindustrialization and fail to become “global.” Global cities are those that therefore become effective command-and-coordination posts for TNCs within a globalizing world economy. Such cities have also assumed a governance role at the local scale and within wider configurations of what some commentators have termed the “glocalization” of state institutions. This refers to processes in which certain national state functions of organization and administration have been devolved to the local scale. An example of this would be London. Since the 1980s London has consolidated its position as a global banking and financial centre, de-linked from the national economy.\nThe global city thesis poses a challenge to state-centric perspectives on contemporary international political economy because it implies the disembedding of cities from their national territorial base, so that they occupy an extraterritorial space. Global cities, it is suggested, have more interconnectedness with other cities and across a transnational field of action than with the national economy. Global cities are also said to share many of the same characteristics because of their connectedness and shared experiences of globalization. They all exhibit clear signs of deindustrialization. They possess the concentration of financial and service industries within their spatial boundaries, as well as the concentration of large pools of labour. On the downside, many also share experiences of class and ethnic conflict. They often have segmented labour markets in which employees of key industries enjoy well-paid and consumerist lifestyles while a lower stratum of workers staffs less well-paid, more precarious, and less attractive positions within the urban economy. It has been further argued that the promotion of global cities runs the risk of economically marginalizing nonurban populations within the national economy.\nAlthough global cities are interconnected, embedded as they are in global production and financial networks, they are also locked into competition with one another to command increasing resources and to attract capital. To successfully compete, local governments have been keen to promote their cities as global. Such cities have been marketed as “entrepreneurial” centres, sites of innovation in the knowledge economy, and as being rich with cultural capital. A common strategy has been to stress the multiethnic qualities of a city, for example. This is intended to stress its cosmopolitan and global character and to disassociate the city from its actual territorial, ethnic, or cultural setting. Such cities also regularly compete to host world events of considerable prestige that present further economic opportunities, such as the Olympic Games.\nThere has been some skepticism regarding the global city thesis in its simplest formulation. On a qualitative level, some scholars questioned whether global cities are indeed new phenomena and pointed to the long-standing existence of similar economic centres over time. One can think of Florence during the Renaissance, for example, or Manchester during the Industrial Revolution. Other commentators questioned whether the ascendance of global cities implies state decline along zero-sum lines. These skeptics argued that a more complex and interdependent relationship exists between the state and cities under its national jurisdiction. Indeed, national governments can play a proactive role in the promotion of key urban centres as global cities. Correspondingly, it is possible that global cities occupy the forefront position within a hierarchy of cities and local spaces that together constitute the national economy. Such a perspective would appear to transcend a dichotomizing view of global cities and the national state.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/story/what-if-the-president-is-impeached",
    "content": "Impeachment proceedings begin when the president is accused of “Treason, Bribery, or other high Crimes and Misdemeanors” (U.S. Constitution, Article II, section 4). The House of Representatives votes on articles of impeachment, or formal charges of misconduct. If approved by a majority of members, the president is impeached, though he or she remains in office. The next step in the impeachment process is a trial in the Senate. While there is debate over whether the Senate is constitutionally required to take up the matter, it has held trials in past cases. Selected members of the House act as prosecutors, the chief justice of the Supreme Court serves as judge, and the senators are the jurors. If at least two-thirds of the senators then present vote for conviction, the president is removed from office and replaced with the vice president. The decision of the Senate cannot be appealed to the federal courts. After leaving office, the former president may still be prosecuted for his or her alleged misconduct.\nOnly three presidents—Andrew Johnson (1868), Bill Clinton (1998), and Donald Trump (twice, in 2019 and 2021)—have been impeached. Neither Johnson nor Clinton were convicted, and Trump’s first impeachment resulted in an acquittal by the Senate. One president, Richard Nixon, resigned his office in 1974 when it became clear that he would be impeached by the House and likely convicted by the Senate. Nixon was pardoned for his alleged misconduct by his successor, Gerald Ford.\nAfter the glitzy red-carpet arrivals, the feel-good montages, and the host’s opening roast, the Oscars ceremony switches to its raison d’être: revealing the previous year’s highest achievers in cinema. One after another, the presenters list the nominees, open a lavish envelope, and reveal the winner in each category. The champions breathlessly accept their awards and, over and over, effusively thank their mothers, God, and the academy. The academy has so much gratitude bestowed on it throughout the ceremony that many of us watching at home may start to wonder: What is this obscure body? The answer is less enthralling than one might think. The academy—that is, the Academy of Motion Picture Arts and Sciences—is the organization that votes for the Oscar winners. Perhaps what’s more interesting is figuring out who the members of the academy are and how they vote.\nThe academy is an exclusive Hollywood institution that has its own governing body (aptly named the Board of Governors), 17 separate branches, and a thorough rule book on membership eligibility and voting processes. Since 2016, when the board announced that it would diversify its membership, the academy has grown to about 8,000 members. It doesn’t publicize the names of all those members, but each spring it releases a list of the individuals it has invited to join its ranks. Invitees have included Mindy Kaling, Rashida Jones, Kendrick Lamar, Melissa Etheridge, and J.K. Rowling. The academy also posts the names of members of its current Board of Governors, which includes elected representatives of the 17 branches. These branches represent the various fields of cinema: acting, directing, writing, sound editing, and others. In 2019 the academy’s Website showed that the Board of Governors’ president was cinematographer John Bailey, its first vice president was makeup artist Lois Burwell, and its other officers included actors Laura Dern, Whoopi Goldberg, and Alfred Molina.\nThe rest of the academy members are not listed, but we can guess who a few are by looking at some of the requirements to join the institution. To qualify, an individual must work in the film industry. This means that neither individuals who work exclusively in television nor members of the press may join. Oscar nominees are often considered for membership automatically, while other candidates must be sponsored by two active members of the branch they wish to join. Each branch also has its own specific requirements. Directors, for example, must have a minimum of two directing credits, at least one of them within the past 10 years. So we can be pretty sure that such Hollywood treasures as Meryl Streep, Jack Nicholson, Steven Spielberg, and Tom Hanks, who have each been nominated several times and have won Oscars, are members of the academy. New members may choose only one of the branches to join. This means that Sofia Coppola and Alfonso Cuarón, for example, who were each nominated for and won Oscars, could sign on to either the directing branch or the writing branch but not both.\nWhile the membership of the academy is largely obscure, the voting process is perhaps only slightly clearer. It involves two phases: first, nominating the Oscar candidates and, second, voting for the winners. In the first phase, members receive a ballot that lists qualifying movies. To be considered for nomination, a movie must be feature-length and must have been publicly screened for paid admission for at least one week at a commercial theater in Los Angeles county between January 1 and December 31 of the award year. Documentaries and foreign films have their own eligibility requirements. Members may nominate only for awards within their branch and for best picture. Emma Stone may thus suggest nominees for best actress, actor, supporting actress, and supporting actor, but she may not nominate candidates in the best sound editing or best sound mixing categories. Each member of the academy picks up to five candidates for each of their designated categories and lists them by preference.\nTo determine the nominees in each category, the ballots are tallied by certified public accountants from a firm designated by the academy’s president in a somewhat arcane system that might seem like a sacred ritual to an outsider. To ensure that nominees have broad, rather than just popular, support, the academy uses instant runoff voting, sometimes called preferential voting, which involves several rounds and a “magic” number, wherein a candidate must receive a predetermined number of votes to be considered a nominee. A few weeks after the nominees are announced in January, the second phase of voting begins. For the final voting, all active or lifetime academy members are allowed to cast ballots in any category, but they are discouraged from voting in categories where they lack expertise. Accountants once again tally the ballots, using the preferential system to determine the winner for best picture but using the popular vote for all other categories.\nAfter all the voting and tallying, the winners are finally determined, but they are not reported to anyone. Only two accountants see the final results, and they are responsible for keeping those results secret until the awards ceremony. The accountants memorize the names of the winners, stuff two sets of envelopes, and pack and store two briefcases at an undisclosed location until the day of the ceremony. At the ceremony, neither the members of the academy nor the producers of the awards show know who will receive an Oscar. It is a complete mystery until the presenter utters one of the most famous lines in Hollywood: “And the Oscar goes to....”",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/quiz/browse",
    "content": "Guess the U.S. State by Its Neighbors\nSee how well you know your boundaries.\nWho Did That? A Historical Bio Quiz\nHow well do you know the histories of history’s biggest names?",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/place/Washington-Pennsylvania",
    "content": "Washington, city, seat (1781) of Washington county, southwestern Pennsylvania, U.S. It lies 28 miles (45 km) southwest of Pittsburgh.\nPrior to the American Revolution the area was the centre of a land dispute with Virginia. Pennsylvania’s claim was finally validated by the Virginia constitution of 1776. Laid out by David Hoge in 1781, Washington was early known as Catfish’s Camp for a Delaware Indian chief who lived there about 1750. It was known as Bassett-town for a short time until renamed for General George Washington. It was a hotbed of activity during the Whiskey Rebellion (an uprising against an excise tax on distilled liquor) of 1794 and was organized as a borough in 1810. The first crematory in the United States was built in Washington in 1876 by Francis Julius Le Moyne, who had to contend with an aroused public opinion, which forced the construction of the building at night. Washington was chartered as a city in 1924.\nThe city is a service point for an agricultural, light industrial, and coal-mining area. Washington and Jefferson College was formed in 1865 by the merger of Washington Academy (1781) and Jefferson College (1802). The Pennsylvania Trolley Museum offers rides on vintage streetcars. Pop. (2000) 15,268; (2010) 13,663.\nOhio River, major river artery of the east-central United States. Formed by the confluence of the Allegheny and Monongahela rivers at Pittsburgh, it flows northwest out of Pennsylvania, then in a general southwesterly direction to join the Mississippi River at Cairo, Illinois (see photograph), after a course of 981 miles (1,579 km). It marks several state boundaries: the Ohio–West Virginia, Ohio–Kentucky, Indiana–Kentucky, and Illinois–Kentucky. The Ohio River contributes more water to the Mississippi than does any other tributary and drains an area of 203,900 square miles (528,100 square km). The river’s valley is narrow, with an average width of less than 0.5 mile (0.8 km) between Pittsburgh and Wheeling (West Virginia), a little more than 1 mile (1.6 km) from Cincinnati (Ohio) to Louisville (Kentucky) and somewhat greater below Louisville.\nThe Ohio is navigable, and, despite seasonal fluctuations that occasionally reach flood proportions, its fairly uniform flow has supported important commerce since settlement first began. Following destructive floods at Johnstown, Pennsylvania, in 1889 and Portsmouth, Ohio, in 1937, the federal government built a series of flood-control dams. While not developed for hydropower in Ohio, the river, kept at a navigable depth of 9 feet (3 metres), carries cargoes of coal, oil, steel, and manufactured articles. It has a total fall of only 429 feet (130 metres), the one major hazard to navigation being the Falls of the Ohio at Louisville, where locks control a descent of about 24 feet (7 metres) within a distance of 2.5 miles (4 km).\nThe Ohio’s tributaries include the Tennessee, Cumberland, Kanawha, Big Sandy, Licking, Kentucky, and Green rivers from the south and the Muskingum, Miami, Wabash, and Scioto rivers from the north. Chief cities along the river, in addition to Pittsburgh, Cairo, Wheeling, and Louisville, are Steubenville, Marietta, Gallipolis, Portsmouth, and Cincinnati in Ohio; Madison, New Albany, Evansville, and Mount Vernon in Indiana; Parkersburg and Huntington in West Virginia; and Ashland, Covington, Owensboro, and Paducah in Kentucky.\nRené-Robert Cavelier, Sieur de La Salle, is said to have been the first European to see the Ohio, in 1669, and he descended it until obstructed by a waterfall (presumably the Falls at Louisville). In the 1750s the river’s strategic importance (especially the fork at Pittsburgh) in the struggle between the French and the English for possession of the interior of the continent became fully recognized. By the treaty of 1763 ending the French and Indian Wars, the English finally gained undisputed control of the territory along its banks. When (by an ordinance of 1787) the area was opened to settlement, most of the settlers entered the region down the headwaters of the Ohio.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/place/Ohio-River",
    "content": "Ohio River, major river artery of the east-central United States. Formed by the confluence of the Allegheny and Monongahela rivers at Pittsburgh, it flows northwest out of Pennsylvania, then in a general southwesterly direction to join the Mississippi River at Cairo, Illinois (see photograph), after a course of 981 miles (1,579 km). It marks several state boundaries: the Ohio–West Virginia, Ohio–Kentucky, Indiana–Kentucky, and Illinois–Kentucky. The Ohio River contributes more water to the Mississippi than does any other tributary and drains an area of 203,900 square miles (528,100 square km). The river’s valley is narrow, with an average width of less than 0.5 mile (0.8 km) between Pittsburgh and Wheeling (West Virginia), a little more than 1 mile (1.6 km) from Cincinnati (Ohio) to Louisville (Kentucky) and somewhat greater below Louisville.\nThe Ohio is navigable, and, despite seasonal fluctuations that occasionally reach flood proportions, its fairly uniform flow has supported important commerce since settlement first began. Following destructive floods at Johnstown, Pennsylvania, in 1889 and Portsmouth, Ohio, in 1937, the federal government built a series of flood-control dams. While not developed for hydropower in Ohio, the river, kept at a navigable depth of 9 feet (3 metres), carries cargoes of coal, oil, steel, and manufactured articles. It has a total fall of only 429 feet (130 metres), the one major hazard to navigation being the Falls of the Ohio at Louisville, where locks control a descent of about 24 feet (7 metres) within a distance of 2.5 miles (4 km).\nThe Ohio’s tributaries include the Tennessee, Cumberland, Kanawha, Big Sandy, Licking, Kentucky, and Green rivers from the south and the Muskingum, Miami, Wabash, and Scioto rivers from the north. Chief cities along the river, in addition to Pittsburgh, Cairo, Wheeling, and Louisville, are Steubenville, Marietta, Gallipolis, Portsmouth, and Cincinnati in Ohio; Madison, New Albany, Evansville, and Mount Vernon in Indiana; Parkersburg and Huntington in West Virginia; and Ashland, Covington, Owensboro, and Paducah in Kentucky.\nRené-Robert Cavelier, Sieur de La Salle, is said to have been the first European to see the Ohio, in 1669, and he descended it until obstructed by a waterfall (presumably the Falls at Louisville). In the 1750s the river’s strategic importance (especially the fork at Pittsburgh) in the struggle between the French and the English for possession of the interior of the continent became fully recognized. By the treaty of 1763 ending the French and Indian Wars, the English finally gained undisputed control of the territory along its banks. When (by an ordinance of 1787) the area was opened to settlement, most of the settlers entered the region down the headwaters of the Ohio.\nMississippi River, the longest river of North America, draining with its major tributaries an area of approximately 1.2 million square miles (3.1 million square km), or about one-eighth of the entire continent. The Mississippi River lies entirely within the United States. Rising in Lake Itasca in Minnesota, it flows almost due south across the continental interior, collecting the waters of its major tributaries, the Missouri River (to the west) and the Ohio River (to the east), approximately halfway along its journey to the Gulf of Mexico through a vast delta southeast of New Orleans, a total distance of 2,340 miles (3,766 km) from its source. With its tributaries, the Mississippi drains all or part of 31 U.S. states and two provinces in Canada.\nAlthough the Mississippi can be ranked as the fourth longest river in the world by adding the length of the Missouri-Jefferson (Red Rock) system to the Mississippi downstream of the Missouri-Mississippi confluence—for a combined length of 3,710 miles (5,971 km)—the 2,340-mile length of the Mississippi proper is comfortably exceeded by 19 other rivers. In volume of discharge, however, the Mississippi’s rate of roughly 600,000 cubic feet (17,000 cubic metres) per second is the largest in North America and the eighth greatest in the world.\nAs the central river artery of a highly industrialized nation, the Mississippi River has become one of the busiest commercial waterways in the world, and, as the unruly neighbour of some of the continent’s richest farmland, it has been subjected to a remarkable degree of human control and modification. Furthermore, the river’s unique contribution to the history and literature of the United States has woven it like a bright thread through the folklore and national consciousness of North America, linking the names of two U.S. presidents—Abraham Lincoln and Ulysses S. Grant—with that of the celebrated author Mark Twain.\nOn the basis of physical characteristics, the Mississippi River can be divided into four distinct reaches, or sections. In its headwaters, from the source to the head of navigation at St. Paul, Minnesota, the Mississippi is a clear, fresh stream winding its unassuming way through low countryside dotted with lakes and marshes. The upper Mississippi reach extends from St. Paul to the mouth of the Missouri River near St. Louis, Missouri. Flowing past steep limestone bluffs and receiving water from tributaries in Minnesota, Wisconsin, Illinois, and Iowa, the river in this segment assumes the character that led Algonquian-speaking Indians to name it the “Father of Waters” (literally misi, “big”; sipi, “water”). Below the Missouri River junction, the middle Mississippi follows a 200-mile (320-km) course to the mouth of the Ohio River. The turbulent, cloudy-to-muddy, and flotsam-laden Missouri, especially when in flood, adds impetus as well as enormous quantities of silt to the clearer Mississippi. Beyond the confluence with the Ohio at Cairo, Illinois, the lower Mississippi attains its full grandeur. Where these two mighty rivers meet, the Ohio is actually the larger; thus, below the Ohio confluence the Mississippi swells to more than twice the size it is above. Often 1.5 miles (2.4 km) from bank to bank, the lower Mississippi becomes a brown, lazy river, descending with deceptive quiet toward the Gulf of Mexico.\nTo geographers, the lower Mississippi has long been a classic example of a meandering alluvial river; that is, the channel loops and curls extravagantly along its floodplain, leaving behind meander scars, cutoffs, oxbow lakes, and swampy backwaters. More poetically, Twain compared its shape to “a long, pliant apple-paring.” Today the sunlight glittering on the twisted ribbon of water remains one of the most distinctive landmarks of a transcontinental flight. Now curbed largely by an elaborate system of embankments (levees), dams, and spillways, this lower section of the Mississippi was the golden, sometimes treacherous, highway for the renowned Mississippi steamboats, those “palaces on paddle wheels” that so fired the public imagination.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/money/service-industry",
    "content": "service industry, an industry in that part of the economy that creates services rather than tangible objects. Economists divide all economic activity into two broad categories, goods and services. Goods-producing industries are agriculture, mining, manufacturing, and construction; each of them creates some kind of tangible object. Service industries include everything else: banking, communications, wholesale and retail trade, all professional services such as engineering, computer software development, and medicine, nonprofit economic activity, all consumer services, and all government services, including defense and administration of justice. A services-dominated economy is characteristic of developed countries. In less-developed countries most people are employed in primary activities such as agriculture and mining.\nThe proportion of the world economy devoted to services grew steadily during the 20th century. In the United States, for example, the service sector accounted for more than half the gross domestic product (GDP) in 1929, two-thirds in 1978, and more than three-quarters in 1993. In the early 21st century, service industries accounted for more than three-fifths of the global GDP and employed more than one-third of the labour force worldwide.\nThe simplest explanation for the growth of service industries is that goods production has become increasingly mechanized. Because machines allow a smaller workforce to produce more tangible goods, the service functions of distribution, management, finance, and sales become relatively more important. Growth in the service sector also results from a large increase in government employment.\nEach month, two reports aim to measure the “mood” of American households regarding both their own finances and the economy at large. Because consumer mood is closely associated with spending habits, tracking consumer confidence and consumer sentiment data can help investors better understand the economic trends driving the markets.\nWhy measure a nebulous thing like “confidence” or “sentiment” in the first place? Think about it this way. When your checking account is flush because you make more than you spend, and your portfolio is on the upswing, you might feel confident you can afford an extra restaurant meal or two a month. Or maybe you’ll decide you finally have enough spare cash to build that addition to your home or drive off in a new set of wheels.\nAlthough confidence and sentiment data don’t measure real actions taken to directly affect the economy—like when you make a down payment at the auto dealer—they do provide regular psychological assessments of thousands of consumers around the country to see how people feel about the current condition of their finances. The reports’ market significance lies in their power to forecast consumer spending, which drives about 70% of U.S. gross domestic product (GDP). The quarterly GDP report is the ultimate measure of economic health, and can have a major impact on the stock and bond markets.\nThe Consumer Confidence Index (CCI) is a monthly report that measures the financial and economic optimism of American households. Published by the Conference Board, a nonprofit economic research institution, it surveys around 5,000 households across all nine census regions in the U.S., all varying in age and income.\nThis index provides a glimpse into the psyche of U.S. consumers, deciphering whether they’re feeling positive, negative, or neutral about their own finances and the general state of the economy (business and employment conditions) in the next six months. It also surveys people about their inflation expectations, which is important for reasons we’ll discuss below.\nThe CCI happens to be the larger of the two confidence reports. It’s published at 10 a.m. ET on the last Tuesday of every month. You can find the latest data and a historic monthly chart at the Conference Board site.\nThe University of Michigan Consumer Sentiment Index (aka “Michigan Sentiment”) also aims to measure the health of the economy from a consumer perspective. Each month, the University of Michigan conducts telephone interviews (at least 500) to gather people’s opinions on their personal finances, the business climate, inflation expectations, employment conditions, and (important here) spending.\nYou can look at the survey by going to the University of Michigan’s survey site and clicking on Questionnaire. A preliminary report comes out early each month, followed by a final report later in the month.\nSimilar to the Consumer Confidence Index, Michigan Sentiment paints a picture of consumers’ attitudes toward their current financial well-being and their future economic expectations. (See figure 1.)\nThese two reports may differ in size and source. But their goal is more or less the same: to determine whether consumers are feeling optimistic or pessimistic about their own economic prospects.\nAnd why does that matter?\nSo, it’s really all about spending, which shouldn’t be a surprise. Consumer spending plays a critical role in the supply and demand equation. It drives employment, production, business profits, and business investments—in short, it drives the economy at large.\nThe key data in each report is the so-called “headline” monthly confidence or sentiment number that reflects the researchers’ full set of measurements. For instance, the Consumer Confidence report’s headline number swung between the low 20s to as high as nearly 140 in the years between 2006 and 2022. Not surprisingly, the lowest levels occurred during the “Great Recession” of 2008–09. Confidence climbed without much pause from late 2009 until the COVID-19 pandemic arrived in 2020, when it sank to five-year lows before a slight rebound.\nRecessions, stock market weakness, geopolitical events, and even pandemics (as we now know) can affect consumer confidence. By comparing the current month’s figure with past performance, you can get a sense of where it lies on the continuum of consumer psychology. It’s also helpful to watch trends. Has the headline figure fallen or risen over the last few months? Trends can give you insight into how consumer feelings develop over time.\nDon’t forget to look beyond the headlines. Both reports provide helpful hints about what consumers expect in the months and years ahead. When inflation soared in 2021 and 2022, the reports’ inflation projections grew in significance.\nOne fear economists have is that inflation expectations can become “entrenched,” meaning consumers expect prices to keep rising sharply for many years ahead. This can lead to people demanding higher wages, forcing businesses to pay them more and then to raise prices so they can afford the higher salaries. This is known as a wage-price spiral, and it’s something the U.S. Federal Reserve tries hard to avoid. The Confidence and Sentiment reports can sometimes offer an early warning.\nThe Michigan Sentiment report is also interesting for its breakdown of sentiment among different people across the economy. For instance, it often compares sentiment of Republicans versus Democrats versus independents. Each sentiment report is accompanied by a statement from the survey’s chief economist, which is worth reading for its general insights about how consumers feel.\nConfidence measures provide key perspectives on the economy. They offer unique insights into the current state of the economy “as experienced,” and they indicate the potential for future spending. Additionally, they provide insight into consumers’ expectations for inflation.\nThere’s a lag time between these reports and their potential effects. They’re viewed as “leading indicators” that can help predict the near- to intermediate-term future of the economy. That’s why business leaders and investors pay close attention to these measures as they try to read the economy’s future course.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/technology/manufacturing",
    "content": "manufacturing, any industry that makes products from raw materials by the use of manual labour or machinery and that is usually carried out systematically with a division of labour. (See industry.) In a more limited sense, manufacturing denotes the fabrication or assembly of components into finished products on a fairly large scale. Among the most important manufacturing industries are those that produce aircraft, automobiles, chemicals, clothing, computers, consumer electronics, electrical equipment, furniture, heavy machinery, refined petroleum products, ships, steel, and tools and dies.\nManufacturing is treated in a number of articles. For treatment of major manufacturing industries, see automotive industry; aerospace industry; ship construction; clothing and footwear industry; floor coverings; furniture industry; chemical industry; soap and detergent; dye; pharmaceutical industry; explosive; elastomer; plastic; man-made fibre; surface coating; adhesive; papermaking; building construction; electronics; food preservation; industrial ceramics; industrial glass; industrial polymers, major; mineral deposit; textile; printing. For treatment of manufacturing methods, processes, and organization, see automation; production system; industrial relations. The utilization of energy in manufacturing is treated in energy conversion. For the application of measurement and control in industrial processes, see analysis; drafting. See also engineering; technology, history of.\nautomation, application of machines to tasks once performed by human beings or, increasingly, to tasks that would otherwise be impossible. Although the term mechanization is often used to refer to the simple replacement of human labour by machines, automation generally implies the integration of machines into a self-governing system. Automation has revolutionized those areas in which it has been introduced, and there is scarcely an aspect of modern life that has been unaffected by it.\nThe term automation was coined in the automobile industry about 1946 to describe the increased use of automatic devices and controls in mechanized production lines. The origin of the word is attributed to D.S. Harder, an engineering manager at the Ford Motor Company at the time. The term is used widely in a manufacturing context, but it is also applied outside manufacturing in connection with a variety of systems in which there is a significant substitution of mechanical, electrical, or computerized action for human effort and intelligence.\nIn general usage, automation can be defined as a technology concerned with performing a process by means of programmed commands combined with automatic feedback control to ensure proper execution of the instructions. The resulting system is capable of operating without human intervention. The development of this technology has become increasingly dependent on the use of computers and computer-related technologies. Consequently, automated systems have become increasingly sophisticated and complex. Advanced systems represent a level of capability and performance that surpass in many ways the abilities of humans to accomplish the same activities.\nAutomation technology has matured to a point where a number of other technologies have developed from it and have achieved a recognition and status of their own. Robotics is one of these technologies; it is a specialized branch of automation in which the automated machine possesses certain anthropomorphic, or humanlike, characteristics. The most typical humanlike characteristic of a modern industrial robot is its powered mechanical arm. The robot’s arm can be programmed to move through a sequence of motions to perform useful tasks, such as loading and unloading parts at a production machine or making a sequence of spot-welds on the sheet-metal parts of an automobile body during assembly. As these examples suggest, industrial robots are typically used to replace human workers in factory operations.\nThis article covers the fundamentals of automation, including its historical development, principles and theory of operation, applications in manufacturing and in some of the services and industries important in daily life, and impact on the individual as well as society in general. The article also reviews the development and technology of robotics as a significant topic within automation. For related topics, see computer science and information processing.\nThe technology of automation has evolved from the related field of mechanization, which had its beginnings in the Industrial Revolution. Mechanization refers to the replacement of human (or animal) power with mechanical power of some form. The driving force behind mechanization has been humankind’s propensity to create tools and mechanical devices. Some of the important historical developments in mechanization and automation leading to modern automated systems are described here.\nThe first tools made of stone represented prehistoric man’s attempts to direct his own physical strength under the control of human intelligence. Thousands of years were undoubtedly required for the development of simple mechanical devices and machines such as the wheel, the lever, and the pulley, by which the power of human muscle could be magnified. The next extension was the development of powered machines that did not require human strength to operate. Examples of these machines include waterwheels, windmills, and simple steam-driven devices. More than 2,000 years ago the Chinese developed trip-hammers powered by flowing water and waterwheels. The early Greeks experimented with simple reaction motors powered by steam. The mechanical clock, representing a rather complex assembly with its own built-in power source (a weight), was developed about 1335 in Europe. Windmills, with mechanisms for automatically turning the sails, were developed during the Middle Ages in Europe and the Middle East. The steam engine represented a major advance in the development of powered machines and marked the beginning of the Industrial Revolution. During the two centuries since the introduction of the Watt steam engine, powered engines and machines have been devised that obtain their energy from steam, electricity, and chemical, mechanical, and nuclear sources.\nEach new development in the history of powered machines has brought with it an increased requirement for control devices to harness the power of the machine. The earliest steam engines required a person to open and close the valves, first to admit steam into the piston chamber and then to exhaust it. Later a slide valve mechanism was devised to automatically accomplish these functions. The only need of the human operator was then to regulate the amount of steam that controlled the engine’s speed and power. This requirement for human attention in the operation of the steam engine was eliminated by the flying-ball governor. Invented by James Watt in England, this device consisted of a weighted ball on a hinged arm, mechanically coupled to the output shaft of the engine. As the rotational speed of the shaft increased, centrifugal force caused the weighted ball to be moved outward. This motion controlled a valve that reduced the steam being fed to the engine, thus slowing the engine. The flying-ball governor remains an elegant early example of a negative feedback control system, in which the increasing output of the system is used to decrease the activity of the system.\nNegative feedback is widely used as a means of automatic control to achieve a constant operating level for a system. A common example of a feedback control system is the thermostat used in modern buildings to control room temperature. In this device, a decrease in room temperature causes an electrical switch to close, thus turning on the heating unit. As room temperature rises, the switch opens and the heat supply is turned off. The thermostat can be set to turn on the heating unit at any particular set point.\nAnother important development in the history of automation was the Jacquard loom (see photograph ), which demonstrated the concept of a programmable machine. About 1801 the French inventor Joseph-Marie Jacquard devised an automatic loom capable of producing complex patterns in textiles by controlling the motions of many shuttles of different coloured threads. The selection of the different patterns was determined by a program contained in steel cards in which holes were punched. These cards were the ancestors of the paper cards and tapes that control modern automatic machines. The concept of programming a machine was further developed later in the 19th century when Charles Babbage, an English mathematician, proposed a complex, mechanical “analytical engine” that could perform arithmetic and data processing. Although Babbage was never able to complete it, this device was the precursor of the modern digital computer. See computers.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/biography/Andy-Warhol",
    "content": "Andy Warhol (born August 6, 1928, Pittsburgh, Pennsylvania, U.S.—died February 22, 1987, New York, New York) was an American artist and filmmaker, an initiator and leading exponent of the Pop art movement of the 1960s whose mass-produced art apotheosized the supposed banality of the commercial culture of the United States. An adroit self-publicist, he projected a concept of the artist as an impersonal, even vacuous, figure who is nevertheless a successful celebrity, businessman, and social climber.\nThe son of Ruthenian (Rusyn) immigrants from what is now eastern Slovakia, Warhol graduated in 1949 from the Carnegie Institute of Technology (now Carnegie Mellon University), Pittsburgh, with a degree in pictorial design. He then went to New York City, where he worked as a commercial illustrator for about a decade.\nWarhol began painting in the late 1950s and received sudden notoriety in 1962, when he exhibited paintings of Campbell’s soup cans, Coca-Cola bottles, and wooden replicas of Brillo soap pad boxes. By 1963 he was mass-producing these purposely banal images of consumer goods by means of photographic silkscreen prints, and he then began printing endless variations of portraits of celebrities in garish colours. The silkscreen technique was ideally suited to Warhol, for the repeated image was reduced to an insipid and dehumanized cultural icon that reflected both the supposed emptiness of American material culture, awash in consumerism and brand marketing, and the artist’s emotional noninvolvement with the practice of his art. Warhol’s work placed him in the forefront of the emerging Pop art movement in America.\nAs the 1960s progressed, Warhol devoted more of his energy to filmmaking. Usually classed as underground films, such motion pictures of his as Chelsea Girls (1966), Eat (1963), My Hustler (1965), and Blue Movie (1969) are known for their inventive eroticism, plotless boredom, and inordinate length (up to 25 hours). Other movies include Poor Little Rich Girl (1965) and Lupe (1966), both of which featured Edie Sedgwick.\nIn 1968 Warhol was shot and nearly killed by Valerie Solanas, one of an assemblage of underground film and rock music stars, assorted hangers-on, and social curiosities who frequented his studio, known as the Factory. (The incident is depicted in the 1996 film I Shot Andy Warhol.) Warhol had by this time become a well-known fixture on the fashion and avant-garde art scene and was an influential celebrity in his own right. Throughout the 1970s and until his death, he continued to produce prints depicting political and Hollywood celebrities, notably Marilyn Monroe. He also involved himself in a wide range of advertising illustrations and other commercial art projects. His The Philosophy of Andy Warhol (1975) was followed by Portraits of the Seventies and Andy Warhol’s Exposures (both 1979).\nWarhol’s work is featured in the Andy Warhol Museum in Pittsburgh. In his will, the artist dictated that his entire estate be used to create a foundation for “the advancement of the visual arts.” The Andy Warhol Foundation for the Visual Arts was established in 1987.\nsilkscreen, sophisticated stenciling technique for surface printing, in which a design is cut out of paper or another thin, strong material and then printed by rubbing, rolling, or spraying paint or ink through the cut out areas. It was developed about 1900 and originally used in advertising and display work. In the 1950s fine artists began to use the process. Its name came from the fine-mesh silk that, when tacked to a wooden frame, serves as a support for the cut-paper stencil, which is glued to it. To make a silkscreen print, the wooden frame holding the screen is hinged to a slightly larger wooden board, the printing paper is placed on the board under the screen, and the paint is pressed through the screen with a squeegee (rubber blade) the same width as the screen. Many colours can be used, with a separate screen for each colour.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/place/Pittsburgh#ref242000",
    "content": "Pittsburgh, city, seat (1788) of Allegheny county, southwestern Pennsylvania, U.S. The city is located at the confluence of the Allegheny and Monongahela rivers, which unite at the point of the “Golden Triangle” (the business district) to form the Ohio River. A city of hills, parks, and valleys, it is the centre of an urban industrial complex that includes the surrounding cities of Aliquippa (northwest), New Kensington (northeast), McKeesport (southeast), and Washington (southwest) and the borough of Wilkinsburg (east). Inc. borough, 1794; city, 1816. Area city, 58 square miles (150 square km). Pop. (2010) 305,704; Pittsburgh Metro Area, 2,356,285; (2020) 302,971; Pittsburgh Metro Area, 2,370,930.\nAlgonquian- and Iroquoian-speaking peoples were early inhabitants of the region. The conflict between the British and French over territorial claims in the area was settled in 1758 when General John Forbes and his British and colonial army expelled the French from Fort Duquesne (built 1754). Forbes named the site for the British statesman William Pitt the Elder. The British built Fort Pitt (1761) to ensure their dominance at the source of the Ohio. Settlers began arriving after Native American forces led by Ottawa chief Pontiac were defeated in 1763; an agreement subsequently was made between Native American groups and the Penn family, and a boundary dispute was ended between Pennsylvania and Virginia. Pittsburgh was laid out (1764) by John Campbell in the area around the fort (now the Golden Triangle). Following the American Revolution, the town became an outfitting point for settlers traveling westward down the Ohio River.\nPittsburgh’s strategic location and wealth of natural resources spurred its commercial and industrial growth in the 19th century. A blast furnace, erected by George Anschutz about 1792, was the forerunner of the iron and steel industry that for more than a century was the city’s economic mainstay; by 1850 Pittsburgh was known as the “Iron City.” The Pennsylvania Canal and the Portage Railroad, both completed in 1834, opened vital markets for trade and shipping. The city suffered a great loss in 1845 when some 24 blocks of businesses, homes, churches, and other buildings were destroyed by fire.\nAfter the American Civil War, great numbers of European immigrants swelled Pittsburgh’s population, and industrial magnates such as Andrew Carnegie, Henry Clay Frick, and Thomas Mellon built their steel empires there. The city became the focus of historic friction between labour and management, and the American Federation of Labor was born there in 1881.\nBy 1900 the city’s population had reached 321,616. Growth continued nearly unabated through World War II, the war years bringing a particularly great boon for the economy. The population crested at more than 675,000 in 1950, after which it steadily declined; by the end of the century, it had returned almost to the 1900 level. Most citizens were still of European ancestry, but the growing African American proportion of the population exceeded one-fourth. During the period of economic and population growth, Pittsburgh had come to epitomize the grimy, polluted industrial city. After the war, however, the city undertook an extensive redevelopment program that emphasized smoke-pollution control, flood prevention, and sewage disposal. In 1957 it became the first American city to generate electricity by nuclear power.\nBy the late 1970s and early ’80s, the steel industry had virtually disappeared—a result of foreign competition and decreased demand. Many of the surrounding mill towns were laid to waste by unemployment, becoming a symbol of the notorious Rust Belt, the U.S. region where steelmaking and manufacturing once thrived before succumbing to widespread unemployment and poverty. Pittsburgh, however, successfully diversified its economy through more emphasis on light industries—though metalworking, chemicals, and plastics remained important—and on such high-technology industries as computer software, industrial automation (robotics), and biomedical and environmental technologies. Numerous industrial research laboratories were established in the area, and the service sector became increasingly important. Pittsburgh long has been one of the nation’s largest inland ports, and it remains a leading transportation centre.\nMuch of the Golden Triangle has been rebuilt and includes Point State Park (containing Fort Pitt Blockhouse and Fort Pitt Museum), the Gateway Center (site of several skyscrapers and a garden), and the David L. Lawrence Convention Center. The University of Pittsburgh was chartered in 1787. Other educational institutions include Carnegie Mellon (1900), Duquesne (1878), Point Park (1960), Chatham (1869), and Carlow (1929) universities and two campuses of the Community College of Allegheny County (1966).\nCentral to the city’s cultural life is the Carnegie Museums of Pittsburgh (formerly Carnegie Institute), an umbrella organization consisting of a number of institutions. Its museums include those for the fine arts and natural history (both founded in 1895), the Carnegie Science Center (1991), which now also houses the Henry Buhl, Jr., Planetarium and Observatory (1939), and the Andy Warhol Museum (1994), which exhibits the works of the Pittsburgh-born artist and filmmaker. Other institutions affiliated with the organization are the Carnegie Library of Pittsburgh, which contains more than 3.3 million volumes, and the Carnegie Music Hall. The Pittsburgh Symphony Orchestra performs at Heinz Hall, a restored movie theatre.\nPhipps Conservatory and Botanical Gardens (1893), which covers 15 acres (6 hectares), is noted for its extensive greenhouses. The city’s zoo, in the northeastern Highland Park neighbourhood, includes an aquarium. Two new sports venues opened in 2001 on the north bank of the Allegheny opposite the Golden Triangle: PNC Park is home of the Pirates, the city’s professional baseball team, and Acrisure Stadium houses the Steelers, its professional football team. The Penguins, Pittsburgh’s professional ice hockey team, plays at PPG Paints Arena. Popular summertime attractions include riverboat excursions on Pittsburgh’s waterways and Kennywood, an amusement park southeast of the city in West Mifflin.\nCarnegie Mellon University, private, coeducational institution of higher learning in Pittsburgh, Pennsylvania, U.S. The university includes the Carnegie Institute of Technology, the College of Humanities and Social Sciences, the College of Fine Arts, the Mellon College of Science, the School of Computer Science, the H. John Heinz III School of Public Policy and Management, and the Graduate School of Industrial Administration. Undergraduate and graduate degree programs are offered in a range of fields. Total enrollment is about 7,700.\nIn 1900 the industrialist Andrew Carnegie gave a gift of $1 million to the city of Pittsburgh for the creation of a technical school. Originally called Carnegie Technical Schools, it was renamed Carnegie Institute of Technology in 1912. The institute merged with the Mellon Institute (established in 1913 in Pittsburgh by financier Andrew W. Mellon) in 1967. The university has built a reputation as a vital arts centre, operating three art galleries, two concert halls, and two theatres. The faculty has included Nobel Prize-winning economists Herbert Alexander Simon and Merton Miller.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/question/Is-a-university-education-worth-it",
    "content": "Whether a university education is worth it is widely debated. Some say college graduates make more money and jobs increasingly require college degrees. Others say student loan debt is crippling for college graduates and forces students to delay adult milestones like marriage. For more on the college worth debate, visit ProCon.org.\nA university is an institution of higher education, usually comprising a college of liberal arts and sciences and graduate and professional schools and having the authority to confer degrees in various fields of study. A university differs from a college in that it is usually larger, has a broader curriculum, and offers graduate and professional degrees in addition to undergraduate degrees. Although universities did not arise in the West until the Middle Ages in Europe, they existed in some parts of Asia and Africa in ancient times.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/place/Allegheny-River",
    "content": "Allegheny River, river rising in the hilly plateau region of Potter county, Pennsylvania, U.S., and flowing generally northward for about 80 miles (130 km). The river enters New York state where the Allegheny Reservoir is impounded at Allegany State Park. Turning southwest, it continues for 120 miles (190 km), meandering to the southeast and again southwest and eventually joining the Monongahela River at Pittsburgh to form the Ohio River. In its total length (321 miles [516 km]), it drains an area of 11,700 square miles (30,300 square km). Its chief tributaries are the Kiskiminetas, Clarion, and Conemaugh rivers and Red Bank, Oil, and French creeks.\nThe Allegheny was important for keelboat navigation before the beginning of railway competition in the mid-19th century. Several dams were built (1903–38) to make the river navigable from Pittsburgh to East Brady. Flood-control dams have been built on many of its major tributaries.\nOhio River, major river artery of the east-central United States. Formed by the confluence of the Allegheny and Monongahela rivers at Pittsburgh, it flows northwest out of Pennsylvania, then in a general southwesterly direction to join the Mississippi River at Cairo, Illinois (see photograph), after a course of 981 miles (1,579 km). It marks several state boundaries: the Ohio–West Virginia, Ohio–Kentucky, Indiana–Kentucky, and Illinois–Kentucky. The Ohio River contributes more water to the Mississippi than does any other tributary and drains an area of 203,900 square miles (528,100 square km). The river’s valley is narrow, with an average width of less than 0.5 mile (0.8 km) between Pittsburgh and Wheeling (West Virginia), a little more than 1 mile (1.6 km) from Cincinnati (Ohio) to Louisville (Kentucky) and somewhat greater below Louisville.\nThe Ohio is navigable, and, despite seasonal fluctuations that occasionally reach flood proportions, its fairly uniform flow has supported important commerce since settlement first began. Following destructive floods at Johnstown, Pennsylvania, in 1889 and Portsmouth, Ohio, in 1937, the federal government built a series of flood-control dams. While not developed for hydropower in Ohio, the river, kept at a navigable depth of 9 feet (3 metres), carries cargoes of coal, oil, steel, and manufactured articles. It has a total fall of only 429 feet (130 metres), the one major hazard to navigation being the Falls of the Ohio at Louisville, where locks control a descent of about 24 feet (7 metres) within a distance of 2.5 miles (4 km).\nThe Ohio’s tributaries include the Tennessee, Cumberland, Kanawha, Big Sandy, Licking, Kentucky, and Green rivers from the south and the Muskingum, Miami, Wabash, and Scioto rivers from the north. Chief cities along the river, in addition to Pittsburgh, Cairo, Wheeling, and Louisville, are Steubenville, Marietta, Gallipolis, Portsmouth, and Cincinnati in Ohio; Madison, New Albany, Evansville, and Mount Vernon in Indiana; Parkersburg and Huntington in West Virginia; and Ashland, Covington, Owensboro, and Paducah in Kentucky.\nRené-Robert Cavelier, Sieur de La Salle, is said to have been the first European to see the Ohio, in 1669, and he descended it until obstructed by a waterfall (presumably the Falls at Louisville). In the 1750s the river’s strategic importance (especially the fork at Pittsburgh) in the struggle between the French and the English for possession of the interior of the continent became fully recognized. By the treaty of 1763 ending the French and Indian Wars, the English finally gained undisputed control of the territory along its banks. When (by an ordinance of 1787) the area was opened to settlement, most of the settlers entered the region down the headwaters of the Ohio.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/event/American-Civil-War",
    "content": "American Civil War, four-year war (1861–65) between the United States and 11 Southern states that seceded from the Union and formed the Confederate States of America.\nThe secession of the Southern states (in chronological order, South Carolina, Mississippi, Florida, Alabama, Georgia, Louisiana, Texas, Virginia, Arkansas, Tennessee, and North Carolina) in 1860–61 and the ensuing outbreak of armed hostilities were the culmination of decades of growing sectional friction over slavery. Between 1815 and 1861 the economy of the Northern states was rapidly modernizing and diversifying. Although agriculture—mostly smaller farms that relied on free labour—remained the dominant sector in the North, industrialization had taken root there. Moreover, Northerners had invested heavily in an expansive and varied transportation system that included canals, roads, steamboats, and railroads; in financial industries such as banking and insurance; and in a large communications network that featured inexpensive, widely available newspapers, magazines, and books, along with the telegraph.\nBy contrast, the Southern economy was based principally on large farms (plantations) that produced commercial crops such as cotton and that relied on slaves as the main labour force. Rather than invest in factories or railroads as Northerners had done, Southerners invested their money in slaves—even more than in land; by 1860, 84 percent of the capital invested in manufacturing was invested in the free (nonslaveholding) states. Yet, to Southerners, as late as 1860, this appeared to be a sound business decision. The price of cotton, the South’s defining crop, had skyrocketed in the 1850s, and the value of slaves—who were, after all, property—rose commensurately. By 1860 the per capita wealth of Southern whites was twice that of Northerners, and three-fifths of the wealthiest individuals in the country were Southerners.\nThe extension of slavery into new territories and states had been an issue as far back as the Northwest Ordinance of 1784. When the slave territory of Missouri sought statehood in 1818, Congress debated for two years before arriving upon the Missouri Compromise of 1820. This was the first of a series of political deals that resulted from arguments between pro-slavery and antislavery forces over the expansion of the “peculiar institution,” as it was known, into the West. The end of the Mexican-American War in 1848 and the roughly 500,000 square miles (1.3 million square km) of new territory that the United States gained as a result of it added a new sense of urgency to the dispute. More and more Northerners, driven by a sense of morality or an interest in protecting free labour, came to believe, in the 1850s, that bondage needed to be eradicated. White Southerners feared that limiting the expansion of slavery would consign the institution to certain death. Over the course of the decade, the two sides became increasingly polarized and politicians less able to contain the dispute through compromise. When Abraham Lincoln, the candidate of the explicitly antislavery Republican Party, won the 1860 presidential election, seven Southern states (South Carolina, Mississippi, Florida, Alabama, Georgia, Louisiana, and Texas) carried out their threat and seceded, organizing as the Confederate States of America.\nIn the early morning hours of April 12, 1861, rebels opened fire on Fort Sumter, at the entrance to the harbour of Charleston, South Carolina. Curiously, this first encounter of what would be the bloodiest war in the history of the United States claimed no victims. After a 34-hour bombardment, Maj. Robert Anderson surrendered his command of about 85 soldiers to some 5,500 besieging Confederate troops under P.G.T. Beauregard. Within weeks, four more Southern states (Virginia, Arkansas, Tennessee, and North Carolina) left the Union to join the Confederacy.\nWith war upon the land, President Lincoln called for 75,000 militiamen to serve for three months. He proclaimed a naval blockade of the Confederate states, although he insisted that they did not legally constitute a sovereign country but were instead states in rebellion. He also directed the secretary of the treasury to advance $2 million to assist in the raising of troops, and he suspended the writ of habeas corpus, first along the East Coast and ultimately throughout the country. The Confederate government had previously authorized a call for 100,000 soldiers for at least six months’ service, and this figure was soon increased to 400,000.\nAt first glance it seemed that the 23 states that remained in the Union after secession were more than a match for the 11 Southern states. Approximately 21 million people lived in the North, compared with some nine million in the South of whom about four million were slaves. In addition, the North was the site of more than 100,000 manufacturing plants, against 18,000 south of the Potomac River, and more than 70 percent of the railroads were in the Union. Furthermore, the Federals had at their command a 30-to-1 superiority in arms production, a 2-to-1 edge in available manpower, and a great preponderance of commercial and financial resources. The Union also had a functioning government and a small but efficient regular army and navy.\nThe Confederacy was not predestined to defeat, however. The Southern armies had the advantage of fighting on interior lines, and their military tradition had bulked large in the history of the United States before 1860. Moreover, the long Confederate coastline of 3,500 miles (5,600 km) seemed to defy blockade, and the Confederate president, Jefferson Davis, hoped to receive decisive foreign aid and intervention. Confederate soldiers were fighting to achieve a separate and independent country based on what they called “Southern institutions,” the chief of which was the institution of slavery. So the Southern cause was not a lost one; indeed, other countries—most notably the United States itself in the American Revolution against Britain—had won independence against equally heavy odds.\nCommand problems plagued both sides. Of the two rival commanders in chief, most people in 1861 thought Davis to be abler than Lincoln. Davis was a graduate of the U.S. Military Academy, a hero of the Mexican-American War, a capable secretary of war under Pres. Franklin Pierce, and a U.S. representative and senator from Mississippi. Lincoln—who had served in the Illinois state legislature and as an undistinguished one-term member of the U.S. House of Representatives—could boast of only a brief period of military service in the Black Hawk War, in which he saw no action.\nAs president and commander in chief of the Confederate forces, Davis revealed many fine qualities, including dignity, firmness, determination, and honesty, but he was flawed by his excessive pride, hypersensitivity to criticism, poor political skills, and tendency to micromanage. He engaged in extended petty quarrels with generals and cabinet members. He also suffered from ill health throughout the conflict. Davis’s effectiveness was further hampered by a political system that limited him to a single six-year term—thereby making him a lame duck immediately upon his election—and that frowned on organized political parties, which Southerners accused of having been at least partly responsible for the coming of the Civil War. The lack of political parties meant that Davis could command no loyalty from a broad group of people such as governors or political appointees when he came under heavy criticism.\nTo a large extent and by his own preference, Davis was his own secretary of war, although five different men served in that post during the lifetime of the Confederacy. Davis himself also filled the position of general in chief of the Confederate armies until he named Robert E. Lee to that position on February 6, 1865, when the Confederacy was near collapse. In naval affairs—an area about which he knew little—the Confederate president seldom intervened directly, allowing the competent secretary of the navy, Stephen Mallory, to handle the Southern naval buildup and operations on the water. Although his position was onerous and quite likely could not have been filled as well by any other Southern political leader—most of them having come to prominence in a period of growing disinclination to compromise—Davis’s overall performance in office left something to be desired.\nTo the astonishment of many, Lincoln grew in stature with time and experience, and by 1864 he had become a consummate politician and war director. Lincoln matured into a remarkably effective president because of his great intelligence, communication skills, humility, sense of purpose, sense of humour, fundamentally moderate nature, and ability to remain focused on the big picture. But he had much to learn at first, especially in strategic and tactical matters and in his choices of army commanders. With an ineffective first secretary of war—Simon Cameron—Lincoln unhesitatingly insinuated himself directly into the planning of military movements. Edwin M. Stanton, a well-known lawyer appointed to the secretaryship on January 20, 1862, was equally untutored in military affairs, but he was fully as active a participant as his superior.\nWinfield Scott was the Federal general in chief when Lincoln took office. The 75-year-old Scott—a hero of the War of 1812 and the Mexican-American War—was a magnificent and distinguished soldier whose mind was still keen, but he was physically incapacitated and had to be retired from the service on November 1, 1861. Scott was replaced by young George B. McClellan, who was an excellent organizer. McClellan, however, lacked tenacity, persistently overestimated the Confederates’ strength (and therefore stalled his attacks), and was openly disdainful of the president. Because he wanted McClellan to focus his attentions on the Army of the Potomac, Lincoln relieved McClellan as general in chief on March 11, 1862. Henry W. Halleck, who proved to be a strong administrator but did little in the way of strategic planning, succeeded McClellan on July 11 and held the position until he was replaced by Ulysses S. Grant on March 9, 1864. Halleck then became chief of staff under Grant in a long-needed streamlining of the Federal high command. Grant served efficaciously as general in chief throughout the remainder of the war.\nAfter the initial call by Lincoln and Davis for troops, and as the war lengthened indeterminately, both sides turned to raising massive armies of volunteers. Local citizens of prominence and means would organize regiments that were uniformed and accoutred at first under the aegis of the states and then mustered into the service of the Union and Confederate governments. On each side, the presidents appointed so-called “political generals,” men who had little or no military training or experience but had important political connections (for example, Northern Democrats) or had ties to immigrant communities. Although successful politically, most of these appointments did not yield happy military results. As the war dragged on, the two governments had to resort to conscription to fill the ranks being so swiftly thinned by battle casualties.\nIn the area of grand strategy, Davis persistently adhered to the defensive, permitting only occasional “spoiling” forays into Northern territory. Perhaps the Confederates’ best chance of winning would have been an early grand offensive into the Union states before the Lincoln administration could find its ablest generals and bring the preponderant resources of the North to bear against the South. On the other hand, protecting the territory the Confederacy already controlled was of paramount importance, and a defensive position allowed the rebels to husband their resources somewhat better. To crush the rebellion and reestablish the authority of the Federal government, Lincoln had to direct his blue-clad armies to invade, capture, and hold most of the vital areas of the Confederacy. His grand strategy was based on Scott’s so-called Anaconda Plan, a design that evolved from strategic ideas discussed in messages between Scott and McClellan on April 27, May 3, and May 21, 1861. It called for a Union blockade of the Confederacy’s coastline as well as a decisive thrust down the Mississippi River and an ensuing strangulation of the South by Federal land and naval forces. But it was to take four years of grim, unrelenting warfare and enormous casualties and devastation before the Confederates could be defeated and the Union preserved.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/topic/Pittsburgh-Steelers",
    "content": "Pittsburgh Steelers, American professional gridiron football team based in Pittsburgh that plays in the NFL. One of the league’s most successful and storied franchises, the Steelers have won six Super Bowl titles and eight American Football Conference (AFC) championships.\nOriginally called the Pittsburgh Pirates, the team was founded in 1933 by Pittsburgh resident Art Rooney, who allegedly used winnings from a wager on a horse race to establish the franchise. (Ownership of the team remains within the Rooney family to this day.) The team was not an early success; it qualified for the playoffs just once in its first 37 years. In 1940 the team changed its nickname to “Steelers” in tribute to Pittsburgh’s main industry at the time. The Steelers tied for the NFL Eastern Division title in 1947, but they were shut out 21–0 by the Philadelphia Eagles in a playoff match to qualify for the NFL championship game. Rooney watched the Steelers struggle through the 1950s and ’60s until their fortunes turned around with the arrival of head coach Chuck Noll in 1969.\nFrom 1969 to 1972 Noll showcased his amazing skill at recognizing talent as he drafted five future Hall of Famers: defensive tackle “Mean” Joe Greene, quarterback Terry Bradshaw, defensive back Mel Blount, linebacker Jack Ham, and running back Franco Harris (remembered for his “Immaculate Reception,” a game-winning catch in the playoffs against the Oakland Raiders in 1972, one of the most remarkable and controversial plays in professional football history). In 1974 Noll selected four more players who would eventually be inducted into the Hall of Fame: center Mike Webster, receivers Lynn Swann and John Stallworth, and linebacker Jack Lambert. These players went on to form a dynasty of unmatched success, winning four Super Bowls (1975, 1976, 1979, and 1980) in six seasons behind a dominant defense known as the “Steel Curtain” and an efficient offense led by Bradshaw. The Steelers teams of the 1970s were also characterized by a fervent fan base, notable for the bright yellow “Terrible Towels”—which were created by the team’s popular and idiosyncratic radio broadcaster for 35 years, Myron Cope—that fans would wave during home games. Pittsburgh faded slightly in the 1980s, with four postseason berths in the decade, and Noll retired in 1991.\nNoll was replaced by Bill Cowher, who led the Steelers to the playoffs in 10 of his 15 seasons with the team. One of Cowher’s most significant personnel moves was his promotion of secondary coach Dick LeBeau to the position of defensive coordinator in 1995: in his two stints (1995–97, 2004–15) as the Steelers’ coordinator, LeBeau put together formidable defenses that defined the Pittsburgh teams of those eras. The Steelers’ defense of the mid-1990s was highlighted by stars such as future Hall of Fame cornerback Rod Woodson and linebackers Greg Lloyd and Kevin Greene. Pittsburgh advanced to the Super Bowl in 1996 but lost to the Dallas Cowboys.\nThe Steelers continued their success into the new century, and in 2006—with a team featuring quarterback Ben Roethlisberger, wide receiver Hines Ward, and safety Troy Polamalu—they defeated the Seattle Seahawks to gain a fifth Super Bowl title. In 2009 the Steelers, under the leadership of second-year head coach Mike Tomlin, beat the Arizona Cardinals in dramatic fashion to capture their record sixth Super Bowl championship. After missing the playoffs following the 2009 regular season, Pittsburgh captured its third AFC championship in a six-year span in 2011 to earn a berth in Super Bowl XLV, a loss to the Green Bay Packers.\nThe Steelers returned to the playoffs after both the 2011 and 2014 regular seasons, but the team lost in the opening round of each postseason. Pittsburgh won its first playoff game following the 2015 season but was eliminated by the Denver Broncos in the divisional round. The team won 11 games the following year and advanced to the playoffs, where the Steelers lost to Tom Brady and the New England Patriots in the AFC championship game for the third time. Pittsburgh topped that regular-season performance in 2017 with 13 wins, which tied with three other teams for the most in the NFL. However, the Steelers were upset in the team’s opening playoff game. In 2018 the team went 9–6–1 while enduring a number of high-profile public spats among many of its best players, missing the postseason for the first time in five years.\nFrom 2019 to 2023 the Steelers continued their reliably solid, if unspectacular, play under the steady leadership of Tomlin, who, since he joined the team in 2007, has never finished a season with fewer than eight wins. A strong defense was a hallmark of the team, led by linebacker T.J. Watt, a perennial Defensive Player of the Year finalist (he won the award in 2021). Pittsburgh made the playoffs after the 2020, 2021, and 2023 seasons, each time losing in the wild card round.\nSuper Bowl, in U.S. professional football, the championship game of the National Football League (NFL), played by the winners of the league’s American Football Conference and National Football Conference each January or February. The game is hosted by a different city each year.\n(Read Walter Camp’s 1903 Britannica essay on inventing American football.)\nThe game grew out of the merger of the NFL and rival American Football League (AFL) in 1966. The agreement called for an end-of-season championship game, and, although the merger was not finalized until 1970, the first such game, then called the AFL-NFL World Championship Game, was played at the Los Angeles Memorial Coliseum on January 15, 1967. Broadcast on two television networks and played before less than a sellout crowd, the game saw the NFL’s Green Bay Packers defeat the AFL’s Kansas City Chiefs, 35–10. The name “Super Bowl” first appeared in 1969, as did the use of Roman numerals, which, because the game is played in a different year from the season it culminates, are used to designate the individual games.\nThe day of the Super Bowl game, known as Super Bowl Sunday, has evolved into an unofficial American holiday, with viewing parties held in homes, taverns, and restaurants throughout the country. The week prior to the game is highlighted by extensive media buildup and a festival atmosphere in the host city. The game itself is accompanied by elaborate pregame and halftime ceremonies and entertainment.\nAll Super Bowls since the first have been sellouts and consistent TV-ratings leaders, with many Super Bowls among the highest-rated televised sporting events of all time. As a result, commercial time during the game is the most expensive of the year; for example, in 2016 a 30-second spot cost approximately $4.8 million. The high-profile advertisements have featured celebrities and noted filmmakers as well as new technologies in hopes of making an impression on the huge Super Bowl audience. Since the 1980s, media scrutiny of and public interest in Super Bowl commercials have nearly matched that accorded the game itself.\nThe table provides a list of Super Bowl results.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/money/Merton-Miller",
    "content": "Merton H. Miller (born May 16, 1923, Boston, Massachusetts, U.S.—died June 3, 2000, Chicago, Illinois) was an American economist who, with Harry M. Markowitz and William F. Sharpe, won the Nobel Prize for Economics in 1990. His contribution (and that of his colleague Franco Modigliani, who received the Nobel Prize for Economics in 1985), known as the Modigliani-Miller theorem, was pioneering work in the field of finance theory.\nMiller attended Harvard University (B.A., 1944), worked at the U.S. Treasury Department, and then graduated from Johns Hopkins University in Baltimore, Maryland (Ph.D., 1952). He taught at the Carnegie Institute of Technology (now Carnegie Mellon University) in Pittsburgh, Pennsylvania, until 1961, when he accepted a position as a professor of finance at the University of Chicago’s Graduate School of Business Administration.\nMiller built upon the work of Markowitz (whose “portfolio theory” established that wealth can best be invested in assets that vary in terms of risk and expected return) and Sharpe (who developed the “capital asset pricing model” to explain how securities prices reflect risks and potential returns). The Modigliani-Miller theorem explains the relationship between a company’s capital asset structure and dividend policy and its market value and cost of capital; the theorem demonstrates that how a manufacturing company funds its activities is less important than the profitability of those activities.\nMiller was recognized as one of the most important developers of theoretical and empirical analysis in the field of corporate finance. In addition to being the business school’s Robert R. McCormick Distinguished Service Professor, Miller served as a director (1990–2000) of the Chicago Mercantile Exchange.\nIn the 1950s, a new crop of statisticians at Bell Laboratories, the RAND Corporation, and several universities wanted to use burgeoning computer power for analysis. They found that stock market data was comprehensive enough to analyze thoroughly, and they set off a revolution in finance.\nIn 1952, Harry Markowitz published a paper called “Portfolio Selection” in The Journal of Finance, setting out what he called the modern portfolio theory (MPT). It caught on, inspired other groundbreaking research, and was eventually renamed Markowitz portfolio theory in his honor. (It helped that the acronym stayed the same.) He won the Nobel Prize for his work in 1990.\nWhether you refer to it as Markowitz portfolio theory or modern portfolio theory, MPT introduced a systematic approach to building and managing investment portfolios. Instead of choosing individual investments, MPT urges investors to consider their risk preferences first.\nAt the heart of modern portfolio theory is the concept of diversification. Diversification involves spreading investments across a range of assets to reduce risk, including stocks, bonds, and alternative assets. MPT argues that by holding a well-diversified portfolio, you can achieve a more favorable risk-return trade-off than you could by concentrating your investments in a single asset or asset class.\nMarkowitz raised key points that continue to matter both in academic finance and the real world:\nTo determine your optimal portfolio, compare your efficient frontier arc with the capital market line (CML). This comparison illustrates the trade-off between the standard deviation (what we now call market volatility) of returns and expected return when combining a risk-free asset (such as U.S. Treasury bonds) with a diversified portfolio of risky assets (such as stocks and alternative investments).\nThe CML slopes upward because the expected return (over and above the risk-free rate) should theoretically be commensurate with the risk an investor is willing to take. The optimal portfolio, then, is the point at which your efficient frontier touches (i.e., is tangent to) the CML.\nIn theory: The CML helps investors determine the optimal allocation between risk-free and risky assets based on their risk preferences. The optimal portfolio is tangent to this line.\nIn practice: You probably won’t calculate your optimal portfolio return like this. The lesson from Markowitz that has carried into the 21st century is that investors can expect the best risk-adjusted return through diversification. That’s why index funds, which track the performance of a broad-based stock index such as the S&P 500, can be a core component of a diversified portfolio, along with Treasuries and other fixed-income securities.\nDespite its profound impact on investment practice, MPT is not without criticism. Detractors argue that the theory makes simplifying assumptions about market behavior, such as the normal distribution of asset returns and constant correlations, which may not hold in the real world.\nOne issue is using the standard deviation as a measure of risk. This benchmark considers a return greater than expected to be as risky as getting a return that’s less than expected, but most rational investors would disagree.\nMany of the limitations reflect the revolutionary nature of Markowitz’s theory. Several economists looked at MPT, saw the benefits of the basic framework, and used it as the starting point for such fundamental financial concepts as the efficient market hypothesis (EMH) and the capital asset pricing model (CAPM). Without MPT, their work might have taken longer to emerge, if it emerged at all.\nHarry Markowitz truly created modern finance, and the MPT remains the cornerstone of investment theory. The principle of managing risk preferences holds to this day. Consider two key ways to apply MPT to your investments:\nAs with most investing concepts, MPT works best as a framework for understanding how the world works—theoretically (hence the name). But you must adapt the theory to your reality to find the portfolio structure that works best for you.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/event/World-War-II",
    "content": "World War II, conflict that involved virtually every part of the world during the years 1939–45. The principal belligerents were the Axis powers—Germany, Italy, and Japan—and the Allies—France, Great Britain, the United States, the Soviet Union, and, to a lesser extent, China. The war was in many respects a continuation, after an uneasy 20-year hiatus, of the disputes left unsettled by World War I. The 40,000,000–50,000,000 deaths incurred in World War II make it the bloodiest conflict, as well as the largest war, in history.\nAlong with World War I, World War II was one of the great watersheds of 20th-century geopolitical history. It resulted in the extension of the Soviet Union’s power to nations of eastern Europe, enabled a communist movement to eventually achieve power in China, and marked the decisive shift of power in the world away from the states of western Europe and toward the United States and the Soviet Union.\n(Read Sir John Keegan’s Britannica entry on the Normandy Invasion.)\nBy the early part of 1939 the German dictator Adolf Hitler had become determined to invade and occupy Poland. Poland, for its part, had guarantees of French and British military support should it be attacked by Germany. Hitler intended to invade Poland anyway, but first he had to neutralize the possibility that the Soviet Union would resist the invasion of its western neighbour. Secret negotiations led on August 23–24 to the signing of the German-Soviet Nonaggression Pact in Moscow. In a secret protocol of this pact, the Germans and the Soviets agreed that Poland should be divided between them, with the western third of the country going to Germany and the eastern two-thirds being taken over by the U.S.S.R.\nHaving achieved this cynical agreement, the other provisions of which stupefied Europe even without divulgence of the secret protocol, Hitler thought that Germany could attack Poland with no danger of Soviet or British intervention and gave orders for the invasion to start on August 26. News of the signing, on August 25, of a formal treaty of mutual assistance between Great Britain and Poland (to supersede a previous though temporary agreement) caused him to postpone the start of hostilities for a few days. He was still determined, however, to ignore the diplomatic efforts of the western powers to restrain him. Finally, at 12:40 PM on August 31, 1939, Hitler ordered hostilities against Poland to start at 4:45 the next morning. The invasion began as ordered. In response, Great Britain and France declared war on Germany on September 3, at 11:00 AM and at 5:00 PM, respectively. World War II had begun.\nIn September 1939 the Allies, namely Great Britain, France, and Poland, were together superior in industrial resources, population, and military manpower, but the German military, or Wehrmacht, because of its armament, training, doctrine, discipline, and fighting spirit, was the most efficient and effective fighting force for its size in the world. The index of military strength in September 1939 was the number of divisions that each nation could mobilize. Against Germany’s 100 infantry divisions and six armoured divisions, France had 90 infantry divisions in metropolitan France, Great Britain had 10 infantry divisions, and Poland had 30 infantry divisions, 12 cavalry brigades, and one armoured brigade (Poland had also 30 reserve infantry divisions, but these could not be mobilized quickly). A division contained from 12,000 to 25,000 men.\nIt was the qualitative superiority of the German infantry divisions and the number of their armoured divisions that made the difference in 1939. The firepower of a German infantry division far exceeded that of a French, British, or Polish division; the standard German division included 442 machine guns, 135 mortars, 72 antitank guns, and 24 howitzers. Allied divisions had a firepower only slightly greater than that of World War I. Germany had six armoured divisions in September 1939; the Allies, though they had a large number of tanks, had no armoured divisions at that time.\nThe six armoured, or panzer, divisions of the Wehrmacht comprised some 2,400 tanks. And though Germany would subsequently expand its tank forces during the first years of the war, it was not the number of tanks that Germany had (the Allies had almost as many in September 1939) but the fact of their being organized into divisions and operated as such that was to prove decisive. In accordance with the doctrines of General Heinz Guderian, the German tanks were used in massed formations in conjunction with motorized artillery to punch holes in the enemy line and to isolate segments of the enemy, which were then surrounded and captured by motorized German infantry divisions while the tanks ranged forward to repeat the process: deep drives into enemy territory by panzer divisions were thus followed by mechanized infantry and foot soldiers. These tactics were supported by dive bombers that attacked and disrupted the enemy’s supply and communications lines and spread panic and confusion in its rear, thus further paralyzing its defensive capabilities. Mechanization was the key to the German blitzkrieg, or “lightning war,” so named because of the unprecedented speed and mobility that were its salient characteristics. Tested and well-trained in maneuvers, the German panzer divisions constituted a force with no equal in Europe.\nThe German Air Force, or Luftwaffe, was also the best force of its kind in 1939. It was a ground-cooperation force designed to support the Army, but its planes were superior to nearly all Allied types. In the rearmament period from 1935 to 1939 the production of German combat aircraft steadily mounted. The table shows the production of German aircraft by years.\nThe standardization of engines and airframes gave the Luftwaffe an advantage over its opponents. Germany had an operational force of 1,000 fighters and 1,050 bombers in September 1939. The Allies actually had more planes in 1939 than Germany did, but their strength was made up of many different types, some of them obsolescent. The corresponding table shows the number of first-line military aircraft available to the Allies at the outbreak of war.\nGreat Britain, which was held back by delays in the rearmament program, was producing one modern fighter in 1939, the Hurricane. A higher-performance fighter, the Spitfire, was just coming into production and did not enter the air war in numbers until 1940.\nThe value of the French Air Force in 1939 was reduced by the number of obsolescent planes in its order of battle: 131 of the 634 fighters and nearly all of the 463 bombers. France was desperately trying to buy high-performance aircraft in the United States in 1939.\nAt sea the odds against Germany were much greater in September 1939 than in August 1914, since the Allies in 1939 had many more large surface warships than Germany had. At sea, however, there was to be no clash between the Allied and the German massed fleets but only the individual operation of German pocket battleships and commerce raiders.\nWhen World War I ended, the experience of it seemed to vindicate the power of the defensive over the offensive. It was widely believed that a superiority in numbers of at least three to one was required for a successful offensive. Defensive concepts underlay the construction of the Maginot Line between France and Germany and of its lesser counterpart, the Siegfried Line, in the interwar years. Yet by 1918 both of the requirements for the supremacy of the offensive were at hand: tanks and planes. The battles of Cambrai (1917) and Amiens (1918) had proved that when tanks were used in masses, with surprise, and on firm and open terrain, it was possible to break through any trench system.\nThe Germans learned this crucial, though subtle, lesson from World War I. The Allies on the other hand felt that their victory confirmed their methods, weapons, and leadership, and in the interwar period the French and British armies were slow to introduce new weapons, methods, and doctrines. Consequently, in 1939 the British Army did not have a single armoured division, and the French tanks were distributed in small packets throughout the infantry divisions. The Germans, by contrast, began to develop large tank formations on an effective basis after their rearmament program began in 1935.\nIn the air the technology of war had also changed radically between 1918 and 1939. Military aircraft had increased in size, speed, and range, and for operations at sea, aircraft carriers were developed that were capable of accompanying the fastest surface ships. Among the new types of planes developed was the dive bomber, a plane designed for accurate low-altitude bombing of enemy strong points as part of the tank-plane-infantry combination. Fast low-wing monoplane fighters were developed in all countries; these aircraft were essentially flying platforms for eight to 12 machine guns installed in the wings. Light and medium bombers were also developed that could be used for the strategic bombardment of cities and military strongpoints. The threat of bomber attacks on both military and civilian targets led directly to the development of radar in England. Radar made it possible to determine the location, the distance, and the height and speed of a distant aircraft no matter what the weather was. By December 1938 there were five radar stations established on the coast of England, and 15 additional stations were begun. So, when war came in September 1939, Great Britain had a warning chain of radar stations that could tell when hostile planes were approaching.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/place/Pittsburgh#ref241999",
    "content": "Pittsburgh, city, seat (1788) of Allegheny county, southwestern Pennsylvania, U.S. The city is located at the confluence of the Allegheny and Monongahela rivers, which unite at the point of the “Golden Triangle” (the business district) to form the Ohio River. A city of hills, parks, and valleys, it is the centre of an urban industrial complex that includes the surrounding cities of Aliquippa (northwest), New Kensington (northeast), McKeesport (southeast), and Washington (southwest) and the borough of Wilkinsburg (east). Inc. borough, 1794; city, 1816. Area city, 58 square miles (150 square km). Pop. (2010) 305,704; Pittsburgh Metro Area, 2,356,285; (2020) 302,971; Pittsburgh Metro Area, 2,370,930.\nAlgonquian- and Iroquoian-speaking peoples were early inhabitants of the region. The conflict between the British and French over territorial claims in the area was settled in 1758 when General John Forbes and his British and colonial army expelled the French from Fort Duquesne (built 1754). Forbes named the site for the British statesman William Pitt the Elder. The British built Fort Pitt (1761) to ensure their dominance at the source of the Ohio. Settlers began arriving after Native American forces led by Ottawa chief Pontiac were defeated in 1763; an agreement subsequently was made between Native American groups and the Penn family, and a boundary dispute was ended between Pennsylvania and Virginia. Pittsburgh was laid out (1764) by John Campbell in the area around the fort (now the Golden Triangle). Following the American Revolution, the town became an outfitting point for settlers traveling westward down the Ohio River.\nPittsburgh’s strategic location and wealth of natural resources spurred its commercial and industrial growth in the 19th century. A blast furnace, erected by George Anschutz about 1792, was the forerunner of the iron and steel industry that for more than a century was the city’s economic mainstay; by 1850 Pittsburgh was known as the “Iron City.” The Pennsylvania Canal and the Portage Railroad, both completed in 1834, opened vital markets for trade and shipping. The city suffered a great loss in 1845 when some 24 blocks of businesses, homes, churches, and other buildings were destroyed by fire.\nAfter the American Civil War, great numbers of European immigrants swelled Pittsburgh’s population, and industrial magnates such as Andrew Carnegie, Henry Clay Frick, and Thomas Mellon built their steel empires there. The city became the focus of historic friction between labour and management, and the American Federation of Labor was born there in 1881.\nBy 1900 the city’s population had reached 321,616. Growth continued nearly unabated through World War II, the war years bringing a particularly great boon for the economy. The population crested at more than 675,000 in 1950, after which it steadily declined; by the end of the century, it had returned almost to the 1900 level. Most citizens were still of European ancestry, but the growing African American proportion of the population exceeded one-fourth. During the period of economic and population growth, Pittsburgh had come to epitomize the grimy, polluted industrial city. After the war, however, the city undertook an extensive redevelopment program that emphasized smoke-pollution control, flood prevention, and sewage disposal. In 1957 it became the first American city to generate electricity by nuclear power.\nBy the late 1970s and early ’80s, the steel industry had virtually disappeared—a result of foreign competition and decreased demand. Many of the surrounding mill towns were laid to waste by unemployment, becoming a symbol of the notorious Rust Belt, the U.S. region where steelmaking and manufacturing once thrived before succumbing to widespread unemployment and poverty. Pittsburgh, however, successfully diversified its economy through more emphasis on light industries—though metalworking, chemicals, and plastics remained important—and on such high-technology industries as computer software, industrial automation (robotics), and biomedical and environmental technologies. Numerous industrial research laboratories were established in the area, and the service sector became increasingly important. Pittsburgh long has been one of the nation’s largest inland ports, and it remains a leading transportation centre.\nMuch of the Golden Triangle has been rebuilt and includes Point State Park (containing Fort Pitt Blockhouse and Fort Pitt Museum), the Gateway Center (site of several skyscrapers and a garden), and the David L. Lawrence Convention Center. The University of Pittsburgh was chartered in 1787. Other educational institutions include Carnegie Mellon (1900), Duquesne (1878), Point Park (1960), Chatham (1869), and Carlow (1929) universities and two campuses of the Community College of Allegheny County (1966).\nCentral to the city’s cultural life is the Carnegie Museums of Pittsburgh (formerly Carnegie Institute), an umbrella organization consisting of a number of institutions. Its museums include those for the fine arts and natural history (both founded in 1895), the Carnegie Science Center (1991), which now also houses the Henry Buhl, Jr., Planetarium and Observatory (1939), and the Andy Warhol Museum (1994), which exhibits the works of the Pittsburgh-born artist and filmmaker. Other institutions affiliated with the organization are the Carnegie Library of Pittsburgh, which contains more than 3.3 million volumes, and the Carnegie Music Hall. The Pittsburgh Symphony Orchestra performs at Heinz Hall, a restored movie theatre.\nPhipps Conservatory and Botanical Gardens (1893), which covers 15 acres (6 hectares), is noted for its extensive greenhouses. The city’s zoo, in the northeastern Highland Park neighbourhood, includes an aquarium. Two new sports venues opened in 2001 on the north bank of the Allegheny opposite the Golden Triangle: PNC Park is home of the Pirates, the city’s professional baseball team, and Acrisure Stadium houses the Steelers, its professional football team. The Penguins, Pittsburgh’s professional ice hockey team, plays at PPG Paints Arena. Popular summertime attractions include riverboat excursions on Pittsburgh’s waterways and Kennywood, an amusement park southeast of the city in West Mifflin.\nCarnegie Mellon University, private, coeducational institution of higher learning in Pittsburgh, Pennsylvania, U.S. The university includes the Carnegie Institute of Technology, the College of Humanities and Social Sciences, the College of Fine Arts, the Mellon College of Science, the School of Computer Science, the H. John Heinz III School of Public Policy and Management, and the Graduate School of Industrial Administration. Undergraduate and graduate degree programs are offered in a range of fields. Total enrollment is about 7,700.\nIn 1900 the industrialist Andrew Carnegie gave a gift of $1 million to the city of Pittsburgh for the creation of a technical school. Originally called Carnegie Technical Schools, it was renamed Carnegie Institute of Technology in 1912. The institute merged with the Mellon Institute (established in 1913 in Pittsburgh by financier Andrew W. Mellon) in 1967. The university has built a reputation as a vital arts centre, operating three art galleries, two concert halls, and two theatres. The faculty has included Nobel Prize-winning economists Herbert Alexander Simon and Merton Miller.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/topic/Carnegie-Mellon-University",
    "content": "Carnegie Mellon University, private, coeducational institution of higher learning in Pittsburgh, Pennsylvania, U.S. The university includes the Carnegie Institute of Technology, the College of Humanities and Social Sciences, the College of Fine Arts, the Mellon College of Science, the School of Computer Science, the H. John Heinz III School of Public Policy and Management, and the Graduate School of Industrial Administration. Undergraduate and graduate degree programs are offered in a range of fields. Total enrollment is about 7,700.\nIn 1900 the industrialist Andrew Carnegie gave a gift of $1 million to the city of Pittsburgh for the creation of a technical school. Originally called Carnegie Technical Schools, it was renamed Carnegie Institute of Technology in 1912. The institute merged with the Mellon Institute (established in 1913 in Pittsburgh by financier Andrew W. Mellon) in 1967. The university has built a reputation as a vital arts centre, operating three art galleries, two concert halls, and two theatres. The faculty has included Nobel Prize-winning economists Herbert Alexander Simon and Merton Miller.\nrobotics, design, construction, and use of machines (robots) to perform tasks done traditionally by human beings. Robots are widely used in such industries as automobile manufacture to perform simple repetitive tasks, and in industries where work must be performed in environments hazardous to humans. Many aspects of robotics involve artificial intelligence; robots may be equipped with the equivalent of human senses such as vision, touch, and the ability to sense temperature. Some are even capable of simple decision making, and current robotics research is geared toward devising robots with a degree of self-sufficiency that will permit mobility and decision-making in an unstructured environment. Today’s industrial robots do not resemble human beings; a robot in human form is called an android.\nJapanese roboticist Masahiro Mori proposed that as human likeness increases in an object’s design, so does one’s affinity for the object, giving rise to the phenomenon called the \"uncanny valley.\" According to this theory, when the artificial likeness nears total accuracy, affinity drops dramatically and is replaced by a feeling of eeriness or uncanniness. Affinity then rises again when true human likeness—resembling a living person—is reached. This sudden decrease and increase caused by the feeling of uncanniness creates a “valley” in the level of affinity.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/story/pro-and-con-college-education",
    "content": "The American debate over whether a college education is worth it began when the colonists arrived from Europe and founded “New College” (later renamed Harvard University) in 1636. In spring 2024 there were more than 17.8 million college students in the United States. [133][269]\nExplore the ProCon debate\nColonial America produced nine colleges that still operate: Harvard University (1636), the College of William & Mary (1693), Yale University (1701), Princeton University (1746), Columbia University (1754), Brown University (1764), Dartmouth College (1769), Rutgers University (1766), and the University of Pennsylvania (1740 or 1749). These universities were funded by the colony or England and usually catered to a specific religious denomination, such as Congregational or Presbyterian (Puritan). Primary and secondary school systems were not yet established, so “college students” were sometimes boys as young as 14 or 15 years old and were admitted to receive preparatory education with the assumption that they would matriculate to college-level courses. [3][78][79][80][81][82][83][84][85][86]\nColonial colleges were mainly founded and attended by wealthy Puritans and followed the models of British and Scottish universities, which focused on general education and moral character. The goal of the college was to produce Christian gentlemen who would inherit family businesses, remain within the Congregational or Presbyterian (Puritan) faith, and be responsible leaders in the New World. Colonial college tuition costs and the loss of an able-bodied man from the family farm or business made the prestige and social status conferred by college unattainable for most families. About 1% of white males aged 18–21 attended college, and students frequently left college after their first or second year, with no “dropout” stigma. Out of 35 students, Yale conferred nine bachelor’s degrees by 1711. Colonial colleges excluded women but sometimes had “Indian schools” to spread Puritanism in Native American communities for religious indoctrination purposes. The American Revolution (1775–83) drained colleges of students who became soldiers and buildings that became barracks and of the funds from England, resulting in the closing of many colleges postwar. [3][87][88]\nThe late 18th and early 19th centuries created a college-building boom, increasing the number of schools from 25 colleges in 1800 to 241 colleges in 1860; the variety of schools to include seminaries, scientific schools, military service academies, and teaching schools; and the programs of study to include medicine, law, military science, and agriculture. State universities came into prominence beginning with the University of North Carolina (1795) and the University of Georgia (1801). In the spring of 1833, Oberlin Collegiate Institute (now Oberlin College) admitted women to a “ladies course” program and in 1837 admitted four women to the baccalaureate program, three of whom graduated in 1841 with degrees. [3][100]\nThe 1862 Morrill Land-Grant Act gave federally controlled land to states to open “land-grant” colleges, which were required to focus on “useful arts” like agriculture, mechanics, mining, and military instruction and thus often included “A&M” (agricultural and mechanical) in the names. The idea of a “useful” education also created schools like the Massachusetts Institute of Technology in 1851. Many craftsmen who relied upon apprenticeships were skeptical of college training and distrusted scholars and scientists. A college degree was still seen by many as a social marker rather than a marker of educational attainment. [3][89]\nBy 1865 most Southern colleges ceased offering classes because the American Civil War caused significant physical damage to many colleges, while others were made into hospitals and shelters for soldiers, and many Southern students and faculty left college for the Confederate Army. In 1870 the number of colleges was 560 (up from just nine colleges by the American Revolution). [3][87]\nThe early 1900s saw institutions created to educate groups excluded by traditional colleges: women, Black people, immigrants, and Roman Catholics. Black colleges remained restricted to grade-school and agricultural- or industrial-focused instruction with little college-level education offered. Iowa State University was the first co-ed land-grant college, though women remained segregated and were expected to study “domestic science” or similar topics. Colleges were built in the South to keep Southern sons “far from the dangerous notions circulating at a Harvard or a Yale,” with high tuitions and a code of honor that included duels amongst students. “Hilltop colleges” in New England opened to cater to older working students training to be teachers or ministers. Colleges built on the emerging Western frontier had small populations to support them and there were often less than a few hundred students who could attend the college. “Diploma mills” also popped up during this time, especially the “medical college,” which frequently had no campus or faculty but would confer degrees in exchange for donations. [3][90][91]\nThe goal of college attendance still was not completion of a bachelor’s degree. Some students took two years of courses in order to earn a license of instruction certificate to teach public school, but many did not complete the degree because, as explained by Roger L. Geiger, distinguished professor of education at Pennsylvania State University, “There was nothing to be done with a bachelor degree that could not also be done without one.” [3][92]\nBy 1900 5% (about 256,000) of 18–21 year old males attended college, up from 3.1% (32,364) in 1860 and 1% (1,237) in 1800. Students were normally accepted based on gender, religion, and race. Graduation rates continued to be low: about 30% of the 1903 freshman class at Kentucky State College graduated, while Transylvania University averaged a 50% dropout rate in the first year and barely 10% graduated with a degree in four years. [3][87]\nAlthough tuition had seen no major increases, the price of college was still too high for the average family. For the 1907–08 academic year, Brown University published an average tuition budget: $105 for tuition, $48 for “incidental fees,” $60 for room, $150 for board, and $30 for books and lab fees, totaling $393 per year, or $9,535.67 in 2012 U.S. dollars. By 1910 “undergraduate life” came into prominence with mascots, school colors, college hymns, intercollegiate athletics, and other traditions. [3]\nWorld War I dropped enrollments on the East Coast by 27–40%, but only 10% of Stanford men left college for the war. In total, 540 colleges were made into training campuses for the Students’ Army Training Corps to train 125,000 men. Around this time, the American Medical Association began lobbying for medical schools to require some college science (if not a completed college degree) for admission into medical schools, law schools followed Harvard Law School’s example to require baccalaureate degrees for admission, and seminaries were requiring at least a year of college. [3][92]\nAfter 1920 college students became associated with parties, gambling, and “bathtub gin.” But such partying was tolerated because of the upward social mobility gained by making contacts and partying with the right crowd. [3]\nBetween 1920 and 1945 secondary schools expanded, increasing the number of high-school graduates, the number of college students from 250,000 to 1.3 million, and the percentage of college students from 5% to 15%. However, an 18- to 24-year-old white person was four times more likely to attend college than a Black person of the same age, and women constituted about 40% of college enrollments but were still being trained in segregation as teachers, good wives, and mothers. [3]\nIn the 1920s and 1930s college tuition started to rise. One national survey showed tuition at $70 in 1920 and $133 in 1940, or from $1,064.44 to $2,889.21 in 2023 U.S. dollars. In comparison, a 1940 new Pontiac car cost $483 ($10,492.38 in 2023 dollars). [3]\nAfter World War II, colleges and universities moved toward advanced, selective programs and expanded the base of students admitted. Research universities, junior colleges (now called community colleges), and for-profit institutions thrived. [3]\nIn the 1939–40 school year student enrollment was under 1.5 million nationally, but by the 1949–50 school year student enrollment grew to 2.7 million. The 1944 G.I. Bill contributed to some of the enrollment gains, and by the beginning of the 1945–46 school year 88,000 veterans were accepted into the program; by 1946 over one million were accepted; and by 1950 14 million veterans were in the program. Women represented about 40% of enrollment in the 1939–40 school year, but that number dropped to 32% in 1950. Individual schools implemented honor programs, specialty seminars, study abroad, and smaller class sizes to attract more discerning students. By 1960 national enrollment was at 3.6 million and at 7.9 million in 1970. Society became interested in the college lives of “Joe College” and “Betty Coed” and created the college ideal of graduating in four years, marrying the college sweetheart, and finding a good job. [3]\nThe federal government created the Higher Education General Information Survey, later renamed the Integrated Postsecondary Education Data System, and collected data in the fall of 1968, the first time standardized data was collected about colleges and universities nationwide. [93]\nPell Grants were introduced in 1972 and increased the number of students for whom higher education was possible. By 1978 the financial aid focus changed from grants to loans, increasing the amount of debt a graduating college student owned. In the 1975–76 school year 75% of students received grants and 21% received loans, compared to the 1984–85 school year, in which 29% of students received grants and 66% received loans. [3][94]\nThe major shift in higher education during this time was the transition from mass higher education, expecting to educate 40–50% of high-school graduates, to universal higher education, expecting to educate all high-school graduates. The shift was seen in public school enrollments, which accounted for about 75% of enrollments in 1970, up from the almost equal split between public and private colleges in 1950. Community colleges and technical institutes also gained students: from 82,000 in 1950 to 1.3 million in 1980. [3]\nTransfer students were accommodated, classes were offered at military bases, and courses were offered at extension sites for nontraditional students while colleges were opening to diverse student populations. Title IX (1972) and affirmative action demanded inclusive admission practices for women and Black students. [3][95][96]\nThe 1970s also saw the shift from higher education for education’s sake to a need for pre-professional studies and a translation to work after graduation. For many, to be considered middle-class or to get a middle-class job required a college degree. [2]\nThe 1970s and 1980s brought questions of whether the return on a college degree was worth the investment. In 1971 a male college graduate earned 22% more than a high-school graduate, but by 1979 a college degree increased earnings by 13%. By 1987 the earning gap was 38%, which was an improvement but added doubts about the stability of higher education as an investment. The 1980s also brought a dramatic increase in the cost of college, which was rising faster than inflation and the average family income. [7][29]\nThe 1990s and 2000s saw a rise in enrollment and tuition costs and a steadily lower unemployment rate for college graduates. College enrollment increased 11% between 1990 and 2000 and increased 37% from 2000–10 to 21 million students. The average college tuition in the 1990–91 school year was $10,620 and rose to $13,393 in 2000–01. Between the 2000–01 school year and the 2010–11 school year public college costs (tuition, room, and board) increased 42% to $18,133.\nThe unemployment rate for workers with a bachelor’s degree or higher in 1990 was 6.5% (compared to 24.9% for high-school dropouts) and was 3.7% in 2000 (compared to 18.4% for high-school dropouts). By 2010 the unemployment rate for college graduates increased to 5.5%, while the rate for college dropouts was 17.3%. [97][98][99]\nA 2011 Pew Research survey showed 50% of college presidents said college is meant to “mature and grow intellectually,” while 48% said college should “provide skills, knowledge and training to help…[students] succeed in the working world.” [25]\nThe number of colleges and universities grew from 1,851 in 1950, to 3,535 in 1990, to 6,900 in 2013. In the 1949–50 school year 2.66 million students were enrolled in colleges and universities; by the 1989–90 school year 13.54 million students were enrolled. In fall 2013, 19.9 million students were enrolled in colleges and universities. [1][2]\nAccording to the U.S. Census Bureau, 33.4% of the adult U.S. population had a bachelor’s degree or higher as of Mar. 30, 2017 (up from 28% in 2006), with 20.8% holding bachelor’s degrees, 9.3% with associate’s degrees, 1.5% with professional degrees, and 1.9% with doctorates. In 1940, when the U.S. Census Bureau began collecting education data, only 4.6% of adults held bachelor’s degrees. [109][110]\nAs many colleges went online or to a hybrid online and in-person model during the COVID-19 pandemic, colleges overall saw a significant drop in fall 2020 enrollment. Undergraduate enrollment fell 3.6% (about 560,000 students) from fall 2019. Community colleges were hit especially hard, with a drop of more than 10% (more than 544,000 students). [130]\n21.7% fewer high-school seniors (class of 2020) had enrolled in college, with a larger drop (32.6%) at high-poverty high schools. 14% fewer students in the class of 2021 filed Free Application for Federal Student Aid forms. [130][131]\nDoug Shapiro, executive research director at the National Student Clearinghouse, called the drops “completely unprecedented.” [130]\nOne survey showed that 75% of households with at least one member attending college classes in fall 2020 changed higher education plans during the COVID-19 pandemic. Most often, households took classes in a different format (39%) or canceled plans altogether (37%), according to Anthony P. Carnevale, director and research professor, and Megan L. Fasules, assistant research professor and research economist, both at the Georgetown University Center on Education and the Workforce. [132]\nCollege enrollment continued to decline after the pandemic, with 17,153,317 students enrolled in spring 2023, a 7.1% decrease in college enrollment from 2019. Both college students and college instructors faced challenges of limited online learning during the pandemic. Rocky classroom experiences resulted in lower test scores, incomplete instruction in vital courses such as math, and increased mental health issues like anxiety. Colleges fear the fallout will last years as reports show that elementary students have experienced the same negative effects of the pandemic. [133][135]\nFreshman enrollment dropped yet again for the fall 2024 term, with the most impact being seen at schools with the largest populations of low-income students. The estimated 5 percent drop comes about a year after there were enormous complications with the FAFSA (Free Application for Federal Student Aid) system that delayed application processing, which may have fueled the almost 6 percent decline in freshman enrolling right after high school. At the same time, the Department of Education reported it would award federal aid to 3 percent more students in 2024, but the department also acknowledged it received 2 percent fewer applications for aid. [267][268]\n“By 2031, 72 percent of jobs in the U.S. will require postsecondary education and/or training. Between 2021 and 2031, there will be 18.5 million job openings per year on average, and some 12.5 million of these annualized openings will require at least some college education,” according to the Center of Education at Georgetown University. [270]\nThe projection above bears out historically: only 22% of American jobs required a high-school diploma or less in 2021, compared to 72% in the 1970s. In 2021, 48% of jobs required an associate’s degree or higher. During the recession between Dec. 2007 and Jan. 2010 jobs requiring college degrees grew by 187,000, while jobs requiring some college or an associate’s degree fell by 1.75 million, and jobs requiring a high-school degree or less fell by 5.6 million. According to researchers at Georgetown University, 99% of job growth (or 11.5 million of 11.6 million jobs) between 2010 and 2016 went to workers with associate’s degrees, bachelor’s degrees, or graduate degrees. [13][104][105][270]\nCollege graduates are well-placed to get jobs because colleges offer networking opportunities, career services, internships, job shadowing, job fairs, and volunteer opportunities in addition to a wide variety of courses that may provide a career direction. Over 80% of college students complete internships before graduation, giving them valuable employment experience before entering the job market. [27]\nThe unemployment rate for Americans aged 25 to 34 with less than a high school education was 7.7 percent in December 2024. The rate drops to 4.4 percent if the person completed some college, to 3.6 percent with an associate’s degree, to 2.4 percent with a bachelor’s degree, and to 2.0 with a graduate or professional degree. [272]\nIn addition to being more likely to have a job, those who have attended or graduated from college are more like to like their jobs: 58% of college graduates and people with some college or associate’s degrees reported being “very satisfied” with their jobs, compared to 50% of high-school graduates and 40% of people without a high-school diploma. [11][101][106][116]\nPlus, college graduates make more money. On average, men and women aged 25 to 35 working full time with a bachelor’s degree or higher earn $77,000 and $65,000 annually respectively. By contrast, the same age group of men and women with only high school diplomas earn $45,000 and $36,000 respectively. And because college graduates are more likely to have better jobs with better salaries, they are also more likely to have health insurance and retirement plans. [11][15][273]\nStudents have the opportunity to interact with other students and faculty, join student organizations and clubs, and take part in discussions and debates.\n“American children party [in college]. But you know something, by partying, they learn social skills. They learn how to interact with each other.…They develop skills which make them innovative. Americans are the most innovative people in the world because of the education system,” according to Vivek Wadhwa, technology entrepreneur and scholar. [7][16][18]\nAccording to Arthur Chickering’s “Seven Vectors” student development theory, “developing mature interpersonal relationships” is one of the seven stages students progress through as they attend college. Students ranked “interpersonal skills” as the most important skill used in their daily lives in a survey of 11,000 college students. [7][16][18]\nStudents live, go to classes, and socialize with other students from around the world and learn from professors with a variety of expertise. The community of people on a college campus means students are likely to make diverse friends and business connections and, potentially, find a partner and life-long friends. Access to a variety of people allows college students to learn about different cultures, religions, and personalities they may have not been exposed to in their hometowns, which broadens their knowledge and perspective.[106]\nCollege students are also more civic-minded. According to the Bureau of Labor Statistics, 43% of college graduates did volunteer work, compared to 19% of high-school graduates and 27% of adults in general. They And, college graduates are more likely to vote, and even more likely to have donated blood in the past year (9%) than people with some college (6%), high-school graduates (4%), and people who did not complete high school (2%). [11][21][22][23][287]\nAdditionally, college graduates attract higher-paying employers to their communities. A 1% increase in college graduates in a community increases the wages of workers without a high-school diploma by 1.9% and the wages of high-school graduates by 1.6%. Plus, college graduates have lower poverty rates and lower rates of reliance on public services due to their lower unemployment rates. [21][28][29]\nAbout 83% of college graduates reported being in excellent health, while 73% of high-school graduates reported the same. A University of Southern California study found that adults over 65 with college degrees spent more years with “good cognition” and fewer years suffering from dementia than adults who did not complete high school. Another study found that 20% of all adults were smokers, while only 9% of college graduates were smokers. 63% of 25–34-year-old college graduates reported exercising vigorously at least once a week, compared to 37% of high-school graduates.\nCollege degrees were linked to lower blood pressure in a 30-year peer-reviewed study and lower levels of cortisol (the stress hormone) by a Carnegie Mellon Psychology department study. Furthermore, only 23% of college graduates aged 35–44 years old were obese, compared to 37% of high-school graduates. College graduates, on average, live six years longer than high-school graduates. [11][15][19][20][120]\nPlus, college graduates have had lower infant mortality rates than high-school graduates. Pregnant women with only a high-school education were 31% more likely to give birth to a low-birth-weight baby than one with a college degree. Children aged two to five years old in households headed by college graduates have a 6% obesity rate, compared to 14% for children in households headed by high-school graduates. 18% more children aged three to five years old with mothers who have a bachelor’s degree could recognize all letters compared to children of high-school graduates. About 59% of children in elementary and middle school with at least one college graduate for a parent participated in after-school activities like sports, arts, and scouting, compared to 27% for high-school graduate parents. [11][15]\n“Diverse environments can help people develop empathy, build meaningful relationships, expand their worldviews, and become more innovative through collaboration,” summarized Purdue Global. [284]\nA 2024 study of 24,000 British people found that those with a diverse social network were more likely to have “higher levels of social cohesion” (being connected and feeling like you belong) in their communities and reported higher levels of personal well-being. \"While interacting with people with the same characteristics may offer a sense of familiarity and comfort, our study suggests that embracing diversity is crucial for fostering resilience and adaptability in an ever-changing world,\" noted study co-author Matt Bennett. [285]\nAs Kellogg School professor Nicole Stephens explained, “In a country that is highly segregated, like the United States, universities are one of the few places where people have the chance to have meaningful and substantive interactions across the lines of social class.” When diverse groups of college students interact, the students had higher GPAs and a greater sense of belonging. [286]\nWhile we think of “diversity” as race, gender, sexuality, and other personal markers, diversity, called the “gold standard of bias reduction” by Stephens, can just as easily apply to ideas. “Higher education is not just about imparting academic knowledge; it’s about learning how to engage with that knowledge critically. The exposure to diverse perspectives made possible by a diverse campus challenges students to question assumptions and prejudices, seek new information, and consider alternative viewpoints. This dynamic learning environment prepares students to tackle complex real-world challenges with a well-rounded and informed new perspective. It encourages them to become active learners who seek out diverse sources of information and engage in meaningful discussions concerning their studies and interpersonal relationships,” as Thaís Roberto of Keystone Education Group explained. [283][286]\nPeople frequently live in areas of homogeneity—where everyone looks like each other, has a similar culture, and similar political backgrounds. College is a place for young adults to experience the diversity of the world, especially as technology and globalization increases interactions with diverse people.\nChris Muller, a personal finance expert, stated:\nWhile the data clearly shows that college remains a smart investment for most students, your individual success will depend largely on making informed choices about your field of study, institution, and financing options. The good news? The benefits extend far beyond just a bigger paycheck, creating positive effects that can last throughout your career and life.\nRemember: The key isn’t just getting any degree—it’s making strategic choices that align with both your career goals and financial reality. By using the resources and strategies outlined above, you can make more confident decisions about your educational investment.\nChris Muller, “New Research Reveals True Value of College Education,” forbes.com, Dec. 29, 2024\nMarisol Cuellar Mejia, Cesar Alesi Perez, Vicki Hsieh, and Hans Johnson, all of the Public Policy Institute of California, stated:\n“Higher education is a critical driver of economic progress. It is also the key policy lever for improving mobility from one generation to the next, especially for low-income, first-generation, Black, and Latino students. As the state’s economy has evolved, the job market has increasingly demanded more highly educated workers, a trend that is projected to continue into the future.\nIn addition to having higher earnings and better job benefits, college graduates are more likely to own a home and less likely to be in poverty or need social services. Society as a whole is also better off, thanks to lower unemployment, less demand for public assistance programs, lower incarceration rates, higher tax revenue, and greater civic engagement.”\n—Marisol Cuellar Mejia et al., “Is College Worth It?,” ppic.org, Mar. 2023\nRichard M. Schulze, founder and chair emeritus of Best Buy Co., Inc. and founder of the Richard M. Schulze Family Foundation, stated:\n“The truth is it [college] remains a crucial driver of success. But we must empower our students with the skills to be innovators, creators and entrepreneurs. Entrepreneurship education empowers students to think creatively, to seek opportunities and solve problems, to empathize with others, to take risks, to accept failure as part of the growth process, and to help take a passion or idea and turn it into a viable business. Learning to think and act like an entrepreneur emboldens students to take charge of their own destinies, and in doing so, it powers the American Dream. My success story [which includes no college education] is the kind of entrepreneurship story that people like to glamorize, but the reason those stories are popular is because they’re so unlikely. What we need right now aren’t idealized stories of success, but a reliable pathway for all bright young minds with the right ideas to make the most of their opportunities, and entrepreneurial education provides just that.”\n—Richard M. Schulze, “Best Buy Founder: What Every US College Should Teach Their Students,” cnn.com, Mar. 6, 2022\nJim McCorkell, founder and CEO of College Possible, stated:\n“The message that college is ‘no longer worth it’ is not only false but also dangerous for America’s low-income students …\nOn the whole, a college degree remains the surest bet for social and economic advancement. The economic returns of college are especially profound for low-income students, and yet they are far more susceptible to college avoidance than their more affluent peers, who are likely to go to college anyway.\nSuch views are hugely problematic for those of us hoping to improve economic mobility in the United States. Almost all the job and wage growth now goes to people with some form of postsecondary education.”\n—Jim McCorkell, “The Dangerous Message in Telling Low-Income Students to Skip College,” hechingerreport.org, June 4, 2019\nMany college graduates are employed in jobs that do not require college degrees. A staggering 52 percent of college graduates had jobs that did not require their degrees, and 75 percent remain in those jobs for a decade after graduation. [274]\n“Having a bachelor’s used to be more rare and candidates with the degree could therefore be more choosy and were more expensive to hire. Today, that is no longer the case,” says Rita McGrath, associate professor at Columbia Business School. A high unemployment rate shifts the supply and demand to the employers’ favor and has made master’s degrees the “new bachelor’s degrees.” In short, too many students earning degrees has diluted the value of a bachelor’s degree.[68][69]\nPlus, college degrees do not guarantee learning or job preparation. A 2023 survey of 800 business professionals found “63 percent saying that recent graduates frequently can’t handle their workload, 61 percent saying they are frequently late to work, 59 percent saying they often miss deadlines and assignments, 58 percent saying that they get offended too easily, 57 percent saying they lack professionalism, 53 percent saying they struggle with eye contact during interviews and 52 percent saying they have poor communication skills.” [275]\n“I don’t think there’s anything wrong with college itself—college is the right choice for a lot of people. The problem is the attitude that if you want to be successful, you have to go to college—that it’s the golden ticket. We’ve all heard this song and dance through our already confusing high school years. It’s college or bust—college or driving an Uber for the rest of your life. Frankly, that’s the stupidest thing I’ve ever heard. I didn’t go to college, and guess what—I don’t drive for Uber. I have a beautiful family, a wildly rewarding life, and a business that just made the Inc. 5000 list for fastest-growing companies in the United States. So, yeah, college is not the only path to success,” explained CEO of Windy City Equipment Service Josh Zolin. [276]\nIn spring 2024 there were more than 17.8 million college students in the United States, and borrowers owed $1.75 trillion in total student debt. [271]\nTuition has risen quicker than income, making college unaffordable for many and forcing students to take out loans. The average annual income for men increased 18 percent from 1971 to 2023 and 132 percent for women. However, the average annual tuition rates for public colleges increased about 129 percent in the same time and about 156 percent for private colleges, making college unaffordable for most average people.\nA study found that 14% of community college students were homeless and 51% had housing insecurity issues (inability to pay rent or utilities, for example), while 33% experienced food insecurity (lack of access to or ability to pay for “nutritionally adequate and safe foods”), though 58% of the students were employed and 42% received federal Pell Grants. [111][112][113]\nFurthermore, student loan debt often forces college graduates to live with their parents and delay marriage, financial independence, and other adult milestones, especially for millennials. In 2013, when the generations was in their late 20s to early 40s, only about 20% were homeowners, and most millennials said their student debt has delayed home ownership by seven years on average. Student loan borrowers delayed saving for retirement (41%), car purchases (40%), home purchases (29%), and marriage (15%). Fewer than 50% of women and 30% of men had passed the “transition to adulthood” milestones by age 30 (finishing school, moving out of their parents’ homes, being financially independent, marrying, and having children). By contrast, in 1960, 77% of women and 65% of men had completed these milestones by age 30. [38][39][121]\nStudent debt also overwhelms many seniors. Whether they cosigned for a child or grandchild’s education, or took out loans for their own educations, there were 7.2 million student loan borrowers aged 50 and over who collectively owed $400.3 billion. About 8% were more than 90 days delinquent in payments. A significant percentage of older borrowers in default were having a portion of their Social Security payments garnished by the U.S. government. [277][278] \nStudent loan debt may not be forgiven in bankruptcy and may not have the same borrower protections as other consumer debt. Medical, legal, credit card, loan, and even gambling debt can immediately be discharged in bankruptcy, but getting student loan debt discharged is much more difficult and rare. Private student loans often do not have the same protections as federal loans like income-based repayments, discharges upon death, or military deferments. [61][70][71]\nPlus, student debt could cause a larger, more general financial crisis for the country. According to the National Association of Consumer Bankruptcy Attorneys, student loans are “beginning to have the same effect” on the economy that the housing bubble and crash created. Former Secretary of Education William Bennett agrees that the student loan debt crisis “is a vicious cycle of bad lending policies eerily similar to the causes of the subprime mortgage crisis.” An advisory council to the Federal Reserve also warned that the growth in student debt “has parallels to the housing crisis.”[61][62][63][64][65]\nTrade professions are necessary for society to function, require less than four years of training, and often pay above average wages. The high number of young adults choosing college over learning a trade has created a “skills gap” in the U.S., and there is now a shortage of “middle-skill” trade workers like machinists, electricians, plumbers, and construction workers. One survey of U.S. manufacturers found that 67% reported a “moderate to severe shortage of talent.” According to the Bureau of Labor Statistics, “middle-skill” jobs made up 45% of projected job openings, but only 25% of the workforce had the skills to fill those jobs. [53][54][55]\nMany people succeed without college degrees. According to the Bureau of Labor Statistics, of the 20 projected fastest growing jobs, seven do not require college degrees. Plus, the following successful people either never enrolled in college or never completed their college degrees: Richard Branson, founder and chair of the Virgin Group; Charles Culpepper, owner and CEO of Coca-Cola; Ellen DeGeneres, comedian and actress; Michael Dell, founder of Dell, Inc.; Walt Disney, Disney Corporation founder; Bill Gates, Microsoft founder; Steve Jobs, cofounder of Apple; Wolfgang Puck, chef and restaurateur; Steve Wozniak, cofounder of Apple; and Mark Zuckerberg, founder of Facebook. [43][44]\nWhether conservative or liberal, “When universities become overtly political, and tilt too far toward one end of the spectrum, they’re denying students and faculty the kind of open-ended inquiry and knowledge-seeking that has long been the basis of American higher education’s success. They’re putting its future at risk,” said New York Times columnist Pamela Paul. [279]\nAmericans are simultaneously convinced that higher education is placing “too much concern [on] protecting students from views they might find offensive” and that “professors are bringing their political and social views into the classroom.” About 59 percent of American adults believe college politics lean toward one side. [280]\nStudents are also bringing their strong political viewpoints to campus. Students with liberal views are avoiding colleges in conservative states that have limited abortion access, higher rates of racial inequality, lower levels of LGBTQ protections, and a lack of gun control laws. And students with conservative views are avoiding colleges in states with legal abortion, laws protecting people of color and the LGBTQ community, and gun restrictions. Almost 30% of students from all political backgrounds were concerned about political opinions being silenced and “canceled” on college campuses. “Perhaps reflecting the drift toward broad political polarization in the U.S. (and elsewhere), we found that students’ gender, race, household income, or region of residence did not arise as statistically significant predictors marking student comfort levels around attending a school in a state they perceive as having an undesirable political landscape,” explain the authors of a Art & Science Group survey. [281]\nThis climate makes students quick to judge and slow to understand because they’re not taking the opportunity to learn from people of varying political, cultural, social, or economic backgrounds. Living and learning in a political bubble only feeds confirmation bias and makes for more fervent, less tolerant political beliefs.[282]\nInstead, students need to experience the real world, where not everyone agrees but where most people don’t yell about their politics.\nScott Galoway, author, podcast host, and entrepreneur, stated:\n“The reality is if you get into an elite university, it’s still a really strong ROI [return on investment]. For most of the majors. The contacts, the credentialing, the certification still pays off even as high as the prices are.\nWhat I think a lot of parents are figuring out is the quote-unquote non-prestige schools, quite frankly, just may not provide the return on investment. And also there’s just a certain type of individual who’s not cut out for college. Unfortunately, in the U.S., there’s a Zeitgeist in our society where if your kid doesn’t get a four-year degree, the kid and the parents have failed—not recognizing that two-thirds of our kids don’t end up with traditional four-year degrees.\nAs the ROI on college has gone down, the compensation for trades jobs has gone up. In the next ten years, there’s going to be five people who leave trades jobs and only two who enter the field. The prospects for many of these jobs that don’t require a college degree are increasing, and all of this adds up to a really interesting and overdue conversation around, Can we stop shaming ourselves if our kid decides not to go to college?”\nKevin T. Dugan, “Scott Galloway on Whether College Is Still Worth It,” nymag.com, Apr. 22, 2024\nSteve Siebold, certified financial educator, states:\n“You won’t find as many college students heading back to class this fall. That’s because enrollment is down nationwide, and rightfully so. There are 4 million fewer students in college now than there were 10 years ago. It’s certainly easy to blame things like the pandemic and a strong labor market, but I believe what it really comes down to is students just don’t want to endure hundreds of thousands of dollars of debt, and it’s hard to blame them….\nMany years ago, it wasn’t even a question. If you wanted a good job you had to go to college. For most kids across America, it was as logical as saying if you want to prevent cavities you have to brush your teeth and floss. Now we are living in a different time and a very different world. In fact, as recently as the last two years, confidence in the value of education has been declining, and college enrollment has fallen by more than 1 million students since spring 2020.\nThere’s no doubt that saying you are a college graduate holds some level of prestige and is necessary for certain occupations. If you want to be a doctor or lawyer, for example, it’s not even a question. On the heels of a recession, however, most high school grads would be better off either entering the workforce immediately and gaining practical real-world experience (which can take you much further than a college education), attending a specialty school geared specifically toward what you want to do with your life, and if college is a must, then considering a more affordable option like doing two years at a community college.”\n—Steve Siebold, “Steve Siebold: College Not Worth the Debt,” triblive.com, Aug. 16, 2022\nMike Rowe, television host of the shows Dirty Jobs and Somebody’s Gotta Do It, states:\n“[W]hen we gave the big push for college back in the 70s, we did it at the expense of alternative education. In other words, we told people, ‘If you don’t get your degree, you’re gonna wind up turning a wrench.’\nThat attitude led to the remove of shop classes around the country. And the removal of shop classes completely obliterated from view the optical and visual proof of opportunity for a whole generation of kids. The skills gap today, in my opinion, is the result of the removal of shop class and the repeated message that the best path for the most people happens to be the most expensive path.\nThis is why, in my opinion, we have $1.6 million in student loans on the books, and 7.3 million open positions, most of which don’t require a four-year degree. We’re just disconnected. We’re rewarding behavior we should be discouraging, we’re lending money we don’t have to kids who are never going to be able to pay it back, to train them for jobs that don’t exist anymore. That’s nuts.”\n—Fox Business Live, “ ‘Dirty Jobs’ Star Mike Rowe Says America’s Workforce Is ‘Disconnected’ ” video.foxbusiness.com, Nov. 7, 2019\nTim Knight, hedge fund manager and author, states:\n“Some of you know that I graduated from college rather swiftly (in just 2 1/2 years)….The information I garnered during those 2 1/2 years hasn’t been useful to me even once during the many years since I graduated, and there isn’t a single contact I made in college that was beneficial to me in any way at all. Simply stated, I could have gone straight from high school to work without any difference.”\n—Tim Knight, “Is College Worth It?,” ZeroHedge, Mar. 7, 2017\n(rankings from 2024 Forbes Top 400)\nThe cumulative wealth of the top ten wealthiest Americans with College Degrees is about $1.5 trillion.\nThe cumulative wealth of the top ten wealthiest Americans without College Degrees is about $701.3 billion.\nThe cumulative wealth of the top ten billionaires with college degrees is 114% more than the cumulative earnings of those without college degrees.\n(*Several families appear as collective units on the Forbes 400 list. Because the wealth is split among multiple people and this list is about individual wealth as related to college degrees, we have not included the families on this list.)\nOf the 45 U.S. presidents, 32 had college degrees and 13 did not. Eight presidents did not attend college, five attended college but did not earn a degree, 21 graduated college with undergraduate degrees only, and 11 earned graduate degrees.\nU.S. News & World Report ranks colleges annually. Below are the colleges and universities with the highest and lowest tuitions among those ranked in the 2022–23 list. Note that U.S. News & World Report ranks colleges and universities in several separate lists (national universities, national liberal arts colleges, etc.), and we have indicated on which list the school is ranked below.\nA frequent argument both for and against college is future earnings of college graduates versus the potential income lost while at college. Below are median annual incomes for men and women as compared to average annual tuitions from 1971–2024. In terms of academic years (fall/spring), the year in the table corresponds to the fall term (i.e., 2024 is the 2024–25 school year).\nAmerican student loan debt rose from $350 billion to $1.61 trillion between 2004 and 2024. Student loan debt was the second-highest household debt from 2010 through 2022, surpassed only by housing loans. However, in 2023 and 2024, auto loan debt exceeded student loan debt.\nAll but one of the top 20 highest paying jobs in 2021 required a doctorate or professional degree (such as an M.D. or RN), while only one of the fastest growing jobs required such a degree.\nConversely, two of the 27 fastest growing jobs required licensing or no degree, 12 required a high-school diploma (or GED) or no formal education, 11 required a bachelor’s or associate’s degree, and one required a master’s degree. 5% was the average rate of job growth.\nAll of the top 20 highest paying jobs require doctoral or professional degrees.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/facts/Pittsburgh",
    "content": "",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/list/cc-the-first-cloned-cat",
    "content": "Meet CC, short for Carbon Copy or Copy Cat (depending on who you ask). She was the world’s first cloned pet.\nCC was born on December 22, 2001, to her surrogate mom, Allie (pictured).\nCC was genetically identical to Rainbow, the cat who donated the genetic material. But the cats looked different because coat patterns and other features can be determined in the womb.\nAlthough CC was a clone of Rainbow, she grew up as her own unique self, with her own look and personality. This photo was taken in 2008.\nCC went on to have her own kittens, who were healthy and genetically unique.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/topic/Native-American",
    "content": "Native American, member of any of the aboriginal peoples of the Western Hemisphere, although the term often connotes only those groups whose original territories were in present-day Canada and the United States.\nPre-Columbian Americans used technology and material culture that included fire and the fire drill; the domesticated dog; stone implements of many kinds; the spear-thrower (atlatl), harpoon, and bow and arrow; and cordage, netting, basketry, and, in some places, pottery. Many Indigenous American groups were hunting and gathering cultures, while others were agricultural peoples. Indigenous American domesticated a variety of plants and animals, including corn (maize), beans, squash, potatoes and other tubers, turkeys, llamas, and alpacas, as well as a variety of semidomesticated species of nut- and seed-bearing plants. These and other resources were used to support communities ranging from small hamlets to cities such as Cahokia, with an estimated population of 10,000 to 20,000 individuals, and Teotihuacán, with some 125,000 to 200,000 residents.\nAt the dawn of the 16th century CE, as the European conquest of the Americas began, Indigenous peoples resided throughout the Western Hemisphere. They were soon decimated by the effects of epidemic disease, military conquest, and enslavement, and, as with other colonized peoples, they were subject to discriminatory political and legal policies well into the 20th, and even the 21st, century. Nonetheless, they have been among the most active and successful Native peoples in effecting political change and regaining their autonomy in areas such as education, land ownership, religious freedom, the law, and the revitalization of traditional culture.\nHistorically, the Indigenous peoples of the Americas have been usually recognized as constituting two broad cultural groupings, American Indians (a term now considered outdated) and Arctic peoples. American Indians are often further grouped by area of residence: Northern America (present-day United States and Canada), Middle America (present-day Mexico and Central America; sometimes called Mesoamerica), and South America. This article is a survey of the culture areas, prehistories, histories, and recent developments of the Indigenous peoples and cultures of the United States and Canada. Some of the terminology used in reference to Indigenous Americans is explained in Sidebar: Tribal Nomenclature: American Indian, Native American, and First Nation; Sidebar: The Difference Between a Tribe and a Band; and Sidebar: Native American Self-Names. An overview of all the Indigenous peoples of the Americas is presented in American Indian; discussions of various aspects of Indigenous American cultures may also be found in the articles pre-Columbian civilizations; Middle American Indian; South American Indian; Arctic: The people; American Indian languages; Native American religions; and Native American arts.\nComparative studies are an essential component of all scholarly analyses, whether the topic under study is human society, fine art, paleontology, or chemistry; the similarities and differences found in the entities under consideration help to organize and direct research programs and exegeses. The comparative study of cultures falls largely in the domain of anthropology, which often uses a typology known as the culture area approach to organize comparisons across cultures.\nThe culture area approach was delineated at the turn of the 20th century and continued to frame discussions of peoples and cultures into the 21st century. A culture area is a geographic region where certain cultural traits have generally co-occurred; for instance, in North America between the 16th and 19th centuries, the Northwest Coast culture area was characterized by traits such as salmon fishing, woodworking, large villages or towns, and hierarchical social organization.\nThe specific number of culture areas delineated for Native America has been somewhat variable because regions are sometimes subdivided or conjoined. The 10 culture areas discussed below are among the most commonly used—the Arctic, the Subarctic, the Northeast, the Southeast, the Plains, the Southwest, the Great Basin, California, the Northwest Coast, and the Plateau. Notably, some scholars prefer to combine the Northeast and Southeast into one Eastern Woodlands culture area or the Plateau and Great Basin into a single Intermontane culture area. Each section below considers the location, climate, environment, languages, peoples, and common cultural characteristics of the area before it was heavily colonized. Prehistoric and post-Columbian Native American cultures are discussed in subsequent sections of this article. A discussion of the Indigenous peoples of the Americas as a whole is found in Indigenous American.\nThis region lies near and above the Arctic Circle and includes the northernmost parts of present-day Alaska and Canada. The topography is relatively flat, and the climate is characterized by very cold temperatures for most of the year. The region’s extreme northerly location alters the diurnal cycle; on winter days the sun may peek above the horizon for only an hour or two, while the proportion of night to day is reversed during the summer months (see midnight sun).\nThe Indigenous peoples of the North American Arctic include the Inuit, Yupik/Yupiit and Unangan (Aleut); their traditional languages are in the Eskimo-Aleut family. Many Alaskan groups prefer to be called Alaska Natives rather than Native Americans; Canada’s Arctic peoples generally prefer the referent Inuit.\nThe Arctic peoples of North America relied upon hunting and gathering. Winters were harsh, but the long hours of summer sunlight supported an explosion of vegetation that in turn drew large herds of caribou and other animals to the inland North. On the coasts, sea mammals and fish formed the bulk of the diet. Small mobile bands were the predominant form of social organization; band membership was generally based on kinship and marriage (see also Sidebar: The Difference Between a Tribe and a Band). Dome-shaped houses were common; they were sometimes made of snow and other times of timber covered with earth. Fur clothing, dog sleds, and vivid folklore, mythology, and storytelling traditions were also important aspects of Arctic cultures. See also Arctic: The people.\nThis region lies south of the Arctic and encompasses most of present-day Alaska and most of Canada, excluding the Maritime Provinces (New Brunswick, Nova Scotia, and Prince Edward Island), which are part of the Northeast culture area. The topography is relatively flat, the climate is cool, and the ecosystem is characterized by a swampy and coniferous boreal forest (taiga) ecosystem.\nProminent tribes include the Innu (Montagnais and Naskapi), Cree, Ojibwe, Denesuline, Dane-zaa, Dene Tha’, Carrier, Gwich’in, Tanaina, and Deg Xinag. Their traditional languages are in the Athabaskan and Algonquian families.\nSmall kin-based bands were the predominant form of social organization, although seasonal gatherings of larger groups occurred at favored fishing locales. Moose, caribou, beavers, waterfowl, and fish were taken, and plant foods such as berries, roots, and sap were gathered. In winter people generally resided in snug semisubterranean houses built to withstand extreme weather; summer allowed for more mobility and the use of tents or lean-tos. Snowshoes, toboggans, and fur clothing were other common forms of material culture. See also American Subarctic peoples.\nThis culture area reaches from the present-day Canadian provinces of Quebec, Ontario, and the Maritimes (New Brunswick, Nova Scotia, and Prince Edward Island) south to the Ohio River valley (inland) and to North Carolina (on the Atlantic Coast). The topography is generally rolling, although the Appalachian Mountains include some relatively steep slopes. The climate is temperate, precipitation is moderate, and the predominant ecosystem is the deciduous forest. There is also extensive coastline and an abundance of rivers and lakes.\nProminent tribes include the Algonquin, Haudenosaunee, Wendat (Wendat), Wampanoag, Mohican, Mohegan, Ojibwe, Ho-chunk (Winnebago), Sauk, Meskwaki, and Peoria. The traditional languages of the Northeast are largely of the Iroquoian and Algonquian language families.\nMost Northeastern peoples engaged in agriculture, and for them the village of a few dozen to a few hundred persons was the most important social and economic unit in daily life. Groups that had access to reliably plentiful wild foods such as wild rice, salmon, or shellfish generally preferred to live in dispersed hamlets of extended families. Several villages or hamlets formed a tribe, and groups of tribes sometimes organized into powerful confederacies. These alliances were often very complex political organizations and generally took their name from the most powerful member tribe.\nCultivated corn (maize), beans, squash, and weedy seed-bearing plants such as Chenopodium formed the economic base for farming groups. All Northeastern peoples took animals including deer, elk, moose, waterfowl, turkeys, and fish. Houses were wickiups (wigwams) or longhouses; both house types were constructed of a sapling framework that was covered with rush matting or sheets of bark. Other common aspects of culture included dugouts made of the trunks of whole trees, birchbark canoes, clothing made of pelts and deerskins, and a variety of medicine societies. See also American Northeast peoples.\nThis region reaches from the southern edge of the Northeast culture area to the Gulf of Mexico; from east to west it stretches from the Atlantic Ocean to somewhat west of the Mississippi valley. The climate is warm temperate in the north and grades to subtropical in the south. The topography includes coastal plains, rolling uplands known as the Piedmont, and a portion of the Appalachian Mountains; of these, the Piedmont was most densely populated. The predominant ecosystems were coastal scrub, wetlands, and deciduous forests.\nPerhaps the best-known Indigenous peoples originally from this region are the Cherokee, Choctaw, Chickasaw, Muscogee, and Seminole, sometimes referred to as the Five Civilized Tribes. Other prominent tribes included the Natchez, Caddo, Apalachee, Timucua, and Guale. Traditionally, most tribes in the Southeast spoke Muskogean languages; there were also some Siouan language speakers and one Iroquoian-speaking group, the Cherokee.\nThe region’s economy was primarily agricultural and often supported social stratification; as chiefdoms, most cultures were structured around hereditary classes of elites and commoners, although some groups used hierarchical systems that had additional status levels. Most people were commoners and lived in hamlets located along waterways. Each hamlet was home to an extended family and typically included a few houses and auxiliary structures such as granaries and summer kitchens; these were surrounded by agricultural plots or fields. Hamlets were usually associated with a town that served as the area’s ceremonial and market center. Towns often included large earthen mounds on which religious structures and the homes of the ruling classes or families were placed. Together, each town and its associated hamlets constituted an autonomous political entity. In times of need these could unite into confederacies, such as those of the Muscogee and Choctaw.\nPeople grew corn, beans, squash, tobacco, and other crops; they also gathered wild plant foods and shellfish, hunted deer and other animals, and fished. House forms varied extensively across the region, including wickiups (wigwams), earth-berm dwellings, and, in the 19th century, chickees (thatched roofs with open walls). The Southeast was also known for its religious iconography, which often included bird themes, and for the use of the “black drink,” an emetic used in ritual contexts. See also American Southeast peoples.\nThe Plains lie in the center of the continent, spanning the area between the western mountains and the Mississippi River valley and from the southern edge of the Subarctic to the Rio Grande in present-day Texas. The climate is of the continental type, with warm summers and cold winters. Relatively flat short-grass prairies with little precipitation are found west of the Missouri River and rolling tallgrass prairies with more moisture are found to its east. Tree-lined river valleys form a series of linear oases throughout the region.\nThe Indigenous peoples of the Plains include speakers of Siouan, Algonquian, Uto-Aztecan, Caddoan, Athabaskan, Kiowa-Tanoan, and Michif languages. Plains peoples also invented a sign language to represent common objects or concepts such as “buffalo” or “exchange.”\nEarth-lodge villages were the only settlements on the Plains until the late 16th century; they were found along major waterways that provided fertile soil for growing corn, beans, squash, sunflowers, and tobacco. The groups who built these communities divided their time between village-based crop production and hunting expeditions, which often lasted for several weeks and involved travel over a considerable area. Plains villagers include the Mandan, Hidatsa, Omaha, Pawnee, and Arikara.\nBy 1750 horses from the Spanish colonies in present-day New Mexico had become common in the Plains and had revolutionized the hunting of bison. This new economic opportunity caused some local villagers to become dedicated nomads, as with the Apsáalooke (who retained close ties with their Hidatsa kin), and also drew agricultural tribes from surrounding areas into a nomadic lifestyle, including the Oceti Sakowin, Blackfoot, Cheyenne, Comanche, Arapaho, and Kiowa.\nGroups throughout the region had in common several forms of material culture, including the tepee, tailored leather clothing, a variety of battle regalia (such as feathered headdresses), and large drums used in ritual contexts. The Sun Dance, a ritual that demanded a high degree of piety and self-sacrifice from its participants, was also found throughout most of the Plains.\nThe Plains is perhaps the culture area in which tribal and band classifications were most conflated. Depictions of indigenous Americans in popular culture have often been loosely based on Plains peoples, encouraging many to view them as the “typical” American Indians. See also Plains peoples.\nThis culture area lies between the Rocky Mountains and the Mexican Sierra Madre, mostly in present-day Arizona and New Mexico. The topography includes plateaus, basins, and ranges. The climate on the Colorado Plateau is temperate, while it is semitropical in most of the basin and range systems; there is little precipitation and the major ecosystem is desert. The landscape includes several major river systems, notably those of the Colorado and the Rio Grande, that create linear oases in the region.\nThe Southwest is home to speakers of Hokan, Uto-Aztecan, Tanoan, Keresan, Kiowa-Tanoan, Penutian, and Athabaskan languages. The region was the home of both agricultural and hunting and gathering peoples, although the most common lifeway combined these two economic strategies. Best known among the agriculturists are the Pueblo peoples, including the Zuni and Hopi. The Yumans, Pima, and Tohono O’odham engaged in both farming and foraging, relying on each to the extent the environment would allow. The Navajo (self-name Diné) and the many Apache groups usually engaged in some combination of agriculture, foraging, and the raiding of other groups.\nThe major agricultural products were corn, beans, squash, and cotton. Wild plant foods, deer, other game, and fish (for those groups living near rivers) were the primary foraged foods. The Pueblo peoples built architecturally remarkable apartment houses of adobe and stone masonry (see pueblo architecture) and were known for their complex kinship structures, kachina (katsina) dances and dolls, and fine pottery, textiles, and kiva and sand paintings. The Navajo built round houses (“hogans”) and were known for their complex clan system, healing rituals, and fine textiles and jewelry. The Apaches, Yumans, Pima, and Tohono O’odham generally built thatched houses or brush shelters and focused their expressive culture on oral traditions. Stone channels and check dams (low walls that slowed the runoff from the sporadic but heavy rains) were common throughout the Southwest, as were basketry and digging sticks. See also American Southwest peoples.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/money/Edward-C-Prescott",
    "content": "Edward C. Prescott (born December 26, 1940, Glens Falls, New York, U.S.—died November 6, 2022, Paradise Valley, Arizona) was an American economist who, with Finn E. Kydland, won the Nobel Prize in Economic Sciences in 2004 for contributions to two areas of dynamic macroeconomics: the time consistency of economic policy and the driving forces behind business cycle fluctuations.\nPrescott studied mathematics at Swarthmore College (B.A., 1962), operations research at Case Western Reserve University (M.S., 1963), and economics at Carnegie Mellon University (Ph.D., 1967). From 1966 to 1971 he taught economics at the University of Pennsylvania, and he then joined the faculty at Carnegie Mellon (1971–80), where he advised Kydland on his doctorate. Prescott later taught at the University of Chicago, the University of Minnesota, and Arizona State University, among others. In addition, he became an adviser to the Federal Reserve Bank of Minneapolis in 1981.\n(Read Milton Friedman’s Britannica entry on money.)\nPrescott and Kydland, working separately and together, influenced the monetary and fiscal policies of governments and laid the basis for the increased independence of many central banks, notably those in Sweden, New Zealand, and the United Kingdom. In their seminal article “Rules Rather than Discretion: The Inconsistency of Optimal Plans” (1977), they demonstrated how a declared commitment to a low inflation rate by policy makers might create expectations of low inflation and unemployment rates. If this monetary policy is then changed and interest rates are reduced—for example, to give a short-term boost to employment—the policy makers’ (and thus the government’s) credibility will be lost and conditions worsened by the “discretionary” policy. In “Time to Build and Aggregate Fluctuations” (1982), the two economists established the microeconomic foundation for business cycle analyses, demonstrating that technology changes or supply shocks, such as oil price hikes, could be reflected in investment and relative price movements and thereby create short-term fluctuations around the long-term economic growth path.\nIn addition to winning the Nobel Prize, Prescott was a fellow of the Brookings Institution, the Guggenheim Foundation, the Econometric Society, and the American Academy of Arts and Sciences; he was elected a member of the National Academy of Sciences in 2008. He was an editor of several journals, including the International Economic Review (1980–90), and his extensive writings covered such wide-ranging topics as business cycles, economic development, general equilibrium theory, and finance.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/biography/Pontiac-Ottawa-chief",
    "content": "Pontiac (born c. 1720, on the Maumee River [now in Ohio, U.S.]—died April 20, 1769, near the Mississippi River [at present-day Cahokia, Ill.]) was an Ottawa Indian chief who became a great intertribal leader when he organized a combined resistance—known as Pontiac’s War (1763–64)—to British power in the Great Lakes area.\nLittle is known of Pontiac’s early life, but by 1755 he had become a tribal chief. His commanding manner and talent for strategic planning enabled him to also become the leader of a loose confederacy among the Ottawa, the Potawatomi, and the Ojibwa. In 1760 he met Maj. Robert Rogers, a British colonial ranger on his way to occupy Michilimackinac (St. Ignace, Mich.) and other forts surrendered by the French during the French and Indian War of 1754–63. Pontiac agreed to let the British troops pass unmolested on condition that he be treated with respect.\nHe soon came to realize, however, that under the British rule his people would no longer be welcome in the forts and that they would ultimately be deprived of their hunting grounds by aggressive settlers encroaching upon their ancestral lands. Thus, in 1762 Pontiac enlisted support from practically every Indian tribe from Lake Superior to the lower Mississippi for a joint campaign to expel the British. In what the English called “Pontiac’s Conspiracy,” he arranged for each tribe to attack the nearest fort (May 1763) and then to combine to wipe out the undefended settlements.\nThe shrewd and daring leader elected to capture Detroit himself, and it is for this military action that he is particularly remembered. When his carefully laid plans for a surprise attack (May 7) were betrayed to the commanding officer, he was forced to lay siege to the fort. On July 31 Pontiac won a brilliant victory at the Battle of Bloody Run, but the besieged fort was nevertheless able to receive reinforcements, and on October 30 Pontiac withdrew to the Maumee River.\nPontiac’s larger plan was more successful. Of the 12 fortified posts attacked by the united tribes, all but 4 were captured; most of the garrisons were wiped out, several relief expeditions were nearly annihilated, and the frontier settlements were plundered and laid desolate. By 1764 continuing British action began to take its toll, however, and Pontiac finally agreed to conclude a treaty of peace in July 1766.\nThree years later, while he was visiting in Illinois, a Peoria Indian stabbed and killed him. His death occasioned a bitter war among the tribes, and the Illinois group was almost annihilated by his avengers.\nOttawa, city, capital of Canada, located in southeastern Ontario. In the eastern extreme of the province, Ottawa is situated on the south bank of the Ottawa River across from Gatineau, Quebec, at the confluence of the Ottawa (Outaouais), Gatineau, and Rideau rivers. The Ottawa River (some 790 miles [1,270 km] long), the principal tributary of the St. Lawrence River, was a key factor in the city’s settlement and development; its watershed, covering more than 57,000 square miles (148,000 square km), facilitated the transport of resources such as furs, timber, and minerals from the region. The river’s Chaudière Falls, just west of the Rideau Canal, while initially a navigational hazard for the fur trade and later for the transport of logging rafts, ultimately proved to be an asset in the production of hydroelectric power for the city and a boon to the growth of industry.\nOriginally a trading and lumbering community that grew into a town of regional significance, Ottawa was named the capital of the Province of Canada in 1857 and retained that status when Canada became a dominion within the British Commonwealth in 1867. Because of its location on the boundary between English-speaking Ontario and French-speaking Quebec and its position as national capital, Ottawa is one of the most bilingual cities in the country. Area, 1,077 square miles (2,790 square km); Ottawa-Gatineau metro. area, 2,427 square miles (6,287 square km). Pop. (2011) 883,391; Ottawa-Gatineau metro. area, 1,254,919; (2021) 1,017,449; Ottawa-Gatineau metro. area, 1,488,307.\nThe earliest inhabitants of the Ottawa region were members of the Algonquin First Nation (Native Americans), who established settlements in the Ottawa River valley. The tribe known as the Ottawa (Outaouais), however, settled in the area for only a short period during the mid-1600s; their traditional territory was considerably farther west on Lake Huron. They were well known as traders (the name Ottawa is believed to be derived from an Algonquian word meaning “to trade”), and they took part in the local fur trade.\nThe first descriptions of Ottawa’s future site were written in 1613 by the founder of New France, Samuel de Champlain. The rivers served as passageways for explorers and fur traders over the following two centuries. In 1763 France ceded all of New France east of the Mississippi River to Great Britain under the Treaty of Paris. The Napoleonic Wars (1792–1815) increased Britain’s need for shipbuilding timber, and the Ottawa River valley offered just such resources. In 1800 a group of farmers from Massachusetts led by Philemon Wright established the area’s first permanent town, Wrightsville, north of the Ottawa River. (It was incorporated in 1875 as the city of Hull, now part of Gatineau.) Wright began harvesting trees in 1806, giving rise to a timber trade that attracted lumberjacks and other itinerant workers. Permanent settlement on the south bank of the river did not occur until the following decade, when, during the War of 1812 between Britain and the United States, it became apparent that the St. Lawrence River between Montreal and Kingston, Ontario, was vulnerable to attack as both a military and an economic target. The British proposed turning the Rideau River into a canal to serve as an alternate shipping and transportation route, diverting traffic up the Ottawa River to Chaudière Falls and back down to Kingston. Lieut. Col. John By of the Royal Engineers was in charge of constructing the more than 126-mile- (203-km-) long canal (1826–32). He also surveyed and laid out a town site on the south bank as a place of residence for his workers and himself; that village became known as Bytown. It was incorporated as a town in 1850 and as the city of Ottawa in 1855.\nThe Rideau Canal was never used as a military route, but its importance in the transportation of timbers, goods, and people was the main factor in the city’s early growth, especially during the period of high U.S. demand for forest products that lasted through the 1800s. Political unrest within Britain’s Canadian colonies (including armed rebellion in 1837) resulted in Britain’s unification of the separate colonies of Upper and Lower Canada into one province, the Province of Canada (1841). When it came time to designate a capital for united Canada, however, political quarrels between rival cities, such as between Quebec city and Toronto and between Montreal and Kingston, induced leaders to call upon Queen Victoria to settle the question. The queen selected Ottawa in late 1857. Although Ottawa was a strong candidate because of its location and its accessibility by rail, the choice still surprised many, given the city’s relatively small size and its identification primarily with the processing and distribution of lumber. When the Dominion of Canada was formed a decade later, Ottawa remained the capital, and it continued to grow as a major administrative centre.\nA large portion of Ottawa was razed by a fire in 1900 that destroyed Hull, and in 1916 another fire consumed most of the Parliament Buildings. Reconstruction began soon afterward, and the Centre Block was completed in 1922. The growth of civil service jobs was slow but expanded in response to events such as World War I, the institution of an income tax, the Great Depression of the 1930s, and World War II. Still, the capital city, to all appearances, remained an industrial pulp and paper mill town, with smokestacks and railways dominating the waterfront.\nIn 1937 Prime Minister William L. Mackenzie King brought architect Jacques Gréber from France to begin the redevelopment of the national capital district. The plan was to “beautify” Ottawa and bring its appearance and amenities more into line with those expected of a capital city. It included the provision of large areas of parkland in and around the federal government buildings and a greenbelt around the city’s perimeter to control urban growth. It also called for the removal of rail lines and industry from the urban centre. With the interruption of World War II, Gréber’s plan was not completed until 1950. Its implementation began through the creation of the National Capital Commission (NCC) by an act of Parliament (1958) and, about the same time, the creation of the National Capital Region, which involved the federal government’s acquisition of some 1,800 square miles (4,700 square km) of land on both the Quebec and the Ontario sides of the Ottawa River. Federal administrative buildings were decentralized, moving to locations on both sides of the river.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/money/Finn-E-Kydland",
    "content": "Finn E. Kydland (born December 1943, Ålgård, near Stavanger, Nor.) is a Norwegian economist, who, with Edward C. Prescott, won the Nobel Prize in Economic Sciences in 2004 for contributions to dynamic macroeconomics, notably the time consistency of economic policy and the driving forces behind business cycles.\nKydland was educated at the Norwegian School of Economics and Business Administration (NHH; B.S., 1968) and Carnegie Mellon University in Pittsburgh (Ph.D., 1973), where Prescott advised on his doctorate. Kydland was an assistant professor of economics at NHH (1973–78) and taught at Carnegie Mellon (1978–2004) before joining the faculty at the University of California, Santa Barbara, in 2004. He also served as a consultant research associate to the Federal Reserve banks of Dallas and Cleveland.\n(Read Milton Friedman’s Britannica entry on money.)\nKydland and Prescott, working separately and together, influenced the monetary and fiscal policies of governments and laid the basis for the increased independence of many central banks, notably those in the United Kingdom, Sweden, and New Zealand. In their seminal article “Rules Rather than Discretion: The Inconsistency of Optimal Plans” (1977), the two economists demonstrated how a declared commitment to a low inflation rate by policy makers might create expectations of low inflation and unemployment rates. If this monetary policy is then changed and interest rates are reduced—for example, to take political advantage of the prosperity generated by increased inflation or to give a short-term boost to employment—the policy makers’ (and thus the government’s) credibility will be lost and conditions worsened by the “discretionary” policy. In “Time to Build and Aggregate Fluctuations” (1982), the pair demonstrated that technology changes or supply shocks, such as oil price hikes, could be reflected in investment and relative price movements and thereby create short-term fluctuations around the long-term economic growth path.\nKydland has authored and coauthored multiple books, including Inflation Persistence and Flexible Prices (2001), Argentina’s Recovery and “Excess” Capital Shallowing of the 1990s (2002), and Monetary Policy, Taxes and the Business Cycle (2004).",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/facts/Carnegie-Mellon-University",
    "content": "",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/money/Andrew-Mellon",
    "content": "Andrew Mellon (born March 24, 1855, Pittsburgh, Pennsylvania, U.S.—died August 26, 1937, Southampton, New York) was an American financier, philanthropist, and secretary of the treasury (1921–32) who reformed the tax structure of the U.S. government in the 1920s. His benefactions made possible the building of the National Gallery of Art in Washington, D.C.\nAfter completing his studies at Western University (now the University of Pittsburgh), Mellon entered his father’s banking house in 1874 and proved so capable that in 1882 his father transferred the bank’s ownership to him. In the next three decades Mellon built up a financial-industrial empire by supplying capital for Pittsburgh-based corporations to expand in such fields as aluminum, steel, oil, coal, coke, and synthetic abrasives. Mellon’s keen judgment of new technologies and potentially successful firms and entrepreneurs enabled him to help found the Aluminum Company of America (Alcoa) and the Gulf Oil Corporation. In alliance with Henry Clay Frick, he helped found the Union Steel Company, which later merged with United States Steel Corporation. He and Frick were also the principal organizers (in 1889) of the Union Trust Company, which became Mellon’s principal financial instrument and acquired his family’s bank. By the early 1920s Mellon had become one of the richest men in the United States.\nMellon was appointed to head the U.S. Treasury by Pres. Warren G. Harding in 1921. In discussions concerning reduction of the national debt, which totaled about $26 billion in 1920 as a result of World War I expenditures, Mellon held that continuance of the high wartime tax rates would discourage business expansion and hence reduce revenue. He also advocated reduction of the surtax rates on incomes. Largely through his efforts Congress repealed the excess-profits tax and gradually lowered the income tax rate until in 1926 the maximum surtax was reduced from 50 to 20 percent. Further reductions in tax rates were later made; by June 30, 1928, the national debt had fallen to $17,604,000,000. As chairman ex officio of the World War I foreign debt commission, Mellon also played a prominent part in formulating U.S. policy concerning the funding of war debts owed to the United States by foreign governments. Mellon’s policies helped stimulate the American economic boom of the 1920s, and he continued to head the Treasury under presidents Calvin Coolidge and Herbert Hoover. His popularity declined after the Great Depression began in 1929, however, and in 1932 he resigned to serve as U.S. ambassador to England for a year.\nOne of the nation’s foremost art collectors, Mellon gave a collection valued at $25 million to the U.S. government in 1937. Among other paintings, it contained Raphael’s Alba Madonna, 23 Rembrandts, and 6 Vermeers. Mellon donated $15 million to build the National Gallery of Art, opened in 1941, to house the collection.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/History-Society",
    "content": "Did Anyone Really Think the Titanic was Unsinkable?\nDid Marie-Antoinette Really Say “Let Them Eat Cake”? \nA Timeline of the Vietnam War\n12 Greek Gods and Goddesses \nThe international conflict was virtually unprecedented in the slaughter, carnage, and destruction it caused, embroiling most of Europe along with Russia, the U.S., the Middle East, and other regions. WWI led to the fall of four great imperial dynasties, resulted in the Bolshevik Revolution, and, by destabilizing Europe, laid the groundwork for WWII.\nWWI: From the First Shots Fired to the Last\nCastles\nWorld Cup\nKings and Queens of Britain\nNotre-Dame de Paris\nWomen of World War I\nSkyscrapers",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/biography/Herbert-A-Simon",
    "content": "Herbert A. Simon (born June 15, 1916, Milwaukee, Wis., U.S.—died Feb. 9, 2001, Pittsburgh, Pa.) was an American social scientist known for his contributions to a number of fields, including psychology, mathematics, statistics, and operations research, all of which he synthesized in a key theory that earned him the 1978 Nobel Prize for Economics. Simon and his longtime collaborator Allen Newell won the 1975 A.M. Turing Award, the highest honour in computer science, for their “basic contributions to artificial intelligence, the psychology of human cognition, and list processing.”\nSimon graduated from the University of Chicago in 1936 and earned a doctorate in political science there in 1943. After holding various posts in political science, he became a professor of administration and psychology at the Carnegie Institute of Technology (now Carnegie Mellon University) in 1949, later becoming the Richard King Mellon University Professor of Computer Science and Psychology there.\nHe is best known for his work on the theory of corporate decision making known as “behaviourism.” In his influential book Administrative Behavior (1947), Simon sought to replace the highly simplified classical approach to economic modeling—based on a concept of the single decision-making, profit-maximizing entrepreneur—with an approach that recognized multiple factors that contribute to decision making. According to Simon, this theoretical framework provides a more realistic understanding of a world in which decision making can affect prices and outputs.\nCrucial to this theory is the concept of “satisficing” behaviour—achieving acceptable economic objectives while minimizing complications and risks—as contrasted with the traditional emphasis on maximizing profits. Simon’s theory thus offers a way to consider the psychological aspects of decision making that classical economists have tended to ignore.\nLater in his career, Simon pursued means of creating artificial intelligence through computer technology. He wrote several books on computers, economics, and management, and in 1986 he won the U.S. National Medal of Science.\ncognitive psychology, Branch of psychology devoted to the study of human cognition, particularly as it affects learning and behaviour. The field grew out of advances in Gestalt, developmental, and comparative psychology and in computer science, particularly information-processing research. Cognitive psychology shares many research interests with cognitive science, and some experts classify it as a branch of the latter. Contemporary cognitive theory has followed one of two broad approaches: the developmental approach, derived from the work of Jean Piaget and concerned with “representational thought” and the construction of mental models (“schemas”) of the world, and the information-processing approach, which views the human mind as analogous to a sophisticated computer system.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/story/whats-the-difference-between-a-president-and-a-prime-minister",
    "content": "World leaders can go by many titles: premier, chancellor, dear respected comrade. But two of the most common are president and prime minister. What differentiates them other than the latter’s hint of continental flavor?\nA prime minister, by definition, must be able to command a legislative majority. In a parliamentary system, the prime minister sets the national agenda, appoints cabinet officials, and governs at the behest of a party or a coalition of parties. In parliamentary systems, presidents—if they exist—serve as largely ceremonial heads of state. In constitutional monarchies, such roles are filled by the king or queen. If a prime minister loses the legislative mandate, opposition parties may call for a vote of confidence in an effort to topple the sitting government. In this event, the president may be called upon to formally dismiss the legislature and schedule fresh elections.\nThe concept of a president as a dual head of state and head of government had its origin with British colonial administrative structures in North America. Leaders of colonial councils were called presidents, as were the heads of some state governments. The presiding representative at the Continental Congress retained the title, and when the U.S. Constitution created the presidency of the United States, the role carried greatly expanded executive powers. Those powers would increase dramatically over time—particularly in periods of national crisis—which led historian Arthur M. Schlesinger, Jr., to describe the modern office as the “imperial presidency.” Still, there remains some separation of powers: the U.S. president cannot directly introduce legislation, and Congress retains the power of the purse. In a worst-case scenario, the legislature and the chief executive could deadlock, creating a situation in which the respective officeholders are essentially serving without governing.\nPerhaps the two most-obvious examples of each office are the U.S. president and the prime minister of the United Kingdom. The French presidency under the Fifth Republic wields considerably more executive power than its American counterpart, although it can still be partially checked by opposition parties in the National Assembly (lower legislative house). Since the appointment of Vladimir Putin as prime minister of Russia in 1999 and his election as president later that year, the balance of executive power in that country has resided in whichever of the two roles he is filling at the time.\nAfter the glitzy red-carpet arrivals, the feel-good montages, and the host’s opening roast, the Oscars ceremony switches to its raison d’être: revealing the previous year’s highest achievers in cinema. One after another, the presenters list the nominees, open a lavish envelope, and reveal the winner in each category. The champions breathlessly accept their awards and, over and over, effusively thank their mothers, God, and the academy. The academy has so much gratitude bestowed on it throughout the ceremony that many of us watching at home may start to wonder: What is this obscure body? The answer is less enthralling than one might think. The academy—that is, the Academy of Motion Picture Arts and Sciences—is the organization that votes for the Oscar winners. Perhaps what’s more interesting is figuring out who the members of the academy are and how they vote.\nThe academy is an exclusive Hollywood institution that has its own governing body (aptly named the Board of Governors), 17 separate branches, and a thorough rule book on membership eligibility and voting processes. Since 2016, when the board announced that it would diversify its membership, the academy has grown to about 8,000 members. It doesn’t publicize the names of all those members, but each spring it releases a list of the individuals it has invited to join its ranks. Invitees have included Mindy Kaling, Rashida Jones, Kendrick Lamar, Melissa Etheridge, and J.K. Rowling. The academy also posts the names of members of its current Board of Governors, which includes elected representatives of the 17 branches. These branches represent the various fields of cinema: acting, directing, writing, sound editing, and others. In 2019 the academy’s Website showed that the Board of Governors’ president was cinematographer John Bailey, its first vice president was makeup artist Lois Burwell, and its other officers included actors Laura Dern, Whoopi Goldberg, and Alfred Molina.\nThe rest of the academy members are not listed, but we can guess who a few are by looking at some of the requirements to join the institution. To qualify, an individual must work in the film industry. This means that neither individuals who work exclusively in television nor members of the press may join. Oscar nominees are often considered for membership automatically, while other candidates must be sponsored by two active members of the branch they wish to join. Each branch also has its own specific requirements. Directors, for example, must have a minimum of two directing credits, at least one of them within the past 10 years. So we can be pretty sure that such Hollywood treasures as Meryl Streep, Jack Nicholson, Steven Spielberg, and Tom Hanks, who have each been nominated several times and have won Oscars, are members of the academy. New members may choose only one of the branches to join. This means that Sofia Coppola and Alfonso Cuarón, for example, who were each nominated for and won Oscars, could sign on to either the directing branch or the writing branch but not both.\nWhile the membership of the academy is largely obscure, the voting process is perhaps only slightly clearer. It involves two phases: first, nominating the Oscar candidates and, second, voting for the winners. In the first phase, members receive a ballot that lists qualifying movies. To be considered for nomination, a movie must be feature-length and must have been publicly screened for paid admission for at least one week at a commercial theater in Los Angeles county between January 1 and December 31 of the award year. Documentaries and foreign films have their own eligibility requirements. Members may nominate only for awards within their branch and for best picture. Emma Stone may thus suggest nominees for best actress, actor, supporting actress, and supporting actor, but she may not nominate candidates in the best sound editing or best sound mixing categories. Each member of the academy picks up to five candidates for each of their designated categories and lists them by preference.\nTo determine the nominees in each category, the ballots are tallied by certified public accountants from a firm designated by the academy’s president in a somewhat arcane system that might seem like a sacred ritual to an outsider. To ensure that nominees have broad, rather than just popular, support, the academy uses instant runoff voting, sometimes called preferential voting, which involves several rounds and a “magic” number, wherein a candidate must receive a predetermined number of votes to be considered a nominee. A few weeks after the nominees are announced in January, the second phase of voting begins. For the final voting, all active or lifetime academy members are allowed to cast ballots in any category, but they are discouraged from voting in categories where they lack expertise. Accountants once again tally the ballots, using the preferential system to determine the winner for best picture but using the popular vote for all other categories.\nAfter all the voting and tallying, the winners are finally determined, but they are not reported to anyone. Only two accountants see the final results, and they are responsible for keeping those results secret until the awards ceremony. The accountants memorize the names of the winners, stuff two sets of envelopes, and pack and store two briefcases at an undisclosed location until the day of the ceremony. At the ceremony, neither the members of the academy nor the producers of the awards show know who will receive an Oscar. It is a complete mystery until the presenter utters one of the most famous lines in Hollywood: “And the Oscar goes to....”",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/money",
    "content": "",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/topic/poverty",
    "content": "poverty, the state of one who lacks a usual or socially acceptable amount of money or material possessions. Poverty is said to exist when people lack the means to satisfy their basic needs. In this context, the identification of poor people first requires a determination of what constitutes basic needs. These may be defined as narrowly as “those necessary for survival” or as broadly as “those reflecting the prevailing standard of living in the community.” The first criterion would cover only those people near the borderline of starvation or death from exposure; the second would extend to people whose nutrition, housing, and clothing, though adequate to preserve life, do not measure up to those of the population as a whole. The problem of definition is further compounded by the noneconomic connotations that the word poverty has acquired. Poverty has been associated, for example, with poor health, low levels of education or skills, an inability or an unwillingness to work, high rates of disruptive or disorderly behaviour, and improvidence. While these attributes have often been found to exist with poverty, their inclusion in a definition of poverty would tend to obscure the relation between them and the inability to provide for one’s basic needs. Whatever definition one uses, authorities and laypersons alike commonly assume that the effects of poverty are harmful to both individuals and society.\nAlthough poverty is a phenomenon as old as human history, its significance has changed over time. Under traditional (i.e., nonindustrialized) modes of economic production, widespread poverty had been accepted as inevitable. The total output of goods and services, even if equally distributed, would still have been insufficient to give the entire population a comfortable standard of living by prevailing standards. With the economic productivity that resulted from industrialization, however, this ceased to be the case—especially in the world’s most industrialized countries, where national outputs were sufficient to raise the entire population to a comfortable level if the necessary redistribution could be arranged without adversely affecting output.\nSeveral types of poverty may be distinguished depending on such factors as time or duration (long- or short-term or cyclical) and distribution (widespread, concentrated, individual).\n(Read Indira Gandhi’s 1975 Britannica essay on global underprivilege.)\nCyclical poverty refers to poverty that may be widespread throughout a population, but the occurrence itself is of limited duration. In nonindustrial societies (present and past), this sort of inability to provide for one’s basic needs rests mainly upon temporary food shortages caused by natural phenomena or poor agricultural planning. Prices would rise because of scarcities of food, which brought widespread, albeit temporary, misery.\nIn industrialized societies the chief cyclical cause of poverty is fluctuations in the business cycle, with mass unemployment during periods of depression or serious recession. Throughout the 19th and early 20th centuries, the industrialized nations of the world experienced business panics and recessions that temporarily enlarged the numbers of the poor. The United States’ experience in the Great Depression of the 1930s, though unique in some of its features, exemplifies this kind of poverty. And until the Great Depression, poverty resulting from business fluctuations was accepted as an inevitable consequence of a natural process of market regulation. Relief was granted to the unemployed to tide them over until the business cycle again entered an upswing. The experiences of the Great Depression inspired a generation of economists such as John Maynard Keynes, who sought solutions to the problems caused by extreme swings in the business cycle. Since the Great Depression, governments in nearly all advanced industrial societies have adopted economic policies that attempt to limit the ill effects of economic fluctuation. In this sense, governments play an active role in poverty alleviation by increasing spending as a means of stimulating the economy. Part of this spending comes in the form of direct assistance to the unemployed, either through unemployment compensation, welfare, and other subsidies or by employment on public-works projects. Although business depressions affect all segments of society, the impact is most severe on people of the lowest socioeconomic strata because they have fewer marginal resources than those of a higher strata.\nIn contrast to cyclical poverty, which is temporary, widespread or “collective” poverty involves a relatively permanent insufficiency of means to secure basic needs—a condition that may be so general as to describe the average level of life in a society or that may be concentrated in relatively large groups in an otherwise prosperous society. Both generalized and concentrated collective poverty may be transmitted from generation to generation, parents passing their poverty on to their children.\nCollective poverty is relatively general and lasting in parts of Asia, the Middle East, most of Africa, and parts of South America and Central America. Life for the bulk of the population in these regions is at a minimal level. Nutritional deficiencies cause disease seldom seen by doctors in the highly developed countries. Low life expectancy, high levels of infant mortality, and poor health characterize life in these societies.\nCollective poverty is usually related to economic underdevelopment. The total resources of many developing nations in Africa, Asia, and South and Central America would be insufficient to support the population adequately even if they were equally divided among all of the citizens. Proposed remedies are twofold: (1) expansion of the gross national product (GNP) through improved agriculture or industrialization, or both, and (2) population limitation. Thus far, both population control and induced economic development in many countries have proved difficult, controversial, and at times inconclusive or disappointing in their results.\nAn increase of the GNP does not necessarily lead to an improved standard of living for the population at large, for a number of reasons. The most important reason is that, in many developing countries, the population grows even faster than the economy does, with no net reduction in poverty as a result. This increased population growth stems primarily from lowered infant mortality rates made possible by improved sanitary and disease-control measures. Unless such lowered rates eventually result in women bearing fewer children, the result is a sharp acceleration in population growth. To reduce birth rates, some developing countries have undertaken nationally administered family-planning programs, with varying results. Many developing nations are also characterized by a long-standing system of unequal distribution of wealth—a system likely to continue despite marked increases in the GNP. Some authorities have observed the tendency for a large portion of any increase to be siphoned off by persons who are already wealthy, while others claim that increases in GNP will always trickle down to the part of the population living at the subsistence level.\nIn many industrialized, relatively affluent countries, particular demographic groups are vulnerable to long-term poverty. In city ghettos, in regions bypassed or abandoned by industry, and in areas where agriculture or industry is inefficient and cannot compete profitably, there are found victims of concentrated collective poverty. These people, like those afflicted with generalized poverty, have higher mortality rates, poor health, low educational levels, and so forth when compared with the more affluent segments of society. Their chief economic traits are unemployment and underemployment, unskilled occupations, and job instability. Efforts at amelioration focus on ways to bring the deprived groups into the mainstream of economic life by attracting new industry, promoting small business, introducing improved agricultural methods, and raising the level of skills of the employable members of the society.\nSimilar to collective poverty in relative permanence but different from it in terms of distribution, case poverty refers to the inability of an individual or family to secure basic needs even in social surroundings of general prosperity. This inability is generally related to the lack of some basic attribute that would permit the individual to maintain himself or herself. Such persons may, for example, be blind, physically or emotionally disabled, or chronically ill. Physical and mental handicaps are usually regarded sympathetically, as being beyond the control of the people who suffer from them. Efforts to ameliorate poverty due to physical causes focus on education, sheltered employment, and, if needed, economic maintenance.\nsocial mobility, movement of individuals, families, or groups through a system of social hierarchy or stratification. If such mobility involves a change in position, especially in occupation, but no change in social class, it is called “horizontal mobility.” An example would be a person who moves from a managerial position in one company to a similar position in another. If, however, the move involves a change in social class, it is called “vertical mobility” and involves either “upward mobility” or “downward mobility.” An industrial worker who becomes a wealthy businessman moves upward in the class system; a landed aristocrat who loses everything in a revolution moves downward in the system.\nIn revolution an entire class structure is altered. Yet once the society has been radically reorganized, further social mobility may be minimal. Social mobility, however, may come about through slower, more subtle changes, such as the movement of individuals or groups from a poor agrarian region to a richer urban one. Throughout history international migration has been an important factor in upward mobility. One instance may be seen in the 19th-century migration of members of the working and peasant classes from Europe to the United States. On the other hand, Western European colonial expansion, while benefiting some, served to enslave others. In modern societies, social mobility is typically measured by career and generational changes in the socioeconomic levels of occupations.\nThe social results of mobility, particularly of the vertical type, are difficult to measure. Some believe that large-scale mobility, both upward and downward, breaks down class structure, rendering a culture more uniform. Others argue that those who attempt to rise or maintain a higher position actually strengthen the class system, for they are likely to be concerned with enforcing class differences. Thus, some sociologists have suggested that class distinctions might be reduced not by individual mobility but by the achievement of social and economic equality for all.\nOne positive consequence of mobility has been a better use of individual aptitude. This has been aided by the expansion of educational opportunities in modern industrial nations. On the negative side, a high rate of vertical mobility may produce individual and societal anomie (a term coined by the French sociologist Émile Durkheim). The individual experiencing anomie feels socially isolated and anxious; in a larger, societal context, generally accepted beliefs and standards of conduct are weakened or disappear.\nMany believe that the class system of Western industrial nations has changed dramatically since the provision of extensive welfare services, beginning in Germany in the 1880s. Greater social mobility has resulted from changes in the occupational structure, typified by an increase in the relative number of white-collar and professional occupations, with a decrease in the less-skilled and manual occupations. This has led to higher standards of living. Such increased mobility, it is argued, has minimized class differences, so that Western nations are moving toward a relatively classless (or predominantly middle-class) society. Yet other observers contend that a new upper class is in the process of formation, comprising production organizers and managers in both the public and the private arenas. Most recently, in postindustrial societies, inequality seems to be increasing between highly educated and poorly educated workers or between those with access to evolving technologies and those who lack such access.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/browse/Cities-Towns",
    "content": "",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/biography/Michael-Chabon",
    "content": "Michael Chabon (born May 24, 1963, Washington, D.C., U.S.) is an American novelist and essayist known for his elegant deployment of figurative language and adventurous experiments with genre conceits. His narratives are frequently suffused with references to world mythology and to his own Jewish heritage.\nChabon was the elder of two children. His father, who was a pediatrician and hospital administrator, and mother, who became a lawyer following the couple’s 1975 divorce, moved the family to the ostensibly utopian planned community of Columbia, Maryland, in 1969. Though Columbia later became the second largest city in Maryland, the town had been occupied for only two years when he arrived, and the young Chabon spent hours exploring the nascent metropolis. Following high school, he enrolled briefly at Carnegie Mellon University (1980–81) before transferring to the University of Pittsburgh, where he earned a bachelor’s degree in English in 1984. He then attended the University of California, Irvine, receiving a master’s degree in English in 1987.\nChabon’s adviser submitted his master’s thesis, a work of fiction, to a New York publisher without his knowledge. The volume, which related the sexual awakenings and existential meanderings of a gangster’s son during his first summer out of college, earned Chabon a record advance and was published as The Mysteries of Pittsburgh (1988; film 2008). Because of Chabon’s refusal to euphemize the protagonist’s homosexual experiences, he attracted a substantial gay following. A Model World and Other Stories (1991) was a compilation of some of his short fiction. His next novel, Wonder Boys (1995; film 2000), centres on a weekend in the life of a stymied creative writing professor as he wrangles with his various personal and professional failures. Chabon conceived the novel on the heels of his own inability to refine the massive manuscript that he had originally intended as his sophomore effort.\nAnother short-story collection, Werewolves in Their Youth (1999), assembled pieces originally published in magazines such as The New Yorker and GQ. Chabon’s third novel, The Amazing Adventures of Kavalier and Clay (2000), was the sprawling tale of two Jewish cousins who, at the cusp of the comic book phenomenon that began in the mid-1930s, devise a superhero and shepherd him to fame in the pages of their own serial. In conveying the vagaries along their path to eventual happiness, Chabon liberally larded the tale with mythological references. The golem of Prague was central among them, serving as a metaphor for both rebirth and the process of generating a fictional character. The novel earned him the Pulitzer Prize in 2001. He followed with Summerland (2002), an expansive young adult novel that features a hero who must save his father (and the world) from the apocalypse by winning a game of baseball against a cast of tricksters drawn from American folklore.\nThe Yiddish Policemen’s Union (2007), which speculatively situated the Jewish state in Sitka, Alaska, rather than Israel, deployed hard-boiled detective novel conventions in relating the resolution of a murder. The novel won a Hugo Award in 2008. Gentlemen of the Road (2007), a picaresque featuring medieval Jewish brigands, was serialized in The New York Times and then published as a novel. Chabon also penned the children’s book The Astonishing Secret of Awesome Man (2011). He scrutinized the consequences of corporate domination and examined American race relations in the novel Telegraph Avenue (2012), which centres on the denizens of a small jazz and soul record shop threatened by the imminent incursion of a rival chain store. The critically acclaimed Moonglow (2016) was inspired by Chabon’s conversations with his dying grandfather.\nMaps and Legends: Reading and Writing Along the Borderlands (2008) and Manhood for Amateurs: The Pleasures and Regrets of a Husband, Father, and Son (2009) were collections of essays ruminating on his obsession with the intersections of fiction genres and on domestic life, respectively. In Pops (2018) he further explored fatherhood. Chabon also ventured into screenwriting, penning a draft of the script for Spider-Man 2 and collaborating on the script for John Carter (2012), adapted from an Edgar Rice Burroughs novel.\nChabon was inducted into the American Academy of Arts and Letters in 2012.\nhistorical novel, a novel that has as its setting a period of history and that attempts to convey the spirit, manners, and social conditions of a past age with realistic detail and fidelity (which is in some cases only apparent fidelity) to historical fact. The work may deal with actual historical personages, as does Robert Graves’s I, Claudius (1934), or it may contain a mixture of fictional and historical characters. It may focus on a single historic event, as does Franz Werfel’s Forty Days of Musa Dagh (1934), which dramatizes the defense of an Armenian stronghold. More often it attempts to portray a broader view of a past society in which great events are reflected by their impact on the private lives of fictional individuals. Since the appearance of the first historical novel, Sir Walter Scott’s Waverley (1814), this type of fiction has remained popular. Though some historical novels, such as Leo Tolstoy’s War and Peace (1865–69), are of the highest artistic quality, many of them are written to mediocre standards. One type of historical novel is the purely escapist costume romance, which, making no pretense to historicity, uses a setting in the past to lend credence to improbable characters and adventures.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/topic/Pittsburgh-Pirates",
    "content": "Pittsburgh Pirates, American professional baseball team based in Pittsburgh, Pennsylvania. Sometimes referred to as the “Bucs,” the Pirates are among the oldest teams in baseball and have won the World Series five times (1909, 1925, 1960, 1971, and 1979).\nThe team that would become the Pirates was founded as the Pittsburgh Alleghenys in 1882 and played in the American Association before moving to the National League (NL) in 1887. League officials accused the Alleghenys of using “piratical” tactics to steal talented players from opposing teams, and the ball club embraced the label and in 1891 officially adopted the name Pirates.\nShortstop and Pittsburgh-area native Honus Wagner was among the team’s early standouts, playing with the Pirates from 1900 to 1917. The winner of eight batting titles and a member of the first group of players inducted into the Baseball Hall of Fame, Wagner led the Pirates to three straight pennants at the turn of the 20th century and to an appearance in the first World Series (1903), which Pittsburgh lost to the Boston Red Sox in eight games.\nThe Pirates won their first World Series title in 1909, but the team struggled in the 1910s before returning to the World Series to defeat the Washington Senators in 1925. They reached the World Series again in 1927 but lost to a standout New York Yankees squad that featured Babe Ruth and Lou Gehrig.\nLess-successful years followed, but the Pirates, led by future Hall of Fame member Roberto Clemente, won the 1960 World Series dramatically with Bill Mazeroski’s game-winning home run in the ninth inning of the seventh game. In the 1970s the Pirates left Forbes Field, their home for more than 60 years, to play in Three Rivers Stadium, where the power hitting of Willie Stargell and Dave Parker helped them clinch the NL Eastern Division six times and win World Series championships in 1971 and 1979. In the mid-1980s the Pirates acquired sluggers Barry Bonds and Bobby Bonilla, and the team finished atop the NL Eastern Division in three consecutive seasons (1990–92) but failed to advance to the World Series each year.\nBonds signed with the San Francisco Giants as a free agent after the 1992 season, and the Pirates (who began playing in PNC Park in 2001) soon became one of the worst teams in baseball. In 2009 the Pirates had their 17th straight losing season, a record for a professional franchise in the four major North American sports leagues. That streak extended to 20 consecutive seasons until it was broken in 2013, when the Bucs, led by NL Most Valuable Player Andrew McCutchen, won 94 games and earned a berth in the postseason, where the team won the one-game Wild Card playoff but was eliminated in the NL Division Series. Pittsburgh returned to the postseason the following year but was eliminated in the Wild Card Game. In 2015 the team won 98 games—Pittsburgh’s best season since the Bonds era and the second most wins in the major leagues that year—but the team was relegated to the Wild Card contest because of the Pirates residing in the same division as the 100-win St. Louis Cardinals, and the Bucs again lost the one-game playoff. The team’s three-season postseason streak ended in 2016 as Pittsburgh posted a losing record. The Pirates continued to struggle in the ensuing years, and in 2021 the team lost 101 games.\nNational League (NL), oldest existing major-league professional baseball organization in the United States. The league began play in 1876 as the National League of Professional Baseball Clubs, replacing the failed National Association of Professional Base Ball Players. The league’s supremacy was challenged by several rival organizations over the years, beginning with the American Association in 1882–91. Of these, only the American League, formed in 1900, survived. Beginning in 1903, the champions of the National and American leagues have engaged in an annual World Series contest to decide the championship of Major League Baseball. The National League consists of 15 teams aligned in three divisions. In the NL East are the Atlanta Braves, Miami Marlins, New York Mets, Philadelphia Phillies, and Washington (D.C.) Nationals. In the NL Central are the Chicago Cubs, Cincinnati Reds, Milwaukee Brewers, Pittsburgh Pirates, and St. Louis Cardinals. In the NL West are the Arizona Diamondbacks, Colorado Rockies, Los Angeles Dodgers, San Diego Padres, and San Francisco Giants.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/place/Allegheny",
    "content": "Allegheny, county, southwestern Pennsylvania, U.S., consisting of a hilly region on the Allegheny Plateau bounded to the southeast by the Monongahela and Youghiogheny rivers and to the northeast by the Allegheny River. The Ohio, Allegheny, and Monongahela rivers converge in the centre of the county to form an area known as the Golden Triangle; this was a strategic point of contention between the French and the English, who fortified the area with Fort Duquesne (1754) and Fort Pitt (1761), respectively. With the defeat of Pontiac’s warriors (1763), the area opened up to settlers who founded Pittsburgh (1764), which became the county seat when Allegheny county was formed in 1788. The county’s name is derived from the Delaware Indian word oolikhanna, meaning “good river.” The Triangle is now Pittsburgh’s central business district and the location of the popular Point State Park.\nPittsburgh, which was linked to outside markets by the Pennsylvania Canal and the Portage Railroad (both completed in 1834), became the nation’s centre for the iron, steel, and glass industries in the 19th century. The city is also the home of the University of Pittsburgh (1787), Duquesne University (1878), and Carnegie Mellon University (1900), as well as the museums, library, and music hall of The Carnegie, formerly the Carnegie Institute.\nOther communities include Bethel Park, Monroeville, McKeesport, Baldwin, and Wilkinsburg. The main economic activities are services (health care, business, education, and engineering), retail trade, manufacturing (steel and industrial machinery), and finance. Area 730 square miles (1,891 square km). Pop. (2000) 1,281,666; (2010) 1,223,348.\nPittsburgh, city, seat (1788) of Allegheny county, southwestern Pennsylvania, U.S. The city is located at the confluence of the Allegheny and Monongahela rivers, which unite at the point of the “Golden Triangle” (the business district) to form the Ohio River. A city of hills, parks, and valleys, it is the centre of an urban industrial complex that includes the surrounding cities of Aliquippa (northwest), New Kensington (northeast), McKeesport (southeast), and Washington (southwest) and the borough of Wilkinsburg (east). Inc. borough, 1794; city, 1816. Area city, 58 square miles (150 square km). Pop. (2010) 305,704; Pittsburgh Metro Area, 2,356,285; (2020) 302,971; Pittsburgh Metro Area, 2,370,930.\nAlgonquian- and Iroquoian-speaking peoples were early inhabitants of the region. The conflict between the British and French over territorial claims in the area was settled in 1758 when General John Forbes and his British and colonial army expelled the French from Fort Duquesne (built 1754). Forbes named the site for the British statesman William Pitt the Elder. The British built Fort Pitt (1761) to ensure their dominance at the source of the Ohio. Settlers began arriving after Native American forces led by Ottawa chief Pontiac were defeated in 1763; an agreement subsequently was made between Native American groups and the Penn family, and a boundary dispute was ended between Pennsylvania and Virginia. Pittsburgh was laid out (1764) by John Campbell in the area around the fort (now the Golden Triangle). Following the American Revolution, the town became an outfitting point for settlers traveling westward down the Ohio River.\nPittsburgh’s strategic location and wealth of natural resources spurred its commercial and industrial growth in the 19th century. A blast furnace, erected by George Anschutz about 1792, was the forerunner of the iron and steel industry that for more than a century was the city’s economic mainstay; by 1850 Pittsburgh was known as the “Iron City.” The Pennsylvania Canal and the Portage Railroad, both completed in 1834, opened vital markets for trade and shipping. The city suffered a great loss in 1845 when some 24 blocks of businesses, homes, churches, and other buildings were destroyed by fire.\nAfter the American Civil War, great numbers of European immigrants swelled Pittsburgh’s population, and industrial magnates such as Andrew Carnegie, Henry Clay Frick, and Thomas Mellon built their steel empires there. The city became the focus of historic friction between labour and management, and the American Federation of Labor was born there in 1881.\nBy 1900 the city’s population had reached 321,616. Growth continued nearly unabated through World War II, the war years bringing a particularly great boon for the economy. The population crested at more than 675,000 in 1950, after which it steadily declined; by the end of the century, it had returned almost to the 1900 level. Most citizens were still of European ancestry, but the growing African American proportion of the population exceeded one-fourth. During the period of economic and population growth, Pittsburgh had come to epitomize the grimy, polluted industrial city. After the war, however, the city undertook an extensive redevelopment program that emphasized smoke-pollution control, flood prevention, and sewage disposal. In 1957 it became the first American city to generate electricity by nuclear power.\nBy the late 1970s and early ’80s, the steel industry had virtually disappeared—a result of foreign competition and decreased demand. Many of the surrounding mill towns were laid to waste by unemployment, becoming a symbol of the notorious Rust Belt, the U.S. region where steelmaking and manufacturing once thrived before succumbing to widespread unemployment and poverty. Pittsburgh, however, successfully diversified its economy through more emphasis on light industries—though metalworking, chemicals, and plastics remained important—and on such high-technology industries as computer software, industrial automation (robotics), and biomedical and environmental technologies. Numerous industrial research laboratories were established in the area, and the service sector became increasingly important. Pittsburgh long has been one of the nation’s largest inland ports, and it remains a leading transportation centre.\nMuch of the Golden Triangle has been rebuilt and includes Point State Park (containing Fort Pitt Blockhouse and Fort Pitt Museum), the Gateway Center (site of several skyscrapers and a garden), and the David L. Lawrence Convention Center. The University of Pittsburgh was chartered in 1787. Other educational institutions include Carnegie Mellon (1900), Duquesne (1878), Point Park (1960), Chatham (1869), and Carlow (1929) universities and two campuses of the Community College of Allegheny County (1966).\nCentral to the city’s cultural life is the Carnegie Museums of Pittsburgh (formerly Carnegie Institute), an umbrella organization consisting of a number of institutions. Its museums include those for the fine arts and natural history (both founded in 1895), the Carnegie Science Center (1991), which now also houses the Henry Buhl, Jr., Planetarium and Observatory (1939), and the Andy Warhol Museum (1994), which exhibits the works of the Pittsburgh-born artist and filmmaker. Other institutions affiliated with the organization are the Carnegie Library of Pittsburgh, which contains more than 3.3 million volumes, and the Carnegie Music Hall. The Pittsburgh Symphony Orchestra performs at Heinz Hall, a restored movie theatre.\nPhipps Conservatory and Botanical Gardens (1893), which covers 15 acres (6 hectares), is noted for its extensive greenhouses. The city’s zoo, in the northeastern Highland Park neighbourhood, includes an aquarium. Two new sports venues opened in 2001 on the north bank of the Allegheny opposite the Golden Triangle: PNC Park is home of the Pirates, the city’s professional baseball team, and Acrisure Stadium houses the Steelers, its professional football team. The Penguins, Pittsburgh’s professional ice hockey team, plays at PPG Paints Arena. Popular summertime attractions include riverboat excursions on Pittsburgh’s waterways and Kennywood, an amusement park southeast of the city in West Mifflin.",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/place/Golden-Triangle-region-Pittsburgh-Pennsylvania",
    "content": "…an area known as the Golden Triangle; this was a strategic point of contention between the French and the English, who fortified the area with Fort Duquesne (1754) and Fort Pitt (1761), respectively. With the defeat of Pontiac’s warriors (1763), the area opened up to settlers who founded Pittsburgh (1764),…\n…the point of the “Golden Triangle” (the business district) to form the Ohio River. A city of hills, parks, and valleys, it is the centre of an urban industrial complex that includes the surrounding cities of Aliquippa (northwest), New Kensington (northeast), McKeesport (southeast), and Washington (southwest) and the borough",
    "type": "html"
  },
  {
    "url": "https://www.britannica.com/place/McKeesport",
    "content": "McKeesport, city, Allegheny county, southwestern Pennsylvania, U.S. It is situated at the junction of the Monongahela and Youghiogheny rivers, 14 miles (23 km) southeast of Pittsburgh. First settled about 1769 by David McKee, a ferry operator, the town was laid out in 1795 by his son John. In 1794 it was a centre of dissident activity during the Whiskey Rebellion (an insurrection against an excise tax on distilled liquors). Coal mining began in the area about 1830, but McKeesport did not develop appreciably until the basic ingredients of the steel industry (coal, iron, and railroads) coalesced in the late 19th century. It became part of the Pittsburgh industrial complex, with steel production as the dominant activity.\nMcKeesport began experiencing considerable unemployment as the steel industries in the Pittsburgh area declined in the 1980s. Some light manufacturing (steel pipe, pressurized containers, speciality steel products) continues in the city, and service industries are important. A marina has been built along the Youghiogheny River. The McKeesport campus of Pennsylvania State University (Penn State McKeesport) opened in 1947. Inc. borough, 1842; city, 1891. Pop. (2000) 24,040; (2010) 19,731.\nPittsburgh, city, seat (1788) of Allegheny county, southwestern Pennsylvania, U.S. The city is located at the confluence of the Allegheny and Monongahela rivers, which unite at the point of the “Golden Triangle” (the business district) to form the Ohio River. A city of hills, parks, and valleys, it is the centre of an urban industrial complex that includes the surrounding cities of Aliquippa (northwest), New Kensington (northeast), McKeesport (southeast), and Washington (southwest) and the borough of Wilkinsburg (east). Inc. borough, 1794; city, 1816. Area city, 58 square miles (150 square km). Pop. (2010) 305,704; Pittsburgh Metro Area, 2,356,285; (2020) 302,971; Pittsburgh Metro Area, 2,370,930.\nAlgonquian- and Iroquoian-speaking peoples were early inhabitants of the region. The conflict between the British and French over territorial claims in the area was settled in 1758 when General John Forbes and his British and colonial army expelled the French from Fort Duquesne (built 1754). Forbes named the site for the British statesman William Pitt the Elder. The British built Fort Pitt (1761) to ensure their dominance at the source of the Ohio. Settlers began arriving after Native American forces led by Ottawa chief Pontiac were defeated in 1763; an agreement subsequently was made between Native American groups and the Penn family, and a boundary dispute was ended between Pennsylvania and Virginia. Pittsburgh was laid out (1764) by John Campbell in the area around the fort (now the Golden Triangle). Following the American Revolution, the town became an outfitting point for settlers traveling westward down the Ohio River.\nPittsburgh’s strategic location and wealth of natural resources spurred its commercial and industrial growth in the 19th century. A blast furnace, erected by George Anschutz about 1792, was the forerunner of the iron and steel industry that for more than a century was the city’s economic mainstay; by 1850 Pittsburgh was known as the “Iron City.” The Pennsylvania Canal and the Portage Railroad, both completed in 1834, opened vital markets for trade and shipping. The city suffered a great loss in 1845 when some 24 blocks of businesses, homes, churches, and other buildings were destroyed by fire.\nAfter the American Civil War, great numbers of European immigrants swelled Pittsburgh’s population, and industrial magnates such as Andrew Carnegie, Henry Clay Frick, and Thomas Mellon built their steel empires there. The city became the focus of historic friction between labour and management, and the American Federation of Labor was born there in 1881.\nBy 1900 the city’s population had reached 321,616. Growth continued nearly unabated through World War II, the war years bringing a particularly great boon for the economy. The population crested at more than 675,000 in 1950, after which it steadily declined; by the end of the century, it had returned almost to the 1900 level. Most citizens were still of European ancestry, but the growing African American proportion of the population exceeded one-fourth. During the period of economic and population growth, Pittsburgh had come to epitomize the grimy, polluted industrial city. After the war, however, the city undertook an extensive redevelopment program that emphasized smoke-pollution control, flood prevention, and sewage disposal. In 1957 it became the first American city to generate electricity by nuclear power.\nBy the late 1970s and early ’80s, the steel industry had virtually disappeared—a result of foreign competition and decreased demand. Many of the surrounding mill towns were laid to waste by unemployment, becoming a symbol of the notorious Rust Belt, the U.S. region where steelmaking and manufacturing once thrived before succumbing to widespread unemployment and poverty. Pittsburgh, however, successfully diversified its economy through more emphasis on light industries—though metalworking, chemicals, and plastics remained important—and on such high-technology industries as computer software, industrial automation (robotics), and biomedical and environmental technologies. Numerous industrial research laboratories were established in the area, and the service sector became increasingly important. Pittsburgh long has been one of the nation’s largest inland ports, and it remains a leading transportation centre.\nMuch of the Golden Triangle has been rebuilt and includes Point State Park (containing Fort Pitt Blockhouse and Fort Pitt Museum), the Gateway Center (site of several skyscrapers and a garden), and the David L. Lawrence Convention Center. The University of Pittsburgh was chartered in 1787. Other educational institutions include Carnegie Mellon (1900), Duquesne (1878), Point Park (1960), Chatham (1869), and Carlow (1929) universities and two campuses of the Community College of Allegheny County (1966).\nCentral to the city’s cultural life is the Carnegie Museums of Pittsburgh (formerly Carnegie Institute), an umbrella organization consisting of a number of institutions. Its museums include those for the fine arts and natural history (both founded in 1895), the Carnegie Science Center (1991), which now also houses the Henry Buhl, Jr., Planetarium and Observatory (1939), and the Andy Warhol Museum (1994), which exhibits the works of the Pittsburgh-born artist and filmmaker. Other institutions affiliated with the organization are the Carnegie Library of Pittsburgh, which contains more than 3.3 million volumes, and the Carnegie Music Hall. The Pittsburgh Symphony Orchestra performs at Heinz Hall, a restored movie theatre.\nPhipps Conservatory and Botanical Gardens (1893), which covers 15 acres (6 hectares), is noted for its extensive greenhouses. The city’s zoo, in the northeastern Highland Park neighbourhood, includes an aquarium. Two new sports venues opened in 2001 on the north bank of the Allegheny opposite the Golden Triangle: PNC Park is home of the Pirates, the city’s professional baseball team, and Acrisure Stadium houses the Steelers, its professional football team. The Penguins, Pittsburgh’s professional ice hockey team, plays at PPG Paints Arena. Popular summertime attractions include riverboat excursions on Pittsburgh’s waterways and Kennywood, an amusement park southeast of the city in West Mifflin.",
    "type": "html"
  }
]